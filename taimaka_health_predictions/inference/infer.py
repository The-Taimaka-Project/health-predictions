# -*- coding: utf-8 -*-
"""infer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zv7p1ycuoeHZ3EG_rUtUMEq0GwhZXF0i

# summary
run inference on the chance of models, time until event models, export waterfall shap waterfall chart values for the AG chance of models for explanability

# setup

run

1.   etl
2.   etl_derioration

to get the dataframes that this notebook depends on
"""
# model version
VERSION = '0.1.0'

# above this percentile, active, label = 0 will have their shap values calculated
TOP_PCT = 0.50
EXPORT_SHAP_WATERFALL = True

# Commented out IPython magic to ensure Python compatibility.

# Change directory to the repository
# %cd health-predictions/taimaka_health_predictions
# %pwd

import pandas as pd

# Set global output format to Pandas
import datetime
import numpy as np
import pickle
import os
import re
import json
from taimaka_health_predictions.utils.digitalocean import DigitalOceanStorage
from taimaka_health_predictions.utils.globals import ETL_DIR, MODEL_DIR, ADMIT_ONLY, NOT_ADMIT_ONLY, logger

from taimaka_health_predictions.inference.util import (
    DetnReaderWriter,
    split_detn_new_onset_medical_complication
)
#import shap

from warnings import simplefilter,filterwarnings

from lifelines import WeibullAFTFitter
from autogluon.features.generators import AutoMLPipelineFeatureGenerator
from autogluon.tabular import TabularDataset, TabularPredictor

detn_reader = DetnReaderWriter()

# run secrets first to set the environment variables for your credentials
do_storage = DigitalOceanStorage()



# use auto ML (autogluon) to predict the 5 deterioration events

admit_current = do_storage.read_pickle( ETL_DIR + 'admit_current.pkl')

simplefilter(action="ignore", category=pd.errors.PerformanceWarning)
simplefilter(action="ignore", category=FutureWarning)

pid_probabilities = admit_current[['pid','status','site_current']].copy()
active_pids = pid_probabilities[pid_probabilities['status']=='active']['pid'].unique()
logger.debug(pid_probabilities.shape)

def export_waterfall_shap_values(explainer2,detn,features):
  import shap

  shap_values_all = explainer2.shap_values(detn[features]) # Calculate SHAP values

  exp_all = shap.Explanation(shap_values_all,
                       explainer2.expected_value,
                       data=detn[features].values,
                       feature_names=features)


  # prompt: create a json object combining each array occurrence of exp_all.values with the corresponding array occurence of exp_all.data

  json_objects = []
  for shap_values_array, feature_values_array in zip(exp_all.values, exp_all.data):
      # Create a dictionary for the current instance
      data_dict = {
          'base_probability': exp_all.base_values,
          'shap_values': shap_values_array.tolist(),
          'feature_values': feature_values_array.tolist(),
          'feature_names': exp_all.feature_names
      }
      json_objects.append(data_dict)

  # prompt: zip detn['pid'] and json_objects into a dict keyed by detn['pid']

  json_dict = dict(zip(detn['pid'], json_objects))


  # Example: Create a Series from the json_dict where the index is the pid
  json_series = pd.Series(json_dict)
  return json_series




def read_detn(label):
  with open(dir + f'{label}.pkl', 'rb') as f:
    detn = pickle.load(f)
  return detn



"""# new onset medical complication

New onset medical complication - 'cat1' complication (see vars in raw ODK data with cat1_ prefix)
"""


detn, label = detn_reader.read_new_onset_medical_complication()
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


y_cat1_cols = [col for col in detn.columns if col.startswith('y_cat1')]
detn.drop(columns=y_cat1_cols,inplace=True) # only looking at label new_onset_medical_complication, not the actual categories

logger.debug(detn.shape)

detn['wk1_calc_los'].fillna(0,inplace=True)
LOS_CUTOFF = 12
MUAC_CUTOFF = 12.5
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()},{detn.shape}')


detn = detn[(detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF) ].copy()

logger.debug(f'{detn[label].sum()},{detn[label].mean()},{detn.shape}')

"""## predictive model importances

### AutoGluon

#### stratified models by length of service
"""

def run_ag_model(label,detn,suffix):
  from autogluon.tabular import TabularDataset, TabularPredictor
  from util import AutogluonWrapper
  import shap
  from taimaka_health_predictions.utils.globals import MODEL_DIR

  predictor, metadata = do_storage.read_autogluon_tarball(path=f"{MODEL_DIR}{label}{suffix}/{VERSION}/model.tar.gz")  

  ag_features = predictor.features()
  logger.debug(f'ag features:, {len(ag_features)}, {ag_features}')


  y_pred_proba_all = predictor.predict_proba(detn[ag_features])

  target_class = 1  # can be any possible value of the label column
  negative_class = 0
  baseline = detn[ag_features][detn[label]==negative_class].sample(20, random_state=0)
  ag_wrapper = AutogluonWrapper(predictor, ag_features, target_class)
  explainer = shap.KernelExplainer(ag_wrapper.predict_proba, baseline)


  return y_pred_proba_all,explainer,ag_features
detn_admit_only,_,_,_ = split_detn_new_onset_medical_complication(detn,label)

#detn_admit_only = make_dummy_columns(detn_admit_only)
pid_not_in_admit = detn[~detn['pid'].isin(detn_admit_only['pid'])]['pid']
detn_filtered = detn[detn['pid'].isin(pid_not_in_admit)].copy()

y_pred_proba_all1,explainer1a,ag_features1a = run_ag_model(label,detn_admit_only,ADMIT_ONLY)

if EXPORT_SHAP_WATERFALL:    
  json_series = export_waterfall_shap_values(explainer1a,detn_admit_only[(detn_admit_only['pid'].isin(active_pids)) & (detn_admit_only[label]== 0)],ag_features1a)

y_pred_proba_all2,explainer2a,ag_features2a = run_ag_model(label,detn_filtered,NOT_ADMIT_ONLY)

y_pred_proba_all_stratified = pd.concat([y_pred_proba_all1,y_pred_proba_all2],axis=0)
y_pred_proba_all_stratified_series = y_pred_proba_all_stratified[1].rename(f'probability_{label}_stratified')
pid_probabilities = pid_probabilities.join(y_pred_proba_all_stratified_series)

# prompt: percentrank of ''probability_new_onset_medical_complication_stratified'
pid_probabilities[f'percentrank_{label}_stratified'] = pid_probabilities[f'probability_{label}_stratified'].rank(pct=True)
top_pct_pids = pid_probabilities[pid_probabilities[f'percentrank_{label}_stratified'] > TOP_PCT]['pid'].unique()

if EXPORT_SHAP_WATERFALL:
  json_series2 = export_waterfall_shap_values(explainer2a,detn_filtered[(detn_filtered['pid'].isin(active_pids)) & (detn_filtered['pid'].isin(top_pct_pids)) & (detn_filtered[label]== 0)],ag_features2a)
  pid_probabilities = pd.merge(pid_probabilities, pd.concat([json_series,json_series2]).rename(f'{label}_shap_data'), left_on='pid', right_index=True, how='left')


# survival
logger.debug(f'{detn[label].sum()},{detn.shape}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn[label].sum()},{detn.shape}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# prompt: filter detn for label_date not null or wk1_calcdate_weekly not null

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn[label].sum()},{detn.shape}')

# prompt: create column in detn called final_date which is status_date or wk1_calcdate_weekly whichever is greater
# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(detn[label_date].notnull().sum())

# prompt: for rows in detn where label==1 set duration_days to label_date - calcdate

# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

# prompt: clip detn['duration_days'] to 0.1

detn['duration_days'] = detn['duration_days'].clip(lower=0.1)


# AFT survival regression
selected_columns = ['muac_diff','wk1_muac','muac_diff_ratio_rate']

X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regr_cols = ['wk1_muac','muac','muac_diff','muac_diff_ratio_rate',label,'duration_days','pid','status']
regression_dataset = detn[regr_cols].copy()
regression_dataset['wk1_muac'].fillna(regression_dataset['muac'],inplace=True)
regression_dataset.drop(columns=['muac'],inplace=True)
regression_dataset.dropna(inplace=True)
#regression_dataset['wk1_muac'].fillna(0,inplace=True)


regression_dataset['duration_days'] = regression_dataset['duration_days'].clip(lower=.01)
aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True, formula=' wk1_muac + muac_diff + muac_diff_ratio_rate')

logger.debug(aft.print_summary())


# censored (i.e., patients w/o complication yet) survival prediction

regr_cols = ['wk1_muac','muac','muac_diff','muac_diff_ratio_rate',label,'duration_days','pid','status']
censored_subjects = detn[regr_cols].copy()
censored_subjects['wk1_muac'].fillna(censored_subjects['muac'],inplace=True)
censored_subjects.drop(columns=['muac'],inplace=True)

regression_dataset[regression_dataset['wk1_muac'].notnull()][['muac_diff','muac_diff_ratio_rate']].isnull().sum()

censored_subjects['muac_diff_ratio_rate'].fillna(0,inplace=True)

censored_subjects.dropna(inplace=True)

censored_subjects = censored_subjects.loc[detn[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]


y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','wk1_muac','muac_diff','muac_diff_ratio_rate',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')

# poor weight gain


detn, label = detn_reader.read_detn_weight_loss_ever()

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 12
MUAC_CUTOFF = 12.7

detn = detn[((detn['wk1_b_discharged']==0) & (detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF)) ].copy()


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

# infer
y_pred_proba_all,explainer,ag_features = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)

pid_probabilities[f'percentrank_{label}'] = pid_probabilities[f'probability_{label}'].rank(pct=True)
top_pct_pids = pid_probabilities[pid_probabilities[f'percentrank_{label}'] > TOP_PCT]['pid'].unique()
if EXPORT_SHAP_WATERFALL:
  json_series = export_waterfall_shap_values(explainer,detn[(detn['pid'].isin(active_pids)) & (detn['pid'].isin(top_pct_pids)) & (detn[label]== 0)],ag_features)
  pid_probabilities = pd.merge(pid_probabilities, json_series.rename(f'{label}_shap_data'), left_on='pid', right_index=True, how='left')



# survival
logger.debug(f'{detn[label].sum()},{detn.shape}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn[label].sum()},{detn.shape}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn[label].sum()},{detn.shape}')

# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(detn[label_date].notnull().sum())


# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

"""### AFT survival regression"""

selected_columns = ['weight_trend','pid']


X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regression_dataset = regression_dataset.dropna()

aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True,formula='weight_trend')
logger.debug(aft.print_summary())

"""### censored (i.e., patients w/o weight loss yet) survival prediction"""

censored_subjects=data_x_numeric.join(data_y)

censored_subjects['weight_trend'].fillna(0,inplace=True)

censored_subjects = censored_subjects.loc[censored_subjects[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]

y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','weight_trend',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)

# muac loss 2 weeks consecutive
detn, label = detn_reader.read_muac_loss_2_weeks_consecutive()

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 12
MUAC_CUTOFF = 12.7

# prompt: detn where wk1_muac between 11.5 and 12.6 and wk1_calc_los between 10 and 12

detn = detn[((detn['wk1_b_discharged']==0) & (detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF)) ].copy()


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


# run inference
y_pred_proba_all,explainer,ag_features = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)

pid_probabilities[f'percentrank_{label}'] = pid_probabilities[f'probability_{label}'].rank(pct=True)
top_pct_pids = pid_probabilities[pid_probabilities[f'percentrank_{label}'] > TOP_PCT]['pid'].unique()
if EXPORT_SHAP_WATERFALL:
  json_series = export_waterfall_shap_values(explainer,detn[(detn['pid'].isin(active_pids)) & (detn['pid'].isin(top_pct_pids)) & (detn[label]== 0)],ag_features)
  pid_probabilities = pd.merge(pid_probabilities, json_series.rename(f'{label}_shap_data'), left_on='pid', right_index=True, how='left')


# survival
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn.shape,detn[label].sum()}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn.shape,detn[label].sum()}')

# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(f'{detn.shape,detn[label].sum()}')

# prompt: for rows in detn where label==1 set duration_days to label_date - calcdate

# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

detn['duration_days'] = detn['duration_days'].clip(lower=0.1)

selected_columns = ['wfh_trend','pid']


X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regression_dataset = regression_dataset.dropna()

aft = WeibullAFTFitter()
aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True,formula='wfh_trend')
logger.debug(aft.print_summary())


censored_subjects=data_x_numeric.join(data_y)

censored_subjects['wfh_trend'].fillna(0,inplace=True)

censored_subjects = censored_subjects.loc[censored_subjects[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]

y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','wfh_trend',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)


detn, label = detn_reader.read_nonresponse()

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


# prompt: only keep detn rows where duration between label==1 min(duration_days) and max(duration_days)
min_duration = detn.groupby(label)['duration_days'].min()[1]
max_duration = detn.groupby(label)['duration_days'].max()[1]

detn_filtered = detn[
    (detn['duration_days'] >= min_duration) &
    (detn['duration_days'] <= max_duration)
]

detn = detn_filtered.copy()
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


y_pred_proba_all,explainer,ag_features = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)

pid_probabilities[f'percentrank_{label}'] = pid_probabilities[f'probability_{label}'].rank(pct=True)
top_pct_pids = pid_probabilities[pid_probabilities[f'percentrank_{label}'] > TOP_PCT]['pid'].unique()
if EXPORT_SHAP_WATERFALL:
  json_series = export_waterfall_shap_values(explainer,detn[(detn['pid'].isin(active_pids)) & (detn['pid'].isin(top_pct_pids)) & (detn[label]== 0)],ag_features)
  pid_probabilities = pd.merge(pid_probabilities, json_series.rename(f'{label}_shap_data'), left_on='pid', right_index=True, how='left')


detn, label = detn_reader.read_status_dead()

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 11
MUAC_CUTOFF = 12.1
NULL_MUAC_LOS_CUTOFF = 4
DURATION_DAYS_CUTOFF = 101

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

detn = detn[(((detn['weekly_last_muac'].isnull()) & (detn['wk1_calc_los'] < NULL_MUAC_LOS_CUTOFF)) & (detn['duration_days'] < DURATION_DAYS_CUTOFF) | ((detn['weekly_last_muac'] < MUAC_CUTOFF) & (detn['wk1_calc_los'] < LOS_CUTOFF)))]

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


# prompt: clip lower boundary of detn['duration_days'] to 0

detn['duration_days'] = detn['duration_days'].clip(lower=0)

detn_admit_only,_,_,_ = split_detn_new_onset_medical_complication(detn,label)
logger.debug(f'{detn_admit_only.shape,detn_admit_only[label].sum()},{detn_admit_only[label].mean()}')

# Find 'pid' values in detn that are NOT in detn_admit_only
pid_not_in_admit = detn[~detn['pid'].isin(detn_admit_only['pid'])]['pid']

# Get rows from detn where 'pid' is in pid_not_in_admit
detn_filtered = detn[detn['pid'].isin(pid_not_in_admit)].copy()
detn_filtered.replace([np.inf, -np.inf], np.nan, inplace=True)


y_pred_proba_all1,explainer1,ag_features1 = run_ag_model(label,detn_admit_only,ADMIT_ONLY)

if EXPORT_SHAP_WATERFALL:
  json_series = export_waterfall_shap_values(explainer1,detn_admit_only[(detn_admit_only['pid'].isin(active_pids)) & (detn_admit_only[label]== 0)],ag_features1)
    
y_pred_proba_all2,explainer2,ag_features2 = run_ag_model(label,detn_filtered,NOT_ADMIT_ONLY)

y_pred_proba_all_stratified = pd.concat([y_pred_proba_all1,y_pred_proba_all2],axis=0)
y_pred_proba_all_stratified_series = y_pred_proba_all_stratified[1].rename(f'probability_{label}_stratified')
pid_probabilities = pid_probabilities.join(y_pred_proba_all_stratified_series)
logger.debug(pid_probabilities.shape)

pid_probabilities[f'percentrank_{label}_stratified'] = pid_probabilities[f'probability_{label}_stratified'].rank(pct=True)
top_pct_pids = pid_probabilities[pid_probabilities[f'percentrank_{label}_stratified'] > TOP_PCT]['pid'].unique()

if EXPORT_SHAP_WATERFALL:
  json_series2 = export_waterfall_shap_values(explainer2,detn_filtered[(detn_filtered['pid'].isin(active_pids)) & (detn_filtered['pid'].isin(top_pct_pids)) & (detn_filtered[label]== 0)],ag_features2)
  pid_probabilities = pd.merge(pid_probabilities, pd.concat([json_series,json_series2]).rename(f'{label}_shap_data'), left_on='pid', right_index=True, how='left')


# clip for better visualization and so AFT models will work, guaranteeing all durations >0
#max_duration = detn[detn[label]==1]['duration_days'].max() +1


detn['duration_days'] = detn['duration_days'].clip(lower=.01)


regr_cols = ['wk1_muac','muac',label,'duration_days','pid']
regression_dataset = detn[regr_cols].copy()
regression_dataset['wk1_muac'].fillna(regression_dataset['muac'],inplace=True)
regression_dataset.drop(columns=['muac'],inplace=True)

regression_dataset.dropna(inplace=True)


#regression_dataset['duration_days'] = regression_dataset['duration_days'].clip(lower=.01)
aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True, formula=' wk1_muac')

logger.debug(aft.print_summary())


regr_cols = ['wk1_muac','muac',label,'duration_days','pid']
censored_subjects = detn[regr_cols].copy()
censored_subjects['wk1_muac'].fillna(censored_subjects['muac'],inplace=True)
censored_subjects.drop(columns=['muac'],inplace=True)


censored_subjects.dropna(inplace=True)

censored_subjects = censored_subjects.loc[detn[label]==0].copy()

censored_subjects_last_obs = censored_subjects['duration_days']
censored_subjects_last_obs = censored_subjects_last_obs.clip(lower=0.01)

y_pred_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_pred_median.rename('median_predicted_days_to_death',inplace=True)

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)


df = pd.merge(censored_subjects[['pid',duration_days_col]],y_pred_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)

# TODO write pid_probabilities to Postgres table
#pid_probabilities.to_excel('pid_probabilities8.xlsx',index=None)
