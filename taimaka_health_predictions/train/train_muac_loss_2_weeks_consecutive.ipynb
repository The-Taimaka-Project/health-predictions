{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rG-dqLJfyAb"
   },
   "outputs": [],
   "source": [
    "# %load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54wUh8uiRFtj"
   },
   "source": [
    "uncomment and run below cell if running in Google Colab.  Make sure your secrets are configured in colab and permitted to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UX1gf3w1fyAd"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/The-Taimaka-Project/health-predictions.git\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/health-predictions')\n",
    "\n",
    "# !pip install boto3\n",
    "# !pip install autogluon.tabular\n",
    "# !pip install shap\n",
    "\n",
    "# import os\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"TAIMAKA_DO_ACCESS_KEY\"] = userdata.get('TAIMAKA_DO_ACCESS_KEY')\n",
    "# os.environ[\"TAIMAKA_DO_SECRET_KEY\"] = userdata.get('TAIMAKA_DO_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyh8tgF5fyAe"
   },
   "outputs": [],
   "source": [
    "# local environment, set up virtual environment\n",
    "# python -m venv .venv\n",
    "# . .venv/bin/activate\n",
    "# then\n",
    "# pip install -r requirements.txt\n",
    "#\n",
    "# or\n",
    "#\n",
    "# pip install jupyter\n",
    "# pip install autogluon.tabular\n",
    "# pip install lightgbm\n",
    "# pip install xgboost\n",
    "# pip install shap\n",
    "\n",
    "\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fho7zTIPR1kS"
   },
   "source": [
    "if running locally, make sure you run secrets env assignments before running the following cells.  I run the assignments in a separate py file and connect to the kernel running this notebook.\n",
    "\n",
    "\n",
    "```\n",
    "%env TAIMAKA_DO_ACCESS_KEY=your access key\n",
    "%env TAIMAKA_DO_SECRET_KEY=your secret key\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHXjOB_drqs6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, PrecisionRecallDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from taimaka_health_predictions.inference.util import (\n",
    "    DetnReaderWriter,\n",
    "    ag_feature_generator,\n",
    "    drop_feature_columns,\n",
    "    gbm_shap,\n",
    "    lightgbm_train,\n",
    "    select_features,\n",
    "    split_detn_new_onset_medical_complication,\n",
    "    strip_column_names,\n",
    ")\n",
    "from taimaka_health_predictions.utils.digitalocean import DigitalOceanStorage\n",
    "from taimaka_health_predictions.utils.globals import ETL_DIR, MODEL_DIR, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxVY_gL4ooJ3"
   },
   "outputs": [],
   "source": [
    "# run secrets first to set the environment variables for your credentials\n",
    "do_storage = DigitalOceanStorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OS7dmhE9Kl9"
   },
   "source": [
    "# get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d6M-ZeSppDz"
   },
   "outputs": [],
   "source": [
    "detn_reader = DetnReaderWriter()\n",
    "detn, label = detn_reader.read_muac_loss_2_weeks_consecutive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px-KgdTt5elV"
   },
   "source": [
    "drop the rows that are ineligible for muac loss, 2 weeks consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k5n38v45okU"
   },
   "outputs": [],
   "source": [
    "detn['wk1_calc_los'].fillna(0,inplace=True)\n",
    "\n",
    "LOS_CUTOFF = 12\n",
    "MUAC_CUTOFF = 12.7\n",
    "logger.info(f'event sum: {detn[label].sum()}, mean: {detn[label].mean()},shape: {detn.shape}')\n",
    "detn = detn[((detn['wk1_b_discharged']==0) & (detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF)) ].copy()\n",
    "logger.info(f'event sum: {detn[label].sum()}, mean: {detn[label].mean()},shape: {detn.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waSllr3CU-wf"
   },
   "outputs": [],
   "source": [
    "columns_to_explicitly_delete = {'household_adults','household_slept','living_children','weekly_avg_muac','weekly_last_wfh','wk1_muac_diff_rate','muac_diff_ratio_rate','muac_diff_ratio'}\n",
    "\n",
    "columns_to_keep = {\n",
    "    \"b_referred_emergency\",\n",
    "    \"b_wast_admit\",\n",
    "    \"cg_age\",\n",
    "    \"enr_age\",\n",
    "    \"wk1_age\",\n",
    "    \"wk1_b_wast\",\n",
    "}\n",
    "\n",
    "detn_filtered = drop_feature_columns(\n",
    "    detn,\n",
    "    label,\n",
    "    drop_muac=False,\n",
    "    drop_weight=False,\n",
    "    drop_height=False,\n",
    "    columns_to_keep=columns_to_keep,\n",
    "    columns_to_explicitly_delete=columns_to_explicitly_delete,\n",
    ")\n",
    "logger.info(detn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40qbJvWg9VSX"
   },
   "source": [
    "# LightGBM iteration for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFP4SyqU5pok"
   },
   "outputs": [],
   "source": [
    "X = detn.drop(columns=label)\n",
    "y = detn[label]\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz-5Kucw6Ifr"
   },
   "outputs": [],
   "source": [
    "X_train_transformed, X_test_transformed = ag_feature_generator(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p56LhtV36m3U"
   },
   "outputs": [],
   "source": [
    "gbm, f1_scored, aic, top_features,average_precision = lightgbm_train(\n",
    "    X_train_transformed, X_test_transformed, y_train, y_test\n",
    ")\n",
    "print(len(gbm.feature_name_), f1_scored,average_precision)\n",
    "\n",
    "X_train_transformed_top = X_train_transformed[top_features].copy()\n",
    "X_test_transformed_top = X_test_transformed[top_features].copy()\n",
    "gbm, f1_scored, aic, top_features,average_precision = lightgbm_train(\n",
    "    X_train_transformed_top, X_test_transformed_top, y_train, y_test\n",
    ")\n",
    "print(len(gbm.feature_name_), f1_scored,average_precision)\n",
    "\n",
    "best_gbm, best_features, results_df, best_aic, features = select_features(\n",
    "    gbm, X_train_transformed_top, X_test_transformed_top, y_train, y_test, 30, 0, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmX_uYNpURhQ"
   },
   "source": [
    "the most important part!  Set N_FEATURES to the number of features you want.  Maximize the f1 score but minimize the number of features.  \n",
    "\n",
    "If you want to see what the 10 features selection would be you can run this cell:\n",
    "```\n",
    "print(features[10])\n",
    "```\n",
    "if you want to compare what was removed from the 10th set to get the 9th, you can run a cell like:\n",
    "```\n",
    "print(set(features[10]) - set(features[9]))\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4kg_sqn6_nz"
   },
   "outputs": [],
   "source": [
    "print(best_aic, \"\\n\", best_features, len(best_features))\n",
    "#results_df.sort_values(by=\"AIC\", ascending=True)\n",
    "results_df.sort_values(by=\"avg_precision\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b2YLJ977osX"
   },
   "outputs": [],
   "source": [
    "N_FEATURES = 14\n",
    "print(N_FEATURES, features[N_FEATURES])\n",
    "\n",
    "top_features = [\n",
    "    col for col in strip_column_names(features[N_FEATURES]) if col in detn.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCBuukFxV48z"
   },
   "source": [
    "try and get the columns to be independent of one another.  There should be few, if any, clustering bars on the right side of the second graph.  \n",
    "\n",
    "One technique to remove the bars is to combine the clustered features via the reduce_dimensionality method.  Make sure to modify the DetnReaderWriter read_new_onset_medical_complication() method to do this.  Then drop the dimensioned columns in the drop_columns method in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX8XnRIcfyAk"
   },
   "outputs": [],
   "source": [
    "gbm_shap(features,N_FEATURES,X_train_transformed,X_test_transformed,X_test_transformed_top,y_train,y_test,cutoff=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W--uI-CF8rPs"
   },
   "source": [
    "# AutoGluon Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8sOTLJ68dl8"
   },
   "outputs": [],
   "source": [
    "# prompt: train test split admit_raw using column y_detn_ever as y\n",
    "\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = detn[top_features]\n",
    "y = detn[label]\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdJOwrtP82sA"
   },
   "outputs": [],
   "source": [
    "AG_PATH = f\"AutogluonModels/{label}\"\n",
    "train_data = TabularDataset(X_train.join(y_train))\n",
    "# sometimes a good quality model generalizes better than a medium quality one does, depends on the data\n",
    "MEDIUM_QUALITY_MODE = True\n",
    "\n",
    "if MEDIUM_QUALITY_MODE == True:\n",
    "    preset = 'medium_quality'\n",
    "    time_lim = 300\n",
    "else:\n",
    "    preset = 'good_quality'\n",
    "    time_lim = 600\n",
    "\n",
    "predictor = TabularPredictor(label=label, eval_metric=\"average_precision\", path=AG_PATH).fit(\n",
    "    train_data, time_limit=time_lim, presets=preset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTGofoba91K3"
   },
   "source": [
    "## evaluate AG model on holdout (i.e., test) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qchia-w99Tn"
   },
   "outputs": [],
   "source": [
    "test_data = TabularDataset(X_test.join(y_test))\n",
    "predictor.calibrate_decision_threshold()\n",
    "y_pred = predictor.predict(test_data.drop(columns=[label]))\n",
    "print(predictor.evaluate(test_data, silent=True))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "f1_scored = f1_score(y_test, y_pred)\n",
    "y_pred_proba = predictor.predict_proba(test_data.drop(columns=[label]))[1]\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "print('f1: ',f1_scored, 'average precision: ', avg_precision)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, y_pred_proba, plot_chance_level=True)\n",
    "_ = display.ax_.set_title(f\"Precision-Recall Curve for {label} \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieu2ZGB9AEDk"
   },
   "source": [
    "## feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia3c3HKlAIyU"
   },
   "outputs": [],
   "source": [
    "autogluon_feature_importance = predictor.feature_importance(\n",
    "    test_data, subsample_size=1000, time_limit=400\n",
    ")\n",
    "autogluon_feature_importance[\"cumsum\"] = (\n",
    "    autogluon_feature_importance[\"importance\"].cumsum()\n",
    "    / autogluon_feature_importance[\"importance\"].sum()\n",
    ")\n",
    "autogluon_feature_importance[\"importance_ratio\"] = (\n",
    "    autogluon_feature_importance[\"importance\"]\n",
    "    / autogluon_feature_importance[\"importance\"].sum()\n",
    ")\n",
    "autogluon_feature_importance[[\"cumsum\", \"importance_ratio\"]]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot on the primary y-axis\n",
    "autogluon_feature_importance_filtered = autogluon_feature_importance[\n",
    "    autogluon_feature_importance[\"importance\"] > 0\n",
    "]\n",
    "ax1.barh(\n",
    "    autogluon_feature_importance_filtered.index,\n",
    "    autogluon_feature_importance_filtered[\"importance_ratio\"],\n",
    "    label=\"Importance Ratio\",\n",
    ")\n",
    "ax1.set_xlabel(\"Importance\")\n",
    "ax1.set_ylabel(\"Features\")\n",
    "ax1.set_title(\"Feature Importance with Cumulative Sum\")\n",
    "ax1.legend(loc=\"upper left\")  # specify location for the first legend\n",
    "ax1.grid(True, axis=\"x\")  # gridlines only on the x-axis for the bar plot\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Create a secondary y-axis\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "# Line plot on the secondary y-axis\n",
    "ax2.plot(\n",
    "    autogluon_feature_importance_filtered[\"cumsum\"],\n",
    "    autogluon_feature_importance_filtered.index,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"red\",\n",
    "    label=\"Cumulative Sum\",\n",
    ")\n",
    "ax2.set_xlabel(\"Cumulative Sum\")\n",
    "ax2.legend(loc=\"upper right\")  # specify location for the second legend\n",
    "\n",
    "# Improve layout\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7h3pMq09eLL"
   },
   "source": [
    "## export the AG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wh0cHz38_YBV"
   },
   "outputs": [],
   "source": [
    "VERSION = \"0.1.0\"\n",
    "\n",
    "metadata = {\n",
    "    \"f1\": f1_scored,\n",
    "    \"average precision\": avg_precision,\n",
    "    \"AG quality\": preset,\n",
    "    \"AG time limit\": time_lim,    \n",
    "    \"version\": VERSION,\n",
    "    \"inputs\": autogluon_feature_importance.sort_values(\n",
    "        by=\"importance\", ascending=False\n",
    "    ).index.tolist(),\n",
    "    \"outputs\": f\"chance of {label}\",\n",
    "    \"description\": (\n",
    "        f\"Predicts chance of {label} given the latest 3 weeks of patient weekly (raw and processed) data plus their admission data.\"\n",
    "    ),\n",
    "    \"feature_engineering\": (\n",
    "        \"wfh_diff_ratio_rate is the change in wfh per kg weight per day using the first and last wfh, wk1_muac_diff_weekly is the difference between most recent visit and prior visit muac\"\n",
    "    ),\n",
    "    \"contact\": \"Brian Chaplin\",\n",
    "}\n",
    "\n",
    "path = f\"{MODEL_DIR}{label}/{VERSION}/model.tar.gz\"\n",
    "\n",
    "do_storage.to_autogluon_tarball(predictor, model_metadata=metadata, path=path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
