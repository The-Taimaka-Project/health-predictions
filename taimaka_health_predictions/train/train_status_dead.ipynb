{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNOndj0ZoIzB"
      },
      "outputs": [],
      "source": [
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncomment and run below cell if running in Google Colab.  Make sure your secrets are configured in colab and permitted to this notebook."
      ],
      "metadata": {
        "id": "54wUh8uiRFtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_rLfHaloIzE"
      },
      "outputs": [],
      "source": [
        "#!pip install boto3\n",
        "#!pip install autogluon.tabular\n",
        "#!pip install shap\n",
        "\n",
        "# !git clone https://github.com/The-Taimaka-Project/health-predictions.git\n",
        "\n",
        "#import sys\n",
        "#sys.path.append('/content/health-predictions')\n",
        "\n",
        "\n",
        "#import os\n",
        "#from google.colab import userdata\n",
        "#os.environ[\"TAIMAKA_DO_ACCESS_KEY\"] = userdata.get('TAIMAKA_DO_ACCESS_KEY')\n",
        "#os.environ[\"TAIMAKA_DO_SECRET_KEY\"] = userdata.get('TAIMAKA_DO_SECRET_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e4_1XunoIzF"
      },
      "outputs": [],
      "source": [
        "# local environment, set up virtual environment\n",
        "# python -m venv .venv\n",
        "# . .venv/bin/activate\n",
        "# then\n",
        "# pip install -r requirements.txt\n",
        "#\n",
        "# or\n",
        "#\n",
        "# pip install jupyter\n",
        "# pip install autogluon.tabular\n",
        "# pip install lightgbm\n",
        "# pip install xgboost\n",
        "# pip install shap\n",
        "\n",
        "# does nothing in Google Colab but necessary if running locally\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if running locally, make sure you run secrets env assignments before running the following cells.  I run the assignments in a separate py file and connect to the kernel running this notebook.\n",
        "\n",
        "\n",
        "```\n",
        "%env TAIMAKA_DO_ACCESS_KEY=your access key\n",
        "%env TAIMAKA_DO_SECRET_KEY=your secret key\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Fho7zTIPR1kS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHXjOB_drqs6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from warnings import simplefilter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from taimaka_health_predictions.inference.util import (\n",
        "    DetnReaderWriter,\n",
        "    ag_feature_generator,\n",
        "    drop_feature_columns,\n",
        "    gbm_shap,\n",
        "    lightgbm_train,\n",
        "    select_features,\n",
        "    split_detn_new_onset_medical_complication,\n",
        "    strip_column_names,\n",
        ")\n",
        "from taimaka_health_predictions.utils.digitalocean import DigitalOceanStorage\n",
        "from taimaka_health_predictions.utils.globals import ETL_DIR, MODEL_DIR, ADMIT_ONLY, NOT_ADMIT_ONLY, logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxVY_gL4ooJ3"
      },
      "outputs": [],
      "source": [
        "# run secrets first to set the environment variables for your credentials\n",
        "do_storage = DigitalOceanStorage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OS7dmhE9Kl9"
      },
      "source": [
        "# get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d6M-ZeSppDz"
      },
      "outputs": [],
      "source": [
        "detn_reader = DetnReaderWriter()\n",
        "detn, label = detn_reader.read_status_dead()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this notebook trains both strata of the death model\n",
        "\n",
        "\n",
        "1.   death occurs on the first visit, detn_admit_only\n",
        "2.   event occurs on the second or beyond visit, detn_filtered\n",
        "\n",
        "uncomment/comment out the train split steps as appropriate and use the gbm feature selection and AG train steps with either\n",
        "\n"
      ],
      "metadata": {
        "id": "dBMMQe3UO_N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOS_CUTOFF = 11\n",
        "MUAC_CUTOFF = 12.1\n",
        "NULL_MUAC_LOS_CUTOFF = 4\n",
        "DURATION_DAYS_CUTOFF = 101\n",
        "\n",
        "logger.info(f'rate{detn[label].mean()},count {detn[label].sum()},shape{detn.shape}')\n",
        "\n",
        "detn = detn[(((detn['weekly_last_muac'].isnull()) & (detn['wk1_calc_los'] < NULL_MUAC_LOS_CUTOFF)) & (detn['duration_days'] < DURATION_DAYS_CUTOFF) | ((detn['weekly_last_muac'] < MUAC_CUTOFF) & (detn['wk1_calc_los'] < LOS_CUTOFF)))]\n",
        "\n",
        "logger.info(f'rate{detn[label].mean()},count {detn[label].sum()},shape{detn.shape}')\n"
      ],
      "metadata": {
        "id": "z5OAmBaRu6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns(detn_filtered):\n",
        "  print(detn_filtered.shape)\n",
        "  columns_to_explicitly_delete = {'muac_diff_ratio','muac','household_adults','household_,slept','living_children','resp_rate', 'temperature','weekly_avg_muac','weekly_last_wfh'\n",
        "    'wfa_trend','hfa_trend','cat1_complications_weekly','admit_cat1_complications','wk1_rainy_season_weekly','lean_season_admit','wfh_rsquared','wfh_trend','status','status_date', 'final_date',\n",
        "    'wk1_calcdate_weekly','wk2_calcdate_weekly','wk3_calcdate_weekly'}\n",
        "  columns_to_keep = {\n",
        "    \"b_referred_emergency\",\n",
        "    \"b_wast_admit\",\n",
        "    \"cg_age\",\n",
        "    \"enr_age\",\n",
        "    \"wk1_age\",\n",
        "    \"wk1_b_wast\",\n",
        "  }\n",
        "\n",
        "  detn_filtered = drop_feature_columns(\n",
        "    detn_filtered,\n",
        "    label,\n",
        "    drop_muac=False,\n",
        "    drop_weight=False,\n",
        "    drop_height=False,\n",
        "    columns_to_keep=columns_to_keep,\n",
        "    columns_to_explicitly_delete=columns_to_explicitly_delete\n",
        "  )\n",
        "  print(detn_filtered.shape)"
      ],
      "metadata": {
        "id": "n0TX4eSkNzhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFNEwdWjuA66"
      },
      "outputs": [],
      "source": [
        "detn_admit_only, _, _, _ = split_detn_new_onset_medical_complication(detn, label)\n",
        "pid_not_in_admit = detn[~detn[\"pid\"].isin(detn_admit_only[\"pid\"])][\"pid\"]\n",
        "\n",
        "# Get rows from detn where 'pid' is in pid_not_in_admit\n",
        "detn_filtered = detn[detn[\"pid\"].isin(pid_not_in_admit)].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_columns(detn_filtered)\n",
        "drop_columns(detn_admit_only)\n",
        "detn_admit_only.drop(columns=[col for col in detn_admit_only.columns if 'rsquared' in col],inplace=True)\n",
        "detn_admit_only.drop(columns=[col for col in detn_admit_only.columns if 'trend' in col],inplace=True)"
      ],
      "metadata": {
        "id": "6tyMCiCjO1-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40qbJvWg9VSX"
      },
      "source": [
        "# LightGBM iteration for feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "point X and y to either detn_admit_only or detn_filtered, depending on which strata you're training.  Just uncomment and comment out the X and y assignment lines appropriately."
      ],
      "metadata": {
        "id": "4Eggep5VPvwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFP4SyqU5pok"
      },
      "outputs": [],
      "source": [
        "# prompt: train test split admit_raw using label column as y\n",
        "# Separate features (X) and target (y)\n",
        "\n",
        "X = detn_admit_only.drop(columns=label)\n",
        "y = detn_admit_only[label]\n",
        "\n",
        "# X = detn_filtered.drop(columns=label)\n",
        "# y = detn_filtered[label]\n",
        "\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")  # Adjust test_size and random_state as needed\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz-5Kucw6Ifr"
      },
      "outputs": [],
      "source": [
        "X_train_transformed, X_test_transformed = ag_feature_generator(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p56LhtV36m3U"
      },
      "outputs": [],
      "source": [
        "gbm, f1_scored, aic, top_features = lightgbm_train(\n",
        "    X_train_transformed, X_test_transformed, y_train, y_test\n",
        ")\n",
        "print(len(gbm.feature_name_), f1_scored)\n",
        "\n",
        "X_train_transformed_top = X_train_transformed[top_features].copy()\n",
        "X_test_transformed_top = X_test_transformed[top_features].copy()\n",
        "gbm, f1_scored, aic, top_features = lightgbm_train(\n",
        "    X_train_transformed_top, X_test_transformed_top, y_train, y_test\n",
        ")\n",
        "print(len(gbm.feature_name_), f1_scored)\n",
        "\n",
        "best_gbm, best_features, results_df, best_aic, features = select_features(\n",
        "    gbm, X_train_transformed_top, X_test_transformed_top, y_train, y_test, 30, 0, -1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4kg_sqn6_nz"
      },
      "outputs": [],
      "source": [
        "print(best_aic, \"\\n\", best_features, len(best_features))\n",
        "results_df.sort_values(by=\"AIC\", ascending=True)\n",
        "#results_df.sort_values(by=\"f1_score\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the most important part!  Set N_FEATURES to the number of features you want.  Maximize the f1 score but minimize the number of features.  \n",
        "\n",
        "If you want to see what the 10 features selection would be you can run this cell:\n",
        "```\n",
        "print(features[10])\n",
        "```\n",
        "if you want to compare what was removed from the 10th set to get the 9th, you can run a cell like:\n",
        "```\n",
        "print(set(features[10]) - set(features[9]))\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lmX_uYNpURhQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b2YLJ977osX"
      },
      "outputs": [],
      "source": [
        "N_FEATURES = 16\n",
        "print(N_FEATURES, features[N_FEATURES])\n",
        "\n",
        "top_features = [\n",
        "    col for col in strip_column_names(features[N_FEATURES]) if col in detn.columns\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try and get the columns to be independent of one another.  There should be few, if any, clustering bars on the right side of the second graph.  \n",
        "\n",
        "One technique to remove the bars is to combine the clustered features via the reduce_dimensionality method.  Make sure to modify the DetnReaderWriter read_new_onset_medical_complication() method to do this.  Then drop the dimensioned columns in the drop_columns method in this notebook.\n"
      ],
      "metadata": {
        "id": "zCBuukFxV48z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCS5q8LDoIzK"
      },
      "outputs": [],
      "source": [
        "gbm_shap(features,N_FEATURES,X_train_transformed,X_test_transformed,X_test_transformed_top,y_train,y_test,cutoff=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W--uI-CF8rPs"
      },
      "source": [
        "# AutoGluon Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "point X and y to either detn_admit_only or detn_filtered, depending on which strata you're training.  Just uncomment and comment out the X and y assignment lines appropriately.  (This MUST match what was done in the previous train test split cell for gbm training.)"
      ],
      "metadata": {
        "id": "SqzAaGFdQGNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8sOTLJ68dl8"
      },
      "outputs": [],
      "source": [
        "# prompt: train test split admit_raw using label column as y\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "\n",
        "X = detn_admit_only[top_features]\n",
        "y = detn_admit_only[label]\n",
        "\n",
        "# X = detn_filtered[top_features]\n",
        "# y = detn_filtered[label]\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=43\n",
        ")  # Adjust test_size and random_state as needed\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdJOwrtP82sA"
      },
      "outputs": [],
      "source": [
        "AG_PATH = f\"AutogluonModels/{label}\"\n",
        "train_data = TabularDataset(X_train.join(y_train))\n",
        "predictor = TabularPredictor(label=label, eval_metric=\"f1\", path=AG_PATH).fit(\n",
        "    train_data, time_limit=300, presets=\"medium_quality\"\n",
        ")\n",
        "# predictor = TabularPredictor(label=label,eval_metric='f1',path=AG_PATH).fit(train_data,time_limit=600,presets='good_quality')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTGofoba91K3"
      },
      "source": [
        "## evaluate AG model on holdout (i.e., test) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qchia-w99Tn"
      },
      "outputs": [],
      "source": [
        "test_data2 = TabularDataset(X_test.join(y_test))\n",
        "predictor.calibrate_decision_threshold()\n",
        "y_pred = predictor.predict(test_data2.drop(columns=[label]))\n",
        "print(predictor.evaluate(test_data2, silent=True))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f1_score(y_test, y_pred))\n",
        "# 0.1.0 detn_filtered is 0.7999505630313477"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieu2ZGB9AEDk"
      },
      "source": [
        "## feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia3c3HKlAIyU"
      },
      "outputs": [],
      "source": [
        "autogluon_feature_importance = predictor.feature_importance(\n",
        "    test_data2, subsample_size=1000, time_limit=400\n",
        ")\n",
        "autogluon_feature_importance[\"cumsum\"] = (\n",
        "    autogluon_feature_importance[\"importance\"].cumsum()\n",
        "    / autogluon_feature_importance[\"importance\"].sum()\n",
        ")\n",
        "autogluon_feature_importance[\"importance_ratio\"] = (\n",
        "    autogluon_feature_importance[\"importance\"]\n",
        "    / autogluon_feature_importance[\"importance\"].sum()\n",
        ")\n",
        "autogluon_feature_importance[[\"cumsum\", \"importance_ratio\"]]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar plot on the primary y-axis\n",
        "autogluon_feature_importance_filtered = autogluon_feature_importance[\n",
        "    autogluon_feature_importance[\"importance\"] > 0\n",
        "]\n",
        "ax1.barh(\n",
        "    autogluon_feature_importance_filtered.index,\n",
        "    autogluon_feature_importance_filtered[\"importance_ratio\"],\n",
        "    label=\"Importance Ratio\",\n",
        ")\n",
        "ax1.set_xlabel(\"Importance\")\n",
        "ax1.set_ylabel(\"Features\")\n",
        "ax1.set_title(\"Feature Importance with Cumulative Sum\")\n",
        "ax1.legend(loc=\"upper left\")  # specify location for the first legend\n",
        "ax1.grid(True, axis=\"x\")  # gridlines only on the x-axis for the bar plot\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# Create a secondary y-axis\n",
        "ax2 = ax1.twiny()\n",
        "\n",
        "# Line plot on the secondary y-axis\n",
        "ax2.plot(\n",
        "    autogluon_feature_importance_filtered[\"cumsum\"],\n",
        "    autogluon_feature_importance_filtered.index,\n",
        "    marker=\"o\",\n",
        "    linestyle=\"-\",\n",
        "    color=\"red\",\n",
        "    label=\"Cumulative Sum\",\n",
        ")\n",
        "ax2.set_xlabel(\"Cumulative Sum\")\n",
        "ax2.legend(loc=\"upper right\")  # specify location for the second legend\n",
        "\n",
        "# Improve layout\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7h3pMq09eLL"
      },
      "source": [
        "## export the AG model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment/uncomment the path assignment depending on which strata you're training.  This MUST match what the 2 train test split cells were set to."
      ],
      "metadata": {
        "id": "5gR5yNyyQNCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh0cHz38_YBV"
      },
      "outputs": [],
      "source": [
        "VERSION = \"0.1.0\"\n",
        "\n",
        "metadata = {\n",
        "    \"version\": VERSION,\n",
        "    \"inputs\": autogluon_feature_importance.sort_values(\n",
        "        by=\"importance\", ascending=False\n",
        "    ).index.tolist(),\n",
        "    \"outputs\": \"chance of death\",\n",
        "    \"description\": (\n",
        "        \"Predicts chance of death\"\n",
        "    ),\n",
        "    \"feature_engineering\": (\n",
        "        \"wfh_trend_z is the PCA dimensionalized reduction of normalized ['wfh_rsquared','wfh_trend'] the r-squared and the slope of the linear regression line of the wfh for the patient's history\"\n",
        "    ),\n",
        "    \"contact\": \"Brian Chaplin\",\n",
        "}\n",
        "\n",
        "# use this for strata 1, admit only\n",
        "path = f\"{MODEL_DIR}{label}{ADMIT_ONLY}/{VERSION}/model.tar.gz\"\n",
        "\n",
        "# use this for strata 2, non-admit only (detn_filtered)\n",
        "#path = f\"{MODEL_DIR}{label}{NOT_ADMIT_ONLY}/{VERSION}/model.tar.gz\"\n",
        "\n",
        "\n",
        "do_storage.to_autogluon_tarball(predictor, model_metadata=metadata, path=path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}