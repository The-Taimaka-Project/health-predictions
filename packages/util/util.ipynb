{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def make_populated_column(detn,variable):\n",
        "  detn[f'{variable}_populated'] = detn[variable].notnull().astype(int)\n",
        "  return detn,f'{variable}_populated'"
      ],
      "metadata": {
        "id": "1Rwcvxl2TEHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find columns in 'df' with nunique between 3 and 10 and aren't boolean\n",
        "def make_categorical(df):\n",
        "  cols_to_check = df.select_dtypes(exclude=['bool','number'])\n",
        "  result_cols = [col for col in cols_to_check.columns if 2 <= df[col].nunique() <= 10]\n",
        "  for col in result_cols:\n",
        "    df[col] = df[col].astype('category')\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "VC_K1RGc-4nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_anthros(detn):\n",
        "  remove_anthros_keep_wk1_muac(detn,False)\n"
      ],
      "metadata": {
        "id": "sHDZuauVjj6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_anthros_keep_wk1_muac(detn,keep_wk1_muac=False):\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wfh' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'calc_los' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wfa' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'hl' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'weight' in col  and col != 'detn_weight_loss_ever'],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'hfa' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'date' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'time' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'day' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'week' in col and col != 'muac_loss_2_weeks_consecutive'],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'give' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'visitnum' in col],inplace=True)\n",
        "  #detn.drop(columns=[col for col in detn.columns if 'h1' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'vd' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'age' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'site' in col],inplace=True)\n",
        "  #detn.drop(columns=[col for col in detn.columns if 'h1' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'maln' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'contprogram' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'cornulc' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'excluded' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'mamneedsitp' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'possexclucrit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'twinpid' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'b_wast' in col],inplace=True)\n",
        "  if keep_wk1_muac:\n",
        "    detn.drop(columns=[col for col in detn.columns if 'muac' in col and col != 'muac_loss_2_weeks_consecutive' and col!= 'wk1_muac'],inplace=True)\n",
        "  else:\n",
        "    detn.drop(columns=[col for col in detn.columns if 'muac' in col and col != 'muac_loss_2_weeks_consecutive'],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'random' in col],inplace=True)\n",
        "\n",
        "  # prompt: find all single-valued columns in detn\n",
        "  # Find single-valued columns in detn\n",
        "  single_valued_cols = [col for col in detn.columns if detn[col].nunique() <= 1]\n",
        "  detn.drop(columns=single_valued_cols,inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'doneses' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pidscannable' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'attachments' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'rutf' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'end_time' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'endtime' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submissiondate' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'name' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pp_cm' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'starttime' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submission_date' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'start_time' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'last_admit' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'c_assigned_cm' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'photo' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'picture' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'drug_record' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'first_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'site_admit' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'sequence_num' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'form_version' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'dose' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submitterid' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'dischqualanthro' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pull_lastms' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'row_count' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submitter_id' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wasreferred' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pull_lastms' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'eff_ref' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pull_lastms' in col],inplace=True)\n",
        "\n",
        "  detn.drop(columns=[col for col in detn.columns if 'device' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'itpotp' in col],inplace=True)\n"
      ],
      "metadata": {
        "id": "R3sCxiuTv_m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_phq_score(admit_current_mh,admit_current,detn):\n",
        "  import statsmodels.api as sm\n",
        "  import pandas as pd\n",
        "  df = admit_current_mh[['age_takewater','rainy_season','phq_score']].copy()\n",
        "\n",
        "\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  # Split the data into independent variables (X) and the dependent variable (y)\n",
        "  X = df[['age_takewater','rainy_season']]\n",
        "  y = df['phq_score']\n",
        "\n",
        "\n",
        "  # Add a constant to the independent variables for the intercept term\n",
        "  X = sm.add_constant(X)\n",
        "\n",
        "\n",
        "  # Fit the linear regression model\n",
        "  model = sm.OLS(y, X).fit()\n",
        "\n",
        "  admit_current['phq_predicted']=model.predict(sm.add_constant(admit_current[['age_takewater','rainy_season']]))\n",
        "\n",
        "  admit_current = pd.merge(admit_current,admit_current_mh[['pid','phq_score']],on='pid',how='left')\n",
        "\n",
        "  admit_current['phq_score'].fillna(admit_current['phq_predicted'],inplace=True)\n",
        "  admit_current['phq_score'].fillna(admit_current_mh['phq_score'].mean(),inplace=True)\n",
        "  detn = pd.merge(detn,admit_current[['pid','phq_score']],on='pid',how='left')\n",
        "  return detn\n"
      ],
      "metadata": {
        "id": "RFezJ8JAzufB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_collinear_columns(df, threshold=0.99,col_ct_threshold=100):\n",
        "  \"\"\"\n",
        "  This function is designed to identify columns within a Pandas DataFrame (df) that are highly correlated with each other, meaning they carry very similar information. This is often referred to as collinearity.\n",
        "\n",
        "    Args:\n",
        "      df: The Pandas DataFrame to analyze.\n",
        "      threshold: The minimum correlation value between two columns to be considered collinear (defaults to 0.99).\n",
        "      col_ct_threshold: The minimum number of non-null values a column must have to be included in the analysis (defaults to 100).\n",
        "\n",
        "    Returns:\n",
        "        just prints\n",
        "  \"\"\"\n",
        "  # Set a threshold for collinearity (e.g., 0.9)\n",
        "\n",
        "  numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "  # Find columns in weekly_joined with more than 20000 count\n",
        "  column_counts = df[numeric_cols].count()\n",
        "\n",
        "  # prompt: use corr() to find columns in weekly_joined that are collinear\n",
        "\n",
        "  # Find highly correlated columns in weekly_raw\n",
        "  correlation_matrix = df[column_counts[column_counts > col_ct_threshold].index.tolist()].corr()\n",
        "\n",
        "\n",
        "  # Find columns where the absolute correlation is above the threshold\n",
        "  collinear_columns = correlation_matrix[\n",
        "    (correlation_matrix > threshold) & (correlation_matrix != 1.0)\n",
        "  ].stack().index.tolist()\n",
        "\n",
        "\n",
        "  # Print the collinear columns\n",
        "  print(f\"Collinear columns in passed dataframe:\")\n",
        "  for col1, col2 in collinear_columns:\n",
        "    print(f\"- {col1} and {col2} (Correlation: {correlation_matrix.loc[col1, col2]:.2f})\")"
      ],
      "metadata": {
        "id": "eDOd6c3iiSmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_gbm_model(gbm5,detn5,idx,iloc,gbm_features,label):\n",
        "  import shap\n",
        "  import matplotlib.pyplot as plt\n",
        "  explainer_gbm2 = shap.TreeExplainer(gbm5)\n",
        "  shap_values_gbm2 = explainer_gbm2.shap_values(detn5[gbm_features])\n",
        "\n",
        "\n",
        "  values_to_display = detn5.loc[idx][['pid',label]]\n",
        "  plt.text(0.3, 0.1, f'{values_to_display}', transform=plt.gca().transAxes)\n",
        "\n",
        "  exp2 = shap.Explanation(shap_values_gbm2[iloc],\n",
        "                       explainer_gbm2.expected_value,\n",
        "                       data=detn5[gbm_features].loc[idx].values,\n",
        "                       feature_names=gbm_features)\n",
        "\n",
        "  log_odds = exp2.values.sum() + exp2.base_values\n",
        "  probability = log_odds_to_probability(log_odds)\n",
        "  plt.text(0.3, 0.5, 'LightGBM model', transform=plt.gca().transAxes)\n",
        "  plt.text(0.3, 0.3, f\"The probability of {label}\\ncorresponding to log odds {log_odds:.3f} is: {probability:.2%}\", transform=plt.gca().transAxes)\n",
        "\n",
        "  shap.plots.waterfall(exp2)  # Pass the Explanation object to waterfall plot\n"
      ],
      "metadata": {
        "id": "Ice_YaHUM3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_detn_new_onset_medical_complication(detn,label):\n",
        "  import pandas as pd\n",
        "\n",
        "  # split detn into 4 very different subsets:\n",
        "  # Use .copy() to ensure detn_admit_only is a copy, not a view\n",
        "  detn_admit_only = detn[(detn['wk1_calcdate_weekly'].isnull()) &\n",
        "  (detn['wk2_calcdate_weekly'].isnull()) & (detn['wk3_calcdate_weekly'].isnull())].copy()\n",
        "  cat1_cols = [col for col in detn.columns if col.startswith('y_')]\n",
        "  # Use .loc to avoid chained indexing and to fill na values\n",
        "  detn_admit_only.loc[:, cat1_cols] = detn_admit_only.loc[:, cat1_cols].fillna(0)\n",
        "\n",
        "  # prompt: rows where  detn[detn['wk2_calcdate_weekly'].isnull()] and pid not in detn_admit_only['pid']\n",
        "  # Use .copy() to ensure detn_wk1_only is a copy, not a view\n",
        "  detn_wk1_only = detn[(detn['wk1_calcdate_weekly'].notnull()) & (detn['wk2_calcdate_weekly'].isnull()) & (detn['wk3_calcdate_weekly'].isnull())].copy()\n",
        "\n",
        "  # prompt: rows where  detn[detn['wk3_calcdate_weekly'].isnull()] and pid not in detn_admit_only['pid'] and pid not in detn_wk1_only\n",
        "  # Use .copy() to ensure detn_wk2 is a copy, not a view\n",
        "  detn_wk2 = detn[(detn['wk1_calcdate_weekly'].notnull()) & (detn['wk2_calcdate_weekly'].notnull()) & (detn['wk3_calcdate_weekly'].isnull())].copy()\n",
        "\n",
        "  # prompt: rows where  detn[detn['wk3_calcdate_weekly'].isnull()] and pid not in detn_admit_only['pid'] and pid not in detn_wk1_only\n",
        "  # Use .copy() to ensure detn_wk3 is a copy, not a view\n",
        "  detn_wk3 = detn[(detn['wk3_calcdate_weekly'].notnull()) & (detn['wk2_calcdate_weekly'].notnull()) & (detn['wk1_calcdate_weekly'].notnull())].copy()\n",
        "\n",
        "  # prompt: drop all columns in detn_admit_only,detn_wk2,detn_wk3 that are for subsequent weeks, even if they're null,\n",
        "  # just to ensure that no future leaks into present\n",
        "\n",
        "  wk1_cols = [col for col in detn.columns if col.startswith('wk1')]\n",
        "  wk2_cols = [col for col in detn.columns if col.startswith('wk2')]\n",
        "  wk3_cols = [col for col in detn.columns if col.startswith('wk3')]\n",
        "\n",
        "  # Drop columns in detn_admit_only that start with wk1 or wk2 or wk3\n",
        "  detn_admit_only.drop(columns=wk1_cols,inplace=True)\n",
        "  detn_admit_only.drop(columns=wk2_cols,inplace=True)\n",
        "  detn_admit_only.drop(columns=wk3_cols,inplace=True)\n",
        "  # b_needsitp = 'indicates whether child was referred to an ITP', may be caused by complication\n",
        "  detn_admit_only.drop(columns='b_needsitp',inplace=True)\n",
        "  # early referrals may be caused by complication\n",
        "  detn_admit_only.drop(columns=[col for col in detn.columns if col.startswith('ref_')],inplace=True)\n",
        "\n",
        "\n",
        "  # Filter detn_admit_only where the label is 0\n",
        "  detn_admit_only_label_0 = detn_admit_only[detn_admit_only[label] == 0].copy()\n",
        "\n",
        "  # Get the columns where all values are null for the filtered data\n",
        "  null_columns = detn_admit_only_label_0.columns[detn_admit_only_label_0.isnull().all()].to_list()\n",
        "  # remove them, otherwise the model will just key on them to find label == 1\n",
        "  cols_to_drop = list(set(null_columns) - set(cat1_cols)) # keep y columns as they won't be used to predict\n",
        "  detn_admit_only.drop(columns=cols_to_drop,inplace=True)\n",
        "\n",
        "  detn_wk1_only.drop(columns=wk2_cols,inplace=True)\n",
        "  detn_wk1_only.drop(columns=wk3_cols,inplace=True)\n",
        "  # rsquared meaningless for single row patients\n",
        "  detn_wk1_only.drop(columns=[col for col in detn_wk1_only.columns if col.endswith('rsquared')],inplace=True)\n",
        "  detn_wk1_only.drop(columns=[col for col in detn_wk1_only.columns if col.endswith('_trend')],inplace=True)\n",
        "\n",
        "  detn_wk2.drop(columns=wk3_cols,inplace=True)\n",
        "  # rsquared is always 1 for the 2 row patients as they have complication on the third anthro row (2nd visit)\n",
        "  detn_wk2.drop(columns=[col for col in detn_wk2.columns if col.endswith('rsquared')],inplace=True)\n",
        "\n",
        "\n",
        "  print(detn_admit_only.shape,detn_wk1_only.shape,detn_wk2.shape,detn_wk3.shape)\n",
        "  #detn_admit_only = make_dummy_columns(detn_admit_only)\n",
        "  #detn_wk1_only = make_dummy_columns(detn_wk1_only)\n",
        "  #detn_wk2 = make_dummy_columns(detn_wk2)\n",
        "  #detn_wk3 = make_dummy_columns(detn_wk3)\n",
        "  return detn_admit_only,detn_wk1_only,detn_wk2,detn_wk3"
      ],
      "metadata": {
        "id": "LG4GD87DvZ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test(detn,ag_features,label):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X = make_dummy_columns(detn[ag_features])\n",
        "\n",
        "  #X = detn.drop(columns=label)\n",
        "  y = detn[label]\n",
        "\n",
        "  # Perform train-test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) #\n",
        "  # prompt: columns in X_train that are datetime64[ns]\n",
        "\n",
        "  datetime_columns = X_train.select_dtypes(include='datetime64[ns]').columns\n",
        "  for col in datetime_columns:\n",
        "    X_train,_ = days_since_min(X_train,col)\n",
        "    X_train.drop(columns=col,inplace=True)\n",
        "    X_test,_ = days_since_min(X_test,col)\n",
        "    X_test.drop(columns=col,inplace=True)\n",
        "\n",
        "\n",
        "  X_train,X_test = impute_missing_values(X_train,X_test)\n",
        "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "4AZTlTfk-Srq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_test(X_test_ag,y_test_ag,predictor,label,y_pred_proba,detn):\n",
        "  #y_pred_proba = predictor.predict_proba(X_test)\n",
        "  import pandas as pd\n",
        "  # Join X_test and y_test\n",
        "  df = X_test_ag.join(y_test_ag)\n",
        "\n",
        "  # prompt: join df to y_pred renaming column to predicted\n",
        "  y_pred = predictor.predict(X_test_ag)\n",
        "  # Rename the y_pred Series to 'predicted'\n",
        "  y_pred = pd.Series(y_pred, name=f'predicted_{label}')\n",
        "\n",
        "  # Concatenate the DataFrames\n",
        "  df = pd.concat([df, y_pred], axis=1)\n",
        "\n",
        "  pred_proba_series = y_pred_proba[1].rename(f'probability_{label}')\n",
        "  df = df.join(pred_proba_series)\n",
        "\n",
        "  test_labelled = df.join(detn[['pid']])\n",
        "  return test_labelled"
      ],
      "metadata": {
        "id": "cXDJ1ztS-TsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_test_lin(X_test,y_test,logreg_model,label,detn):\n",
        "  import pandas as pd\n",
        "  y_pred_proba = logreg_model.predict_proba(X_test)\n",
        "  y_pred = logreg_model.predict(X_test)\n",
        "  # Rename the y_pred Series to 'predicted'\n",
        "  y_pred = pd.Series(y_pred, name=f'predicted_{label}')\n",
        "\n",
        "  # Join X_test and y_test\n",
        "  df = X_test.join(y_test)\n",
        "\n",
        "  # prompt: join df to y_pred renaming column to predicted\n",
        "  # Reset index of both DataFrames before concatenation\n",
        "  df = df.reset_index(drop=False)\n",
        "  y_pred = y_pred.reset_index(drop=True)\n",
        "\n",
        "  # Concatenate the DataFrames\n",
        "  df = pd.concat([df, y_pred], axis=1)\n",
        "\n",
        "  # prompt: get the second column in y_pred_proba\n",
        "  df = df.join(pd.Series(y_pred_proba[:, 1], name=f'probability_{label}'))\n",
        "\n",
        "  df = df.set_index('index')\n",
        "  test_labelled_lin = df.join(detn[['pid']])\n",
        "  return test_labelled_lin\n"
      ],
      "metadata": {
        "id": "mgfxtGPMUsP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regress_general(detn_mh,regressor_cols,label,OLS=True):\n",
        "  import statsmodels.api as sm\n",
        "  # Prepare the data\n",
        "  df = detn_mh[regressor_cols + [label]]\n",
        "  print(df.shape)\n",
        "  df = df.dropna()\n",
        "  print(df.shape)\n",
        "  X = df[regressor_cols]\n",
        "  y = df[label]\n",
        "\n",
        "\n",
        "  # Add a constant to the independent variable for the intercept\n",
        "  X = sm.add_constant(X)\n",
        "\n",
        "  # Fit the OLS model\n",
        "  if OLS:\n",
        "    model = sm.OLS(y,X).fit()\n",
        "  else:\n",
        "    model = sm.Logit(y, X).fit()\n",
        "\n",
        "  # Print the model summary\n",
        "  print(model.summary())\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "habTW6SxLaWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regress_ols(detn_mh,regressor_col,label):\n",
        "  return linear_regress_general(detn_mh,regressor_col,label,OLS=True)\n"
      ],
      "metadata": {
        "id": "lnomuihdqpSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dimensionality(detn,columns_for_reduction,reduced_column_name):\n",
        "  from sklearn.decomposition import PCA\n",
        "  import pandas as pd\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  # Select the columns for dimensionality reduction\n",
        "  df_nonnull=detn[columns_for_reduction].dropna()\n",
        "  # Create a StandardScaler object\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(df_nonnull)\n",
        "\n",
        "  # Transform the data\n",
        "  scaled_data = scaler.transform(df_nonnull)\n",
        "\n",
        "  # Create a PCA object with the desired number of components\n",
        "  pca = PCA(n_components=1)  # Reduce to 1 component\n",
        "\n",
        "  # Fit the PCA model on the selected columns\n",
        "  pca.fit(scaled_data)\n",
        "  print(pca.explained_variance_ratio_)\n",
        "  # Transform the data to reduce its dimensionality\n",
        "  reduced_data = pca.transform(scaled_data)\n",
        "  reduced_data.columns = [reduced_column_name]\n",
        "  # Concatenate the reduced data with the original DataFrame\n",
        "  detn = pd.concat([detn, reduced_data], axis=1)\n",
        "  #detn.drop(columns=columns_for_reduction,inplace=True)\n",
        "  return detn\n"
      ],
      "metadata": {
        "id": "T1e6ra1SnL6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_probabilities(pid_probabilities,label,X_train_transformed,X_test_transformed,gbm,detn):\n",
        "  import pandas as pd\n",
        "  gbm_probability_label = f'gbm_probability_{label}'\n",
        "  y_pred_proba = gbm.predict_proba(X_train_transformed)\n",
        "  df_train = X_train_transformed.join(detn['pid'])\n",
        "  df_train = df_train.reset_index(drop=False)\n",
        "  df_train = df_train.join(pd.Series(y_pred_proba[:, 1], name=gbm_probability_label))\n",
        "  df_train = df_train.set_index('index')\n",
        "\n",
        "  y_pred_proba = gbm.predict_proba(X_test_transformed)\n",
        "  df_test = X_test_transformed.join(detn['pid'])\n",
        "  df_test = df_test.reset_index(drop=False)\n",
        "  df_test = df_test.join(pd.Series(y_pred_proba[:, 1], name=gbm_probability_label))\n",
        "  df_test = df_test.set_index('index')\n",
        "  #df[df['pid']== '23-2685'].T\n",
        "  df = pd.concat([df_train,df_test],axis=0)\n",
        "  pid_probabilities = pd.merge(pid_probabilities,df[['pid',gbm_probability_label]],on='pid',how='left')\n",
        "  return pid_probabilities\n"
      ],
      "metadata": {
        "id": "6jPQ9d_fJipI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_test_tree(X_test,y_test,tree_model,label,detn):\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "\n",
        "  y_pred_proba = tree_model.predict_proba(X_test)\n",
        "  y_pred = tree_model.predict(X_test)\n",
        "  # Rename the y_pred Series to 'predicted'\n",
        "  y_pred = pd.Series(y_pred, name=f'predicted_{label}')\n",
        "  tree_features = X_test.columns[np.array(tree_model.feature_importances_, dtype=bool)].to_list()\n",
        "  # Join X_test and y_test\n",
        "  df = X_test[tree_features].join(y_test)\n",
        "\n",
        "  # prompt: join df to y_pred renaming column to predicted\n",
        "  # Reset index of both DataFrames before concatenation\n",
        "  df = df.reset_index(drop=False)\n",
        "  y_pred = y_pred.reset_index(drop=True)\n",
        "\n",
        "  # Concatenate the DataFrames\n",
        "  df = pd.concat([df, y_pred], axis=1)\n",
        "\n",
        "  # prompt: get the second column in y_pred_proba\n",
        "  df = df.join(pd.Series(y_pred_proba[:, 1], name=f'probability_{label}'))\n",
        "\n",
        "  df = df.set_index('index')\n",
        "  test_labelled_tree = df.join(detn[['pid']])\n",
        "  return test_labelled_tree\n"
      ],
      "metadata": {
        "id": "bYJuR78mU2Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X13ANC6LPpM6"
      },
      "outputs": [],
      "source": [
        "def days_since_min(detn,variable):\n",
        "  # Calculate the number of days since the minimum of 'wk1_nv_date_weekly'\n",
        "  min_date = detn[variable].min()\n",
        "  variable_days = f'days_since_min_{variable}'\n",
        "  detn[variable_days] = (detn[variable] - min_date).dt.days\n",
        "  return detn,variable_days\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def regress(df,pid,variable):\n",
        "  import statsmodels.formula.api as smf\n",
        "  import numpy as np\n",
        "\n",
        "  \"\"\"regress variable against cumulative_days\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): anthropomorphic data\n",
        "      pid (str): patient id\n",
        "      variable (str): variable to regress against cumulative_days\n",
        "\n",
        "  Returns:\n",
        "      trend: coefficient of variable against cumulative_days\n",
        "      r_squared: r-squared value of the regression model\n",
        "  \"\"\"\n",
        "  # Filter for the specific pid\n",
        "  anthros_pid = df[df['pid'] == pid]\n",
        "  if anthros_pid[variable].count() < 2:\n",
        "    return None, None\n",
        "\n",
        "  # Fit the linear regression model\n",
        "  #print(anthros_pid.columns)\n",
        "  #print(f'{variable} ~ cumulative_days')\n",
        "  np.seterr(divide='ignore', invalid='ignore')\n",
        "  model = smf.ols(f'{variable} ~ cumulative_days', data=anthros_pid).fit()\n",
        "\n",
        "  # Get the R-squared value\n",
        "  if model.rsquared < 0:\n",
        "    r_squared = 0\n",
        "  else:\n",
        "    r_squared = model.rsquared\n",
        "\n",
        "  # Get the coefficients\n",
        "  coefficients = model.params\n",
        "\n",
        "  return coefficients['cumulative_days'], r_squared\n"
      ],
      "metadata": {
        "id": "H0YTumzzq4y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutogluonWrapper:\n",
        "    def __init__(self, predictor, feature_names, target_class=None):\n",
        "        self.ag_model = predictor\n",
        "        self.feature_names = feature_names\n",
        "        self.target_class = target_class\n",
        "        if target_class is None and predictor.problem_type != 'regression':\n",
        "            print(\"Since target_class not specified, SHAP will explain predictions for each class\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        import pandas as pd\n",
        "        if isinstance(X, pd.Series):\n",
        "            X = X.values.reshape(1,-1)\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.feature_names)\n",
        "        preds = self.ag_model.predict_proba(X)\n",
        "        if self.ag_model.problem_type == \"regression\" or self.target_class is None:\n",
        "            return preds\n",
        "        else:\n",
        "            return preds[self.target_class]"
      ],
      "metadata": {
        "id": "1pChpr12ZAP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model(source_path , destination_path ):\n",
        "  import shutil\n",
        "\n",
        "  # Use shutil.copytree to copy the directory\n",
        "  try:\n",
        "      shutil.copytree(source_path, destination_path)\n",
        "      print(f\"Successfully copied '{source_path}' to '{destination_path}'\")\n",
        "  except FileExistsError:\n",
        "      print(f\"Destination directory '{destination_path}' already exists. Please remove or choose a different destination.\")\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Source directory '{source_path}' not found.\")\n",
        "  except OSError as e:\n",
        "      print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "QGtspwPJbkY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain(idx,explainer,X_test):\n",
        "  import shap\n",
        "  shap_values_single = explainer.shap_values(X_test.loc[idx]) # Calculate SHAP values\n",
        "\n",
        "  exp = shap.Explanation(shap_values_single,\n",
        "                       explainer.expected_value,\n",
        "                       data=X_test.loc[idx].values,\n",
        "                       feature_names=X_test.columns)\n",
        "  shap.plots.waterfall(exp)  # Pass the Explanation object to waterfall plot"
      ],
      "metadata": {
        "id": "ASWWVMgzchcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_survival3(pid,aft,regression_dataset,label,covariate1,covariate2,covariate3):\n",
        "  import matplotlib.pyplot as plt\n",
        "  pid_row = regression_dataset[regression_dataset['pid'] == pid]\n",
        "  if pid_row.empty:\n",
        "    print(f\"No data found for PID: {pid}.  Is a covariate null?\")\n",
        "    return\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "  survival_series = aft.predict_survival_function(pid_row,conditional_after=pid_row['duration_days'])\n",
        "  survival_series.columns = [f'{pid} {label} survival']\n",
        "  ax1 = survival_series.plot(ax=ax[0])\n",
        "  ax[0].set_title(f'Survival function for {pid}')\n",
        "  ax[0].set_xlabel('Days')\n",
        "  ax[0].set_ylabel('Survival Probability')\n",
        "  ax[0].set_label(pid)\n",
        "  median_days = aft.predict_median(pid_row, conditional_after=pid_row['duration_days']).iloc[0]\n",
        "  ax[0].text(0.03, 0.6, f\"median time to\\n{label}: {median_days.round(0):.0f} days\", transform=ax[0].transAxes)\n",
        "  if covariate1 is not None:\n",
        "    ax[0].text(0.03, 0.2, f\"{covariate1}: {pid_row[covariate1].iloc[0]}\", transform=ax[0].transAxes)\n",
        "  if covariate2 is not None:\n",
        "    ax[0].text(0.03, 0.3, f\"{covariate2}: {pid_row[covariate2].iloc[0]}\", transform=ax[0].transAxes)\n",
        "  if covariate3 is not None:\n",
        "    ax[0].text(0.03, 0.4, f\"{covariate3}: {pid_row[covariate3].iloc[0]}\", transform=ax[0].transAxes)\n",
        "  hazard_series = aft.predict_hazard(pid_row,conditional_after=pid_row['duration_days'])\n",
        "  hazard_series.columns = [f'{pid} {label} rate per day']\n",
        "\n",
        "  hazard_series.plot(ax=ax[1])\n",
        "  ax[1].set_title(f'Hazard function for {pid}')\n",
        "  ax[1].set_xlabel('Days')\n",
        "  ax[1].set_ylabel(f'Chance of {label} occurring (per day)')\n",
        "  ax[1].yaxis.set_label_position(\"right\")\n",
        "  ax[1].set_label(pid)\n",
        "  fig.suptitle(f'Survival Curves for {pid} ({label})')\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "aIQoC_oM-1EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot both survival_series and hazard_series using fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4)) and put text on them\n",
        "def plot_survival(pid,aft,regression_dataset,label,covariate1,covariate2):\n",
        "  plot_survival3(pid,aft,regression_dataset,label,covariate1,covariate2,None)"
      ],
      "metadata": {
        "id": "vcg2OM0B_X3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform logistic regression\n",
        "def logistic_regression(log_reg_top_features,detn,label,scale=True):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import confusion_matrix,classification_report\n",
        "  import matplotlib.pyplot as plt\n",
        "  import pandas as pd\n",
        "  # Set global output format to Pandas\n",
        "  from sklearn import set_config\n",
        "  set_config(transform_output=\"pandas\")\n",
        "\n",
        "\n",
        "  detn_top_features = detn[log_reg_top_features].copy()\n",
        "  detn_top_features.fillna(detn_top_features.mean(),inplace=True)\n",
        "  #detn_top_features.dropna(inplace=True)\n",
        "  print(f'--- Correlation of Features with {label}')\n",
        "  print(detn_top_features[log_reg_top_features].corrwith(detn_top_features[label]))\n",
        "\n",
        "  X = detn_top_features.drop(columns=label)\n",
        "  y = detn_top_features[label]\n",
        "  if scale:\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "  # Perform train-test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
        "\n",
        "  # Initialize and train the Logistic Regression model\n",
        "  logreg_model = LogisticRegression(max_iter=1000,C=1,penalty=\"l1\", solver='liblinear') # Change solver to liblinear or saga\n",
        "  logreg_model.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = logreg_model.predict(X_test)\n",
        "\n",
        "  # Evaluate the model (example: using a confusion matrix)\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(\"\\nConfusion Matrix:\")\n",
        "  print(cm)\n",
        "\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # prompt: get feature importance of logreg_model\n",
        "  coefs = logreg_model.coef_[0]\n",
        "  feature_importances = pd.Series(abs(coefs), index=X_train.columns)\n",
        "  feature_importances = feature_importances[feature_importances > .01]\n",
        "  feature_importances.sort_values(ascending=True,inplace=True)\n",
        "  print('\\nCoefficients:')\n",
        "  print(feature_importances)\n",
        "  if scale:\n",
        "    feature_importances.plot(kind='barh')\n",
        "    plt.show()\n",
        "  return logreg_model,X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "KhYMrgA8dI4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree(max_depth,X_train,y_train,X_test,y_test):\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.metrics import confusion_matrix,balanced_accuracy_score,accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "  from sklearn import tree\n",
        "  import matplotlib.pyplot as plt\n",
        "  model = DecisionTreeClassifier(max_depth=max_depth)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_proba = model.predict_proba(X_test)\n",
        "  print('score',model.score(X_test, y_test))\n",
        "\n",
        "  print('balanced acc',balanced_accuracy_score(y_test,y_pred))\n",
        "  print('accuracy',accuracy_score(y_test,y_pred ))\n",
        "  print('precision',precision_score(y_test,y_pred ))\n",
        "  print('recall',recall_score(y_test,y_pred ))\n",
        "  print('f1',f1_score(y_test,y_pred ))\n",
        "  print('auc',roc_auc_score(y_test,y_pred ))\n",
        "  # Generate the confusion matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(cm)\n",
        "\n",
        "  tree.plot_tree(model, proportion=True)\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "1iXoP03xWXjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_f1(max_depth,X_train,y_train,X_test,y_test):\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  model = DecisionTreeClassifier(max_depth=max_depth)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return model,y_pred"
      ],
      "metadata": {
        "id": "mr_vCbSwVOI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get AIC value for model\n",
        "# Assuming 'model' is your trained LGBMRegressor model and X_train_transformed, y_train are available\n",
        "# Calculate AIC\n",
        "def calculate_aic(model, X, y):\n",
        "    import numpy as np\n",
        "    y_pred = model.predict(X)\n",
        "    n = len(y)\n",
        "    k = len(model.feature_importances_)  # Number of features (parameters) in your model\n",
        "    residuals = y - y_pred\n",
        "    sse = np.sum(residuals ** 2)\n",
        "    aic = 2 * k - 2 * np.log(sse)\n",
        "    return aic\n"
      ],
      "metadata": {
        "id": "rGxQer8yqAcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lightgbm_regress(X_train_transformed, X_test_transformed, y_train, y_test):\n",
        "  # prompt: train lightgbm model on X_train_transformed\n",
        "  import pandas as pd\n",
        "  import lightgbm as lgb\n",
        "  from sklearn.metrics import mean_squared_error,r2_score\n",
        "  gbm = lgb.LGBMRegressor(verbosity=-1)\n",
        "  # Train the LightGBM model\n",
        "  gbm.fit(X_train_transformed, y_train)\n",
        "  # Make predictions on the test set\n",
        "  y_pred = gbm.predict(X_test_transformed)\n",
        "  feature_importances = gbm.feature_importances_\n",
        "  # Get feature names\n",
        "  feature_names = X_train_transformed.columns\n",
        "  # Create a DataFrame for better visualization\n",
        "  feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "  # Filter features with importance greater than 0\n",
        "  important_features = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
        "  top_features = important_features['Feature'].to_list()\n",
        "  aic = calculate_aic(gbm, X_test_transformed, y_test)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "  #print(f\"Mean Squared Error: {mse}, \"R-squared: {r2}\")\n",
        "\n",
        "  return gbm,mse,aic,top_features"
      ],
      "metadata": {
        "id": "crR8Xr82qBN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features_regress(gbm,X_train_transformed_top,X_test_transformed_top, y_train, y_test,max_features,min_features,step):\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  results4 = []\n",
        "  features = {}\n",
        "  best_aic = 1000000\n",
        "  for n in np.arange(max_features,min_features,step):\n",
        "    top_n_features =  get_top_features(gbm,X_train_transformed_top,features_to_select=n)\n",
        "    X_train_transformed_top = X_train_transformed_top[top_n_features].copy()\n",
        "    X_test_transformed_top = X_test_transformed_top[top_n_features].copy()\n",
        "    #print(n,len(top_n_features),X_train_transformed_top.shape)\n",
        "    gbm,mse,aic,top_features = lightgbm_regress(X_train_transformed_top, X_test_transformed_top, y_train, y_test)\n",
        "    #print(len(gbm.feature_name_))\n",
        "    data = {'mse': mse, 'AIC': aic, 'num_features': len(gbm.feature_name_)}\n",
        "    features[n] = gbm.feature_names_in_\n",
        "    results4.append(data)\n",
        "    if aic < best_aic:\n",
        "      best_aic = aic\n",
        "      best_features = gbm.feature_names_in_\n",
        "      best_gbm = gbm\n",
        "  return best_gbm,best_features,pd.DataFrame(results4),best_aic,features"
      ],
      "metadata": {
        "id": "r1WRPkmrqNvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_tree_model(X_train,y_train,X_test,y_test,range = range(1,3)):\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.metrics import f1_score\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  best_model = None\n",
        "  best_f1 = 0\n",
        "  best_aic = float('inf')\n",
        "  best_pred = None\n",
        "  for max_depth in range:\n",
        "    tree_model,y_pred = decision_tree_f1(max_depth,X_train,y_train,X_test,y_test)\n",
        "    y_pred_prob = tree_model.predict_proba(X_test)[:, 1] # Get probabilities for class 1 only\n",
        "\n",
        "    log_likelihood = np.sum(y_test * np.log(y_pred_prob) + (1 - y_test) * np.log(1 - y_pred_prob))\n",
        "\n",
        "    # Calculate AIC\n",
        "    #k =  Number of parameters (features)\n",
        "    positive_features_count = sum(1 for importance in tree_model.feature_importances_ if importance > 0)\n",
        "    k = positive_features_count # Number of parameters (features)\n",
        "    aic = 2 * k - 2 * log_likelihood\n",
        "    f1_scored = f1_score(y_test,y_pred )\n",
        "    print(f'max_depth: {max_depth}, f1: {f1_scored}, aic: {aic}')\n",
        "    if aic < best_aic:\n",
        "      best_aic = aic\n",
        "      best_f1 = f1_scored\n",
        "      best_model = tree_model\n",
        "      best_pred = y_pred\n",
        "  if best_model is None:\n",
        "    print('no best model')\n",
        "    best_model = tree_model\n",
        "    best_pred = y_pred\n",
        "\n",
        "\n",
        "\n",
        "  feature_importances = best_model.feature_importances_\n",
        "\n",
        "  # Get feature names\n",
        "  feature_names = X_train.columns\n",
        "\n",
        "  # Create a DataFrame for better visualization\n",
        "  importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "  # Filter out features with zero importance\n",
        "  important_features = importance_df[importance_df['Importance'] > 0]\n",
        "\n",
        "  # Print or use the important features as needed\n",
        "  important_features['Feature'].to_list()\n",
        "\n",
        "  return best_model,best_f1,best_pred,best_aic,important_features['Feature'].to_list()"
      ],
      "metadata": {
        "id": "JbIKa0lpVCYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(tree_model):\n",
        "  import numpy as np\n",
        "  n_nodes = tree_model.tree_.node_count\n",
        "  children_left = tree_model.tree_.children_left\n",
        "  children_right = tree_model.tree_.children_right\n",
        "  feature = tree_model.tree_.feature\n",
        "  threshold = tree_model.tree_.threshold\n",
        "  values = tree_model.tree_.value\n",
        "  columns = tree_model.feature_names_in_\n",
        "\n",
        "  node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
        "  is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
        "  stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
        "  while len(stack) > 0:\n",
        "      # `pop` ensures each node is only visited once\n",
        "      node_id, depth = stack.pop()\n",
        "      node_depth[node_id] = depth\n",
        "\n",
        "      # If the left and right child of a node is not the same we have a split\n",
        "      # node\n",
        "      is_split_node = children_left[node_id] != children_right[node_id]\n",
        "      # If a split node, append left and right children and depth to `stack`\n",
        "      # so we can loop through them\n",
        "      if is_split_node:\n",
        "          stack.append((children_left[node_id], depth + 1))\n",
        "          stack.append((children_right[node_id], depth + 1))\n",
        "      else:\n",
        "          is_leaves[node_id] = True\n",
        "\n",
        "  print(\n",
        "      \"The binary tree structure has {n} nodes and has \"\n",
        "      \"the following tree structure:\\n\".format(n=n_nodes)\n",
        "  )\n",
        "  for i in range(n_nodes):\n",
        "      if is_leaves[i]:\n",
        "        print(\n",
        "            \"{space}node={node} is a leaf node with value={value}.\".format(\n",
        "                space=node_depth[i] * \"\\t\", node=i, value=np.around(values[i], 3)\n",
        "            )\n",
        "        )\n",
        "      else:\n",
        "          print(\n",
        "            \"{space}node={node} is a split node with value={value}: \"\n",
        "            \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
        "            \"else to node {right}.\".format(\n",
        "                space=node_depth[i] * \"\\t\",\n",
        "                node=i,\n",
        "                left=children_left[i],\n",
        "                feature=columns[feature[i]],\n",
        "                threshold=threshold[i],\n",
        "                right=children_right[i],\n",
        "                value=np.around(values[i], 3),\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "BOFpVJDTWqEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_tree_sample(clf,idx,X_test):\n",
        "  node_indicator = clf.decision_path(X_test)\n",
        "  leaf_id = clf.apply(X_test)\n",
        "  n_nodes = clf.tree_.node_count\n",
        "  feature = clf.tree_.feature\n",
        "  threshold = clf.tree_.threshold\n",
        "  values = clf.tree_.value\n",
        "  columns = clf.feature_names_in_\n",
        "\n",
        "  sample_id = X_test.index.get_loc(idx)\n",
        "  # obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\n",
        "  node_index = node_indicator.indices[\n",
        "      node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n",
        "  ]\n",
        "\n",
        "  print(f\"Decision tree rules used to predict sample loc {idx} row number {sample_id}:\\n\")\n",
        "  for node_id in node_index:\n",
        "      # continue to the next node if it is a leaf node\n",
        "      if leaf_id[sample_id] == node_id:\n",
        "          continue\n",
        "\n",
        "      # check if value of the split feature for sample 0 is below threshold\n",
        "      if X_test.iloc[sample_id, feature[node_id]] <= threshold[node_id]:\n",
        "          threshold_sign = \"<=\"\n",
        "      else:\n",
        "          threshold_sign = \">\"\n",
        "\n",
        "      print(\n",
        "          \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n",
        "          \"{inequality} {threshold})\".format(\n",
        "              node=node_id,\n",
        "              sample=sample_id,\n",
        "              feature=columns[feature[node_id]],\n",
        "              value=X_test.iloc[sample_id, feature[node_id]],\n",
        "              inequality=threshold_sign,\n",
        "              threshold=threshold[node_id],\n",
        "          )\n",
        "      )"
      ],
      "metadata": {
        "id": "22CK0wozmfal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aic(gbm, X_test_transformed, y_test):\n",
        "  import numpy as np\n",
        "  # prompt: get AIC value for gbm\n",
        "  # Assuming 'gbm' is your trained LightGBM model and 'X_test_transformed' and 'y_test' are defined.\n",
        "  # Calculate negative log-likelihood (you may need to adjust based on your specific loss function)\n",
        "  y_pred_prob = gbm.predict_proba(X_test_transformed)[:, 1] # Get probabilities for class 1 only\n",
        "\n",
        "  #y_pred_prob = gbm.predict(X_test_transformed, num_iteration=gbm.best_iteration)\n",
        "  log_likelihood = np.sum(y_test * np.log(y_pred_prob) + (1 - y_test) * np.log(1 - y_pred_prob))\n",
        "\n",
        "  # Calculate AIC\n",
        "  #k = len(gbm.feature_name()) # Number of parameters (features)\n",
        "  k = len(gbm.feature_name_) # Number of parameters (features)\n",
        "  aic = 2 * k - 2 * log_likelihood\n",
        "  # print(f\"AIC: {aic}\")\n",
        "  return aic"
      ],
      "metadata": {
        "id": "hBg9Xt6zyF5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_train(X_train_transformed, X_test_transformed, y_train, y_test):\n",
        "  # prompt: train lightgbm model on X_train_transformed\n",
        "  import pandas as pd\n",
        "  from sklearn.metrics import accuracy_score,f1_score\n",
        "  # Set global output format to Pandas\n",
        "  from sklearn import set_config\n",
        "  set_config(transform_output=\"pandas\")\n",
        "  import numpy as np\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  model = LogisticRegression(max_iter=1000,C=1,penalty=\"l1\", solver='liblinear')\n",
        "  # scale so abs of coefs becomes importance\n",
        "  X_train_transformed = StandardScaler().fit_transform(X_train_transformed)\n",
        "  X_test_transformed = StandardScaler().fit_transform(X_test_transformed)\n",
        "\n",
        "  model.fit(X_train_transformed, y_train)\n",
        "  # Make predictions on the test set\n",
        "  y_pred = model.predict(X_test_transformed)\n",
        "  # Convert probabilities to class labels (e.g. 0 or 1)\n",
        "  y_pred_class = [1 if prob > 0.5 else 0 for prob in y_pred] # Adjust threshold as needed\n",
        "  # Evaluate the model (example metrics)\n",
        "\n",
        "  f1_scored = f1_score(y_test,y_pred_class)\n",
        "  # print(f\"f1: {f1_scored}\")\n",
        "  coefs = model.coef_[0]\n",
        "  feature_importances = pd.Series(abs(coefs), index=X_train_transformed.columns)\n",
        "  #feature_importances.sort_values(ascending=True,inplace=True)\n",
        "  #important_feature_indices = feature_importances[feature_importances > 0]\n",
        "\n",
        "  # Predict log probabilities for the model\n",
        "  log_prob = model.predict_log_proba(X_test_transformed)\n",
        "  # Calculate log-likelihood for the model\n",
        "  log_likelihood = log_prob[np.arange(len(y_test)), y_test].sum()\n",
        "  k = len(model.feature_names_in_) + 1  # Number of parameters in model\n",
        "  aic = 2 * k - 2 * log_likelihood\n",
        "  #print(f\"AIC: {aic}\")\n",
        "\n",
        "  return model,f1_scored,aic,feature_importances"
      ],
      "metadata": {
        "id": "fTDw1tECW6Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_logistic_features(model,X_train_transformed_imputed,important_feature_indices,X_test_transformed_imputed, y_train, y_test):\n",
        "  \"\"\"\n",
        "  Selects the optimal set of features for a logistic regression model using AIC.\n",
        "\n",
        "  Iterates through decreasing subsets of the most important features (based on initial logistic regression), trains a logistic regression model on each subset, and selects the subset with the lowest AIC.\n",
        "\n",
        "  Args:\n",
        "    model: The initial trained logistic regression model.\n",
        "    X_train_transformed_imputed: The imputed and transformed training features.\n",
        "    important_feature_indices: A pandas Series containing feature importance scores.\n",
        "    X_test_transformed_imputed: The imputed and transformed test features.\n",
        "    y_train: The training target variable.\n",
        "    y_test: The test target variable.\n",
        "\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the best model, the best feature set, a DataFrame of results, the best AIC, and a dictionary of features used for each iteration.\n",
        "  \"\"\"\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  results_lin = []\n",
        "  features_lin = {}\n",
        "  best_aic_lin = 1000000\n",
        "  # n = 100 or important_feature_indices.size, whichever is less\n",
        "  for n in np.arange(min(40, important_feature_indices.size),1,-1):\n",
        "    top_n_features =  important_feature_indices.nlargest(n).index.to_list()\n",
        "    X_train_transformed_top = X_train_transformed_imputed[top_n_features].copy()\n",
        "    X_test_transformed_top = X_test_transformed_imputed[top_n_features].copy()\n",
        "    #print(n,len(top_n_features),X_train_transformed_top.shape)\n",
        "    model,f1_scored,aic,important_feature_indices = logistic_train(X_train_transformed_top, X_test_transformed_top, y_train, y_test)\n",
        "    #print(len(model.feature_names_in_))\n",
        "    data = {'f1_score': [f1_scored], 'AIC': [aic], 'num_features': [len(model.feature_names_in_)]}\n",
        "    features_lin[n] = model.feature_names_in_\n",
        "    results_lin.append(data)\n",
        "    if aic < best_aic_lin:\n",
        "      best_aic_lin = aic\n",
        "      best_features_lin = list(model.feature_names_in_)\n",
        "      model = LogisticRegression(max_iter=1000,C=1,penalty=\"l1\", solver='liblinear')\n",
        "      model.fit(X_train_transformed_top[best_features_lin], y_train)\n",
        "      best_model = model\n",
        "      best_features_index = important_feature_indices\n",
        "  return best_model,best_features_lin,pd.DataFrame(results_lin),best_aic_lin,features_lin"
      ],
      "metadata": {
        "id": "f-LFwAVFOWUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_missing_values(X_train,X_test):\n",
        "  import numpy as np\n",
        "  #print(X_train.isnull().sum()[X_train.isnull().sum() > 0].sort_values(ascending=False) / X_train.shape[0])\n",
        "  #print(X_train.isnull().sum().sum())\n",
        "  X_train_imputed = X_train.copy()\n",
        "  X_test_imputed = X_test.copy()\n",
        "  # Replace infinite values with NaN\n",
        "  X_train_imputed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "  X_test_imputed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "  # Impute NaN values with the mean\n",
        "  X_train_imputed.fillna(X_train_imputed.mean(), inplace=True)\n",
        "  X_test_imputed.fillna(X_test_imputed.mean(), inplace=True)\n",
        "  #print(X_test_imputed.isnull().sum()[X_test_imputed.isnull().sum() > 0].sort_values(ascending=False) / X_test_imputed.shape[0])\n",
        "  X_test_not_null_cols = X_test_imputed.columns[X_test_imputed.notnull().any()].tolist()\n",
        "  X_train_not_null_cols = X_train_imputed.columns[X_train_imputed.notnull().any()].tolist()\n",
        "  not_null_cols = list(set(X_train_not_null_cols) & set(X_test_not_null_cols))\n",
        "  #print(X_train_imputed[not_null_cols].isnull().sum().sum())\n",
        "  #print(X_test_imputed[not_null_cols].isnull().sum().sum())\n",
        "  return X_train_imputed[not_null_cols],X_test_imputed[not_null_cols]"
      ],
      "metadata": {
        "id": "yjHUA5PlxYdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find notnull columns in detn_admit\n",
        "def get_best_lin_model(detn_admit_only,label):\n",
        "  import shap\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import numpy as np\n",
        "  not_null_cols = detn_admit_only.columns[detn_admit_only.notnull().any()].tolist()\n",
        "  multi_value_cols = [col for col in detn_admit_only[not_null_cols].columns if detn_admit_only[col].nunique() > 1]\n",
        "  numeric_dtypes = detn_admit_only[multi_value_cols].select_dtypes(include=['number']).columns.to_list()\n",
        "  numeric_dtypes.remove(label)\n",
        "  X = detn_admit_only[numeric_dtypes]\n",
        "  y = detn_admit_only[label]\n",
        "\n",
        "  # Perform train-test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
        "  #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "  X_train, X_test = impute_missing_values(X_train,X_test)\n",
        "\n",
        "  model,f1_scored,aic,important_feature_indices = logistic_train(X_train, X_test, y_train, y_test)\n",
        "\n",
        "  best_model,best_features_lin,results_lin_df,best_aic_lin,features_lin = select_logistic_features (model,X_train,important_feature_indices,X_test, y_train, y_test)\n",
        "\n",
        "  explainer_linear = shap.Explainer(best_model, X_train[best_features_lin])\n",
        "\n",
        "  return best_model,best_features_lin,results_lin_df,best_aic_lin,features_lin,X_test,y_test,explainer_linear,X_train,y_train\n"
      ],
      "metadata": {
        "id": "hoBb5Gb59G5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_features(gbm,X_train_transformed,features_to_select):\n",
        "  import pandas as pd\n",
        "  # prompt: get feature importance from gbm\n",
        "\n",
        "  # Get feature importances from the trained LightGBM model\n",
        "  #feature_importances = gbm.feature_importance()\n",
        "  feature_importances = gbm.feature_importances_\n",
        "\n",
        "  # Get feature names\n",
        "  feature_names = X_train_transformed.columns\n",
        "\n",
        "  # Create a DataFrame for better visualization\n",
        "  feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "\n",
        "  # Sort the DataFrame by importance in descending order\n",
        "  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "  # prompt: get the top 100 features with importance > 0\n",
        "\n",
        "  # Filter features with importance greater than 0\n",
        "  important_features = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
        "\n",
        "  # Get the top 100 features\n",
        "  top_100_features = important_features.head(features_to_select)\n",
        "\n",
        "  top_features = top_100_features['Feature'].to_list()\n",
        "  return top_features\n"
      ],
      "metadata": {
        "id": "gknIm6lg0gq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lightgbm_train(X_train_transformed, X_test_transformed, y_train, y_test):\n",
        "  # prompt: train lightgbm model on X_train_transformed\n",
        "  import pandas as pd\n",
        "  import lightgbm as lgb\n",
        "  from sklearn.metrics import accuracy_score,f1_score\n",
        "  #gbm = lgb.LGBMClassifier()\n",
        "  gbm = lgb.LGBMClassifier(objective='binary', metric='binary_logloss',verbosity=-1)\n",
        "  # Train the LightGBM model\n",
        "  gbm.fit(X_train_transformed, y_train)\n",
        "  # Make predictions on the test set\n",
        "  y_pred = gbm.predict(X_test_transformed)\n",
        "  # Convert probabilities to class labels (e.g. 0 or 1)\n",
        "  y_pred_class = [1 if prob > 0.5 else 0 for prob in y_pred] # Adjust threshold as needed\n",
        "  # Evaluate the model (example metrics)\n",
        "  accuracy = accuracy_score(y_test, y_pred_class)\n",
        "  #print(f\"Accuracy: {accuracy}\")\n",
        "  f1_scored = f1_score(y_test,y_pred_class)\n",
        "  #print(f\"f1: {f1_scored}\")\n",
        "  # Get feature importances from the trained LightGBM model\n",
        "  feature_importances = gbm.feature_importances_\n",
        "  # Get feature names\n",
        "  feature_names = X_train_transformed.columns\n",
        "  # Create a DataFrame for better visualization\n",
        "  feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "  # Filter features with importance greater than 0\n",
        "  important_features = feature_importance_df[feature_importance_df['Importance'] > 0]\n",
        "  top_features = important_features['Feature'].to_list()\n",
        "  aic = get_aic(gbm, X_test_transformed, y_test)\n",
        "  return gbm,f1_scored,aic,top_features"
      ],
      "metadata": {
        "id": "VfWbicdryHSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns(detn):\n",
        "  drop_columns_muac(detn,drop_muac=True)\n"
      ],
      "metadata": {
        "id": "i0HAHZpTMDwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns_muac(detn,drop_muac=True):\n",
        "  detn.drop(columns=[col for col in detn.columns if 'lastms' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'otp' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'precalcsite' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wast' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'attachments' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'photo' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'picture' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'canmovevisit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'staffmember' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'bednet' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'receivedsmc' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'device' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'lookup_calc' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submitter' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'dose' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'settlement' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'calcdate' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wfh' in col and col not in ['weekly_last_wfh']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'manual_daystonv' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'resp_rate_2' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'doneses' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'end_time' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'endtime' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submissiondate' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'name' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'pp_cm' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'starttime' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submission_date' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'start_time' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'last_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'c_assigned_cm' in col],inplace=True)\n",
        "  #detn.drop(columns=[col for col in detn.columns if 'wfa' in col],inplace=True)\n",
        "  #detn.drop(columns=[col for col in detn.columns if 'hfa' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'first_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'site_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if '_week' in col and col not in ['muac_loss_2_weeks_consecutive']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'site_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'site_admit' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if col.endswith('_week') and not col.endswith('_weekly')],inplace=True)\n",
        "  #detn.drop(columns=[col for col in detn.columns if 'hl' in col],inplace=True)\n",
        "  if drop_muac:\n",
        "    detn.drop(columns=[col for col in detn.columns if 'muac' in col and col not in ['weekly_last_muac','muac_loss_2_weeks_consecutive']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'todate' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if col.endswith('_age')],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'birthdate' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'vax_dates' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if col.startswith('vd_')],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'sequence_num' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if col.endswith('visitnum')],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'row_count' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'los' in col and col not in ['wk1_calc_los','detn_weight_loss_ever','muac_loss_2_weeks_consecutive']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'time_minutes' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'wfa' in col and col not in ['wfa_trend']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'hl' in col and col not in ['hl_trend']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'hfa' in col and col not in ['hfa_trend']],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'form' in col ],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'date' in col ],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'drug_record' in col ],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if col.endswith('vax')],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'weight' in col and col not in ['wk1_weight_diff_rate','detn_weight_loss_ever']],inplace=True)"
      ],
      "metadata": {
        "id": "ADQYSECyo-le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gbm_shap(features,N_FEATURES,X_train_transformed,X_test_transformed,X_test_transformed_top,y_train,y_test,cutoff):\n",
        "  import shap\n",
        "  gbm_best_features = list(features[N_FEATURES])\n",
        "  gbm2,_,_,_ = lightgbm_train(X_train_transformed[gbm_best_features], X_test_transformed[gbm_best_features], y_train, y_test)\n",
        "  # Create a SHAP explainer object\n",
        "  explainer = shap.TreeExplainer(gbm2)\n",
        "  shap_values = explainer.shap_values(X_test_transformed[gbm_best_features])\n",
        "\n",
        "  # Generate summary plot\n",
        "  shap.summary_plot(shap_values, X_test_transformed_top[gbm_best_features])\n",
        "  # Wrap shap_values in an Explanation object\n",
        "  shap_values_explanation = shap.Explanation(shap_values,\n",
        "                                           data=X_test_transformed_top[gbm_best_features].values, # Assuming .values gives you the underlying NumPy array\n",
        "                                           feature_names=gbm_best_features) # Assuming .columns gives you the feature names\n",
        "\n",
        "\n",
        "  clustering = shap.utils.hclust(X_test_transformed_top[gbm_best_features], y_test)\n",
        "  shap.plots.bar( shap_values_explanation, max_display=20,clustering=clustering,clustering_cutoff=cutoff)\n"
      ],
      "metadata": {
        "id": "JEIh8b1asqkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_result_columns(detn,label):\n",
        "  \"\"\"\n",
        "  drop columns that happen as a result of the deterioration, reverse causality\n",
        "  \"\"\"\n",
        "  detn.drop(columns=[col for col in detn.columns if 'interpolated' in col],inplace=True)\n",
        "  # could be caused by poor (or good) weight gain\n",
        "  detn.drop(columns=[col for col in detn.columns if 'sachets' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'discharge' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'dose' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'rationweeks' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'nv_date' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'eligible' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'drugs' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'dischq' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'lag' in col],inplace=True)\n",
        "  # Actively receiving treatment may because of complication rather than predicting it\n",
        "  detn.drop(columns=[col for col in detn.columns if 'status_text' in col],inplace=True)\n",
        "  # set to 1 if Actively receiving treatment\n",
        "  detn.drop(columns=[col for col in detn.columns if 'correct_status' in col],inplace=True)\n",
        "  # for zero weeks (those w/o wk1) completely determines new onset complication, by def'n\n",
        "  if 'weekly_row_count' in detn.columns:\n",
        "    detn.drop(columns=[col for col in detn.columns if 'weekly_row_count' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'form_version' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'submitter_id' in col],inplace=True)\n",
        "\n",
        "  # for the 4 in that have only 1 week, if either is set to 1, then new complications always 0, probably caused by new onset complication, rather than predicting it\n",
        "  detn.drop(columns=[col for col in detn.columns if 'imci_emergency_otp' in col],inplace=True)\n",
        "  detn.drop(columns=[col for col in detn.columns if 'referred_emergency' in col],inplace=True)\n",
        "\n",
        "\n",
        "  #detn.drop(columns=[f'{label}_date'], inplace=True)\n",
        "\n",
        "  return detn"
      ],
      "metadata": {
        "id": "xwwyY_yLw2H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_logreg(idx,test_labelled_logreg,label):\n",
        "  for df, explainer, X_test, f1_scored in test_labelled_logreg:\n",
        "    if idx in df.index:\n",
        "        test_labelled_lin = df\n",
        "        lin_model_f1 = f1_scored\n",
        "        #print(test_labelled_lin.loc[idx])\n",
        "        _ = explain_prediction(idx,explainer,df,X_test,label)\n",
        "        break\n",
        "  return test_labelled_lin,lin_model_f1"
      ],
      "metadata": {
        "id": "RiU9plCSL4iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features(gbm,X_train_transformed_top,X_test_transformed_top, y_train, y_test,max_features,min_features,step):\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  results4 = []\n",
        "  features = {}\n",
        "  best_aic = 1000000\n",
        "  for n in np.arange(max_features,min_features,step):\n",
        "    top_n_features =  get_top_features(gbm,X_train_transformed_top,features_to_select=n)\n",
        "    X_train_transformed_top = X_train_transformed_top[top_n_features].copy()\n",
        "    X_test_transformed_top = X_test_transformed_top[top_n_features].copy()\n",
        "    #print(n,len(top_n_features),X_train_transformed_top.shape)\n",
        "    gbm,f1_scored,aic,top_features = lightgbm_train(X_train_transformed_top, X_test_transformed_top, y_train, y_test)\n",
        "    #print(len(gbm.feature_name_))\n",
        "    data = {'f1_score': f1_scored, 'AIC': aic, 'num_features': len(gbm.feature_name_)}\n",
        "    features[n] = gbm.feature_names_in_\n",
        "    results4.append(data)\n",
        "    if aic < best_aic:\n",
        "      best_aic = aic\n",
        "      best_features = gbm.feature_names_in_\n",
        "      best_gbm = gbm\n",
        "  return best_gbm,best_features,pd.DataFrame(results4),best_aic,features"
      ],
      "metadata": {
        "id": "_NwnC0TU3LxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_column_names(top_features):\n",
        "  # prompt: for each column in top_features remove the period and the text after the period\n",
        "\n",
        "  # Assuming 'top_features' list is already defined as in your provided code.\n",
        "  top_features = [col.split('.', 1)[0] for col in top_features]\n",
        "  top_features = [col.replace('_day', '') if col.endswith('_day') else col for col in top_features]\n",
        "  top_features = [col.replace('_month', '') if col.endswith('_month') else col for col in top_features]\n",
        "  top_features = set(top_features)\n",
        "  top_features = list(top_features)\n",
        "  return top_features\n"
      ],
      "metadata": {
        "id": "uYi5GXyo3Mkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_patient_probabilities(label,idx,test_labelled_tree,test_labelled,test_labelled_lin,tree_model_f1,ag_model_f1,lin_model_f1,test_labelled_gbm,gbm_model_f1):\n",
        "  probability = test_labelled_tree.loc[idx][f'probability_{label}']\n",
        "  ag_probability = test_labelled.loc[idx][f'probability_{label}']\n",
        "  gbm_probability = test_labelled_gbm.loc[idx][f'probability_{label}']\n",
        "  print('\\n')\n",
        "  print(f'Autogluon (f1: {ag_model_f1:f}) probability: {ag_probability:f}')\n",
        "  print(f'GBM classifier (f1: {gbm_model_f1:f}) probability: {gbm_probability:f}')\n",
        "  print(f'Decision tree classifier (f1: {tree_model_f1:f}) probability: {probability:f}')\n",
        "  if lin_model_f1 != None:\n",
        "    lin_probability = test_labelled_lin.loc[idx][f'probability_{label}']\n",
        "    print(f'Logistic regression (f1: {lin_model_f1:f}) probability: {lin_probability:f}')"
      ],
      "metadata": {
        "id": "VWHWIdO1x3Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_prediction(idx,explainer,test_labelled,X_test,label):\n",
        "  import shap\n",
        "  import matplotlib.pyplot as plt\n",
        "  # Instead of:\n",
        "  # shap.plots.waterfall(explainer.shap_values(X_test.iloc[0,:]))\n",
        "\n",
        "  # Use:\n",
        "  #shap_values_single = explainer.shap_values(X_test.iloc[0,:]) # Calculate SHAP values\n",
        "\n",
        "\n",
        "  shap_values_single = explainer.shap_values(X_test.loc[idx]) # Calculate SHAP values\n",
        "\n",
        "  values_to_display = test_labelled.loc[idx][['pid',f'probability_{label}',f'predicted_{label}',label]]\n",
        "  plt.text(0.3, 0.1, f'{values_to_display}', transform=plt.gca().transAxes)\n",
        "  exp = shap.Explanation(shap_values_single,\n",
        "                       explainer.expected_value,\n",
        "                       #data=X_test.iloc[75,:].values,\n",
        "                       data=X_test.loc[idx].values,\n",
        "                       feature_names=X_test.columns)\n",
        "  shap.plots.waterfall(exp)  # Pass the Explanation object to waterfall plot\n",
        "  return values_to_display\n"
      ],
      "metadata": {
        "id": "cA7xMUV63Dpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_prediction_gbm(idx,gbm,test_labelled_gbm,X_test_transformed,label):\n",
        "  import shap\n",
        "  import matplotlib.pyplot as plt\n",
        "  explainer_gbm2 = shap.TreeExplainer(gbm)\n",
        "  shap_values_gbm2 = explainer_gbm2.shap_values(X_test_transformed)\n",
        "\n",
        "  iloc =  X_test_transformed.index.get_loc(idx)\n",
        "\n",
        "  values_to_display = test_labelled_gbm.loc[idx][['pid',f'probability_{label}',f'predicted_{label}',label]]\n",
        "  plt.text(0.3, 0.1, f'{values_to_display}', transform=plt.gca().transAxes)\n",
        "\n",
        "\n",
        "  exp2 = shap.Explanation(shap_values_gbm2[iloc],\n",
        "                       explainer_gbm2.expected_value,\n",
        "                       #data=X_test.iloc[75,:].values,\n",
        "                       data=X_test_transformed.loc[idx].values,\n",
        "                       feature_names=X_test_transformed.columns)\n",
        "\n",
        "  log_odds = exp2.values.sum() + exp2.base_values\n",
        "\n",
        "  probability = log_odds_to_probability(log_odds)\n",
        "  plt.text(0.3, 0.5, 'LightGBM model', transform=plt.gca().transAxes)\n",
        "  plt.text(0.3, 0.3, f\"The probability of {label}\\ncorresponding to log odds {log_odds:.3f} is: {probability:.2%}\", transform=plt.gca().transAxes)\n",
        "\n",
        "  shap.plots.waterfall(exp2)  # Pass the Explanation object to waterfall plot\n",
        "\n",
        "  return values_to_display\n"
      ],
      "metadata": {
        "id": "3nviCdTchrdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_odds_to_probability(log_odds):\n",
        "  \"\"\"Converts log odds to probability.\n",
        "\n",
        "  Args:\n",
        "    log_odds: The log odds value.\n",
        "\n",
        "  Returns:\n",
        "    The probability.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  odds = np.exp(log_odds)\n",
        "  probability = odds / (1 + odds)\n",
        "  return probability"
      ],
      "metadata": {
        "id": "E9wFb_2d-MPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_bool(df):\n",
        "    for col in df.columns:\n",
        "        # Check if the column has exactly two unique non-null values (excluding NaN)\n",
        "        # and if both of these values can be interpreted as boolean (0/1, True/False, etc.)\n",
        "\n",
        "        # Get unique non-null values\n",
        "        series1 = df[col].dropna()\n",
        "        if len(series1) < 2:\n",
        "          continue\n",
        "        unique_values = series1.unique()\n",
        "\n",
        "        if len(unique_values) == 2 and all(val in [0, 1, True, False] for val in unique_values):\n",
        "            # Convert to boolean only if both values are boolean-like\n",
        "            df[col] = df[col].astype(bool)\n",
        "    return df"
      ],
      "metadata": {
        "id": "5oQK4CrV8IBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bool_to_int(df):\n",
        "  \"\"\"Converts all boolean columns in a pandas DataFrame to integer type.\"\"\"\n",
        "  for column in df.columns:\n",
        "    if df[column].dtype == bool:\n",
        "      df[column] = df[column].astype(int)\n",
        "  return df"
      ],
      "metadata": {
        "id": "u9O7lhXgn26z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_cols(admit_current_row,cols,which_cat='cat1'):\n",
        "  true_cols = []\n",
        "  for col in cols:\n",
        "    if admit_current_row[col].iloc[0] == True:\n",
        "      true_cols.append(col)\n",
        "\n",
        "  if true_cols:\n",
        "    print(f\"{which_cat} Columns with True values upon admission:\")\n",
        "    for col in true_cols:\n",
        "      print(col)\n",
        "  else:\n",
        "    print(f'all {which_cat} columns false upon admission')\n",
        "\n"
      ],
      "metadata": {
        "id": "xVD1l8QH4sdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dummy_columns(df):\n",
        "  import pandas as pd\n",
        "  categorical_cols = df.select_dtypes(include=['category']).columns\n",
        "  if len(categorical_cols) == 0:\n",
        "    return df\n",
        "\n",
        "  # Create dummy variables for categorical columns\n",
        "  dummy_df = pd.get_dummies(df[categorical_cols], drop_first=True)\n",
        "  for col in dummy_df.columns:\n",
        "        if dummy_df[col].dtype == bool:\n",
        "            dummy_df[col] = dummy_df[col].astype(int)\n",
        "\n",
        "  # Get non-categorical columns\n",
        "  non_categorical_cols = df.select_dtypes(exclude=['category']).columns\n",
        "\n",
        "  # Concatenate dummy variables with non-categorical columns\n",
        "  df = pd.concat([df[non_categorical_cols], dummy_df], axis=1)\n",
        "  df.columns = [col.replace(' ', '_') for col in df.columns]\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "MgCZYJ_7-cW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_corr(detn, variable, label):\n",
        "  plot_corr_jitter(detn, variable, label, 0.1)\n"
      ],
      "metadata": {
        "id": "DPLER6bsFnBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_anthros(values_to_display,admit_weekly,admit_current,detn,cat1_cols,cat2_cols,boolean_cat1_weekly_cols,boolean_cat2_weekly_cols,label,cat1_weekly_cols,cat2_weekly_cols):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "\n",
        "  pid = values_to_display['pid']\n",
        "  admit_weekly_rows = admit_weekly[admit_weekly['pid']==pid]\n",
        "  admit_current_row =admit_current[admit_current['pid']==pid]\n",
        "  detn_row = detn[detn['pid']==pid]\n",
        "  # prompt: get columns that contain cat1 and end in _weekly from admit_weekly\n",
        "  #check_cols(admit_current_row,cat1_cols)\n",
        "  #check_cols(admit_current_row,cat2_cols,which_cat='cat2')\n",
        "\n",
        "  # Filter columns that contain 'cat1' and end with '_weekly'\n",
        "\n",
        "  # prompt: columns in admit_weekly_rows[boolean_cat2_weekly_cols].sum() where sum() > 0\n",
        "\n",
        "  # Get columns where the sum is greater than 0\n",
        "  positive_sum_cols1 = admit_weekly_rows[boolean_cat1_weekly_cols].sum()[lambda x: x > 0].index.to_list()\n",
        "  positive_sum_cols1.insert(0,'calcdate_days_since_first')\n",
        "  positive_sum_cols1.insert(0,'sequence_num')\n",
        "  positive_sum_cols1.insert(0,'calcdate_weekly')\n",
        "\n",
        "\n",
        "  admit_weekly_rows[positive_sum_cols1]\n",
        "  #\n",
        "  # prompt: get positive_sum_cols1 for calcdate_weekly == detn_row[f'{label}_date']\n",
        "  onset_date = detn_row[f'{label}_date'].iloc[0]\n",
        "  print('onset_date',onset_date)\n",
        "\n",
        "  filtered_admit_weekly_rows = admit_weekly_rows[admit_weekly_rows['calcdate_weekly'] == onset_date ].copy()\n",
        "\n",
        "  filtered_admit_weekly_rows['calcdate_weekly'] = filtered_admit_weekly_rows['calcdate_weekly'].dt.date\n",
        "\n",
        "  # prompt: columns in admit_weekly_rows[boolean_cat2_weekly_cols].sum() where sum() > 0\n",
        "  # Get columns where the sum is greater than 0\n",
        "  positive_sum_cols2 = admit_weekly_rows[boolean_cat2_weekly_cols].sum()[lambda x: x > 0].index.to_list()\n",
        "  positive_sum_cols2.insert(0,'calcdate_weekly')\n",
        "  admit_weekly_rows[positive_sum_cols2]\n",
        "\n",
        "\n",
        "  # prompt: find rows in admit_weekly_rows where  boolean_cat2_weekly_cols.any()\n",
        "\n",
        "  # Find rows where any boolean_cat2_weekly_cols is True\n",
        "  rows_with_true_cat2 = admit_weekly_rows[admit_weekly_rows[boolean_cat2_weekly_cols].any(axis=1)].copy()\n",
        "\n",
        "  #positive_sum_cols2.remove('calcdate')\n",
        "  positive_sum_cols2.insert(0,'calcdate_days_since_first')\n",
        "  #positive_sum_cols2.insert(0,'calcdate_weekly')\n",
        "\n",
        "  #positive_sum_cols1.insert(0,'calcdate_days_since_first')\n",
        "  positive_sum_cols2.insert(0,'sequence_num')\n",
        "  rows_with_true_cat2['calcdate_weekly'] = rows_with_true_cat2['calcdate_weekly'].dt.date\n",
        "\n",
        "\n",
        "  rows_with_true_cat2[positive_sum_cols2]\n",
        "  cat1_weekly_cols.insert(0,'calcdate_weekly')\n",
        "  cat2_weekly_cols.insert(0,'calcdate_weekly')\n",
        "\n",
        "\n",
        "  anthro_cols = ['calcdate','muac','weight','hl']\n",
        "  admit_weekly_rows = admit_weekly_rows.rename(columns={\n",
        "    'calcdate_weekly': 'calcdate',\n",
        "    'muac_weekly': 'muac',\n",
        "    'weight_weekly': 'weight',\n",
        "    'hl_weekly': 'hl'\n",
        "  })\n",
        "\n",
        "  admit_current_row = admit_current_row.rename(columns={\n",
        "    'hl_admit': 'hl'\n",
        "  })\n",
        "\n",
        "  anthros = pd.concat([admit_weekly_rows[anthro_cols],admit_current_row[anthro_cols]],axis=0)\n",
        "  anthros,variable_days = days_since_min(anthros,'calcdate')\n",
        "  anthros.sort_values(by='calcdate',inplace=True)\n",
        "\n",
        "  anthroz_cols = ['calcdate','wfh_weekly', 'hfa_weekly', 'wfa_weekly']\n",
        "\n",
        "  temp_row = admit_current_row.rename(columns={\n",
        "    'wfh': 'wfh_weekly',\n",
        "    'hfa': 'hfa_weekly',\n",
        "    'wfa': 'wfa_weekly'\n",
        "  })\n",
        "\n",
        "\n",
        "  anthroz = pd.concat([admit_weekly_rows[anthroz_cols],temp_row[anthroz_cols]],axis=0)\n",
        "\n",
        "  anthroz = anthroz.rename(columns={\n",
        "    'wfh_weekly': 'wfh',\n",
        "    'hfa_weekly': 'hfa',\n",
        "    'wfa_weekly': 'wfa'\n",
        "  })\n",
        "  anthroz,variable_days = days_since_min(anthroz,'calcdate')\n",
        "  anthroz.sort_values(by='calcdate',inplace=True)\n",
        "\n",
        "  # Create the plot\n",
        "  fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "  # Plot MUAC, weight, and hl on the primary y-axis\n",
        "  ax1.plot(anthros['days_since_min_calcdate'], anthros['muac'], label='MUAC', marker='o')\n",
        "  line = ax1.lines[-1]  # Get the most recently added line (the plot line)\n",
        "  muac_color = line.get_color()\n",
        "  ax1.plot(anthros['days_since_min_calcdate'], anthros['weight'], label='Weight', marker='x')\n",
        "  line = ax1.lines[-1]  # Get the most recently added line (the plot line)\n",
        "  weight_color = line.get_color()\n",
        "\n",
        "  # Create a secondary y-axis\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.plot(anthros['days_since_min_calcdate'], anthros['hl'], label='HL', marker='s', color='green')\n",
        "  line = ax2.lines[-1]  # Get the most recently added line (the plot line)\n",
        "  hl_color = line.get_color()\n",
        "\n",
        "  # Add labels and title\n",
        "  ax1.set_xlabel('Days Since Admission (Minimum Calcdate)')\n",
        "  ax1.set_ylabel('MUAC and Weight')\n",
        "  ax2.set_ylabel('HL')\n",
        "  plt.title(f'MUAC, Weight, and HL over Time for {pid}')\n",
        "\n",
        "\n",
        "  if rows_with_true_cat2.shape[0] > 0:\n",
        "    first_cat2_onset = rows_with_true_cat2['calcdate_weekly'].iloc[0]\n",
        "    first_cat2_onset = pd.to_datetime(first_cat2_onset)\n",
        "    first_cat2 = anthros[anthros['calcdate'] == first_cat2_onset]['days_since_min_calcdate'].iloc[0]\n",
        "\n",
        "    ax1.axvline(x=first_cat2-0.5, color='black', linestyle='--')\n",
        "\n",
        "    # Add text at the midpoint of the y-axis\n",
        "    y_midpoint = (ax1.get_ylim()[0] + ax1.get_ylim()[1]) / 2\n",
        "    cat2_text = rows_with_true_cat2[positive_sum_cols2].T\n",
        "    #ax1.text(first_cat2 - 0.6, y_midpoint, f'{cat2_text}', rotation=0, va='center', ha='right')\n",
        "\n",
        "  onset_date = detn_row[f'{label}_date'].iloc[0]\n",
        "\n",
        "  # Check if the filtered Series is empty before accessing iloc[0]\n",
        "  filtered_series = anthros[anthros['calcdate'] == onset_date]['days_since_min_calcdate']\n",
        "  if not filtered_series.empty:\n",
        "    first_cat1 = filtered_series.iloc[0]\n",
        "    ax1.axvline(x=first_cat1, color='red', linestyle='--')\n",
        "    # Add text at the midpoint of the y-axis\n",
        "    cat1_info = filtered_admit_weekly_rows[positive_sum_cols1].T\n",
        "    y_midpoint = (ax1.get_ylim()[0] + ax1.get_ylim()[1]) / 2\n",
        "    ax1.text(first_cat1 + 0.1 , y_midpoint, f'{cat1_info}', rotation=0, va='center', ha='left')\n",
        "\n",
        "\n",
        "  # Add trend lines\n",
        "  if anthros.shape[0] > 2:\n",
        "    z = np.polyfit(anthros['days_since_min_calcdate'], anthros['muac'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax1.plot(anthros['days_since_min_calcdate'],p(anthros['days_since_min_calcdate']), \"--\", color=muac_color, label=\"MUAC Trend\")\n",
        "\n",
        "    z = np.polyfit(anthros['days_since_min_calcdate'], anthros['weight'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax1.plot(anthros['days_since_min_calcdate'],p(anthros['days_since_min_calcdate']), \"--\", color=weight_color, label=\"Weight Trend\")\n",
        "\n",
        "    z = np.polyfit(anthros['days_since_min_calcdate'], anthros['hl'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax2.plot(anthros['days_since_min_calcdate'],p(anthros['days_since_min_calcdate']), \"--\", color=hl_color, label=\"HL Trend\")\n",
        "\n",
        "  # Add legends\n",
        "  lines, labels = ax1.get_legend_handles_labels()\n",
        "  lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "  ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
        "\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show(block=True)\n",
        "\n",
        "  # prompt: plot 'wfhz', 'hfaz', 'wfaz'  with trend lines in anthroz by days_since_min_calcdate\n",
        "\n",
        "  # Create the plot\n",
        "  fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "  for col in ['wfh', 'wfa']:\n",
        "    ax.plot(anthroz['days_since_min_calcdate'], anthroz[col], label=col, marker='o')\n",
        "    if anthroz.shape[0] > 2:\n",
        "      z = np.polyfit(anthroz['days_since_min_calcdate'], anthroz[col], 1)\n",
        "      p = np.poly1d(z)\n",
        "      line = ax.lines[-1]  # Get the most recently added line (the plot line)\n",
        "      color = line.get_color()\n",
        "      ax.plot(anthroz['days_since_min_calcdate'], p(anthroz['days_since_min_calcdate']), \"--\", color=color, label=f\"{col} Trend\")\n",
        "\n",
        "      # Create a secondary y-axis\n",
        "  ax2 = ax.twinx()\n",
        "  ax2.plot(anthroz['days_since_min_calcdate'], anthroz['hfa'], label='hfa',color='green', marker='o')\n",
        "  if anthroz.shape[0] > 2:\n",
        "    z = np.polyfit(anthroz['days_since_min_calcdate'], anthroz['hfa'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    line = ax2.lines[-1]  # Get the most recently added line (the plot line)\n",
        "    color = line.get_color()\n",
        "    ax2.plot(anthroz['days_since_min_calcdate'], p(anthroz['days_since_min_calcdate']), \"--\", color=color, label=f\"hfa Trend\")\n",
        "  ax2.set_ylabel('HFA')\n",
        "\n",
        "  lines, labels = ax.get_legend_handles_labels()\n",
        "  lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "  ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
        "\n",
        "\n",
        "  if rows_with_true_cat2.shape[0] > 0:\n",
        "    first_cat2_onset = rows_with_true_cat2['calcdate_weekly'].iloc[0]\n",
        "    first_cat2_onset = pd.to_datetime(first_cat2_onset)\n",
        "    first_cat2 = anthros[anthros['calcdate'] == first_cat2_onset]['days_since_min_calcdate'].iloc[0]\n",
        "\n",
        "\n",
        "    ax.axvline(x=first_cat2-0.5, color='black', linestyle='--')\n",
        "\n",
        "    # Add text at the midpoint of the y-axis\n",
        "    y_midpoint = (ax.get_ylim()[1] + ax.get_ylim()[0]) / 2\n",
        "    cat2_text = rows_with_true_cat2[positive_sum_cols2].T\n",
        "    ax.text(first_cat2-0.6, y_midpoint, f'{cat2_text}', rotation=0, va='center', ha='right')\n",
        "\n",
        "\n",
        "  onset_date = detn_row[f'{label}_date'].iloc[0]\n",
        "  filtered_series = anthros[anthros['calcdate'] == onset_date]['days_since_min_calcdate']\n",
        "  if not filtered_series.empty:\n",
        "    first_cat1 = filtered_series.iloc[0]\n",
        "\n",
        "    ax.axvline(x=first_cat1, color='red', linestyle='--')\n",
        "    # Add text at the midpoint of the y-axis\n",
        "    cat1_info = filtered_admit_weekly_rows[positive_sum_cols1].T\n",
        "    #ax.text(first_cat1+0.1, y_midpoint, f'{cat1_info}', rotation=0, va='center', ha='left')\n",
        "\n",
        "\n",
        "  ax.set_xlabel('Days Since Admission (Minimum Calcdate)')\n",
        "  ax.set_ylabel('WFH and WFA')\n",
        "  plt.title(f'wfh, hfa, and wfa over Time for {pid}')\n",
        "  #ax.legend()\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QJIuhgcD4dFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_recent_columns(detn):\n",
        "  recent_admit_columns = ['c_imci_emergency',\n",
        " 'where_referred_emergency',\n",
        " 'other_state',\n",
        " 'other_lga',\n",
        " 'ts_assessed_malnstatus',\n",
        " 'ts_assessed_needitp',\n",
        " 'manual_nvdate',\n",
        " 'pt_photo',\n",
        " 'supp_vd_ipv1',\n",
        " 'supp_vd_rota2',\n",
        " 'supp_vd_rota3',\n",
        " 'supp_vd_ipv2',\n",
        " 'cleaning_note']\n",
        "\n",
        "  recent_raw_columns = ['b_dpth_sorethroat',\n",
        " 'b_dpth_diffswallow',\n",
        " 'b_dpth_bloody',\n",
        " 'b_dpth_lymph',\n",
        " 'b_suspecteddipth',\n",
        " 'ofstaffmember',\n",
        " 'spec_imci_em_other',\n",
        " 'other_state',\n",
        " 'other_lga',\n",
        " 'b_figurepid',\n",
        " 'manual_prev_status',\n",
        " 'c_azafu_symptoms',\n",
        " 'b_azafu_multivomitepi',\n",
        " 'b_azafu_vomitl24',\n",
        " 'c_azafu_vomitl24freq',\n",
        " 'b_azafu_vomiteveryoral',\n",
        " 'b_azafu_soughtcare',\n",
        " 'c_azafu_whycare',\n",
        " 'b_azafu_overnightcare',\n",
        " 'c_azafu_whyovernightcare',\n",
        " 'c_azafu_urinefreq',\n",
        " 'c_azafu_urinecolor',\n",
        " 'b_azafu_otherprob',\n",
        " 'b_azafu_reqclinician',\n",
        " 'proceed_previnel',\n",
        " 'inac_weight_measurement',\n",
        " 'hl_measurement',\n",
        " 'c_physician_assess',\n",
        " 'b_phys_req_itp',\n",
        " 'muac_measurement',\n",
        " 'indiv_valid_admit',\n",
        " 'manual_admit_type_other',\n",
        " 'patient_picture',\n",
        " 'cg_relationship_other',\n",
        " 'phone_owner_other',\n",
        " 'otherlang_text',\n",
        " 'referring_other',\n",
        " 'migrate_reason_other',\n",
        " 'b_receivedsmc',\n",
        " 'b_bednet',\n",
        " 'c_bednettype',\n",
        " 'wall_type_other',\n",
        " 'drinking_water_other',\n",
        " 'toilet_other',\n",
        " 'resp_rate_2',\n",
        " 'resp_rate_3',\n",
        " 'b_swellingtender',\n",
        " 'q_conf_override_lref',\n",
        " 'why_noaccepthts',\n",
        " 'supp_vd_rota2',\n",
        " 'supp_vd_rota3',\n",
        " 'supp_vd_ipv2',\n",
        " 'pp_rota2_precalc',\n",
        " 'pp_rota3_precalc',\n",
        " 'pp_ipv2_precalc',\n",
        " 'override_conf',\n",
        " 'why_override_rat',\n",
        " 'manual_treatment',\n",
        " 'conf_initremover',\n",
        " 'remdrug_filter',\n",
        " 'rem_drugs_which',\n",
        " 'expl_remal_act',\n",
        " 'expl_remlorat',\n",
        " 'expl_remnystatin',\n",
        " 'expl_remtetra',\n",
        " 'expl_remzincox',\n",
        " 'expl_remotomed',\n",
        " 'conf_twoaddover',\n",
        " 'conf_changedose',\n",
        " 'confirmmovevisit',\n",
        " 'manual_nvdate',\n",
        " 'manual_daystonv']\n",
        "\n",
        "  admit_columns_to_delete =  set(recent_admit_columns).intersection(set(detn.columns))\n",
        "\n",
        "  detn.drop(columns=admit_columns_to_delete,inplace=True)\n",
        "  raw_columns_to_delete =  set(recent_raw_columns).intersection(set(detn.columns))\n",
        "\n",
        "  detn.drop(columns=raw_columns_to_delete,inplace=True)\n"
      ],
      "metadata": {
        "id": "AunGqfnwPPwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: for col in y find the top 5 correlations in X and make a dataframe from it\n",
        "\n",
        "\n",
        "# Assuming X and y are already defined as in the provided code\n",
        "# and cat1_notests contains the column names in y\n",
        "\n",
        "def top_correlations(X, y_col, top_n,detn):\n",
        "  \"\"\"\n",
        "  Finds the top N correlations between columns in X and a specific column in y.\n",
        "\n",
        "  Args:\n",
        "      X: DataFrame containing the features.\n",
        "      y_col: String, name of the column in y to check correlations against.\n",
        "      top_n: Integer, the number of top correlations to return.\n",
        "\n",
        "  Returns:\n",
        "      DataFrame: DataFrame with the top N correlated features and their correlation values.\n",
        "  \"\"\"\n",
        "  correlations = X.corrwith(detn[y_col]).abs().sort_values(ascending=False)\n",
        "  return pd.DataFrame({\n",
        "    'Feature': correlations.index[0:top_n],\n",
        "    'Correlation': correlations.values[0:top_n]\n",
        "  })\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CP5pDD4XTsDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ag_feature_generator(X_train,X_test):\n",
        "  from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
        "  auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
        "  X_train_transformed = auto_ml_pipeline_feature_generator.fit_transform(X=X_train)\n",
        "  X_test_transformed = auto_ml_pipeline_feature_generator.transform(X_test)\n",
        "  # Replace whitespace in column names with underscores\n",
        "  X_train_transformed.columns = [col.replace(' ', '_') for col in X_train_transformed.columns]\n",
        "  X_test_transformed.columns = [col.replace(' ', '_') for col in X_test_transformed.columns]\n",
        "\n",
        "  X_train_transformed.columns = X_train_transformed.columns.str.replace('[, ]', '_', regex=True)  # Replace commas and spaces\n",
        "  X_test_transformed.columns = X_test_transformed.columns.str.replace('[, ]', '_', regex=True)  # Replace commas and spaces\n",
        "  # TODO allow periods through\n",
        "  X_train_transformed.columns = X_train_transformed.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)\n",
        "  X_test_transformed.columns = X_test_transformed.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)\n",
        "\n",
        "\n",
        "  # prompt: remove duplicate column names in X_train_transformed\n",
        "\n",
        "  # Drop duplicate columns in X_train_transformed\n",
        "  X_train_transformed = X_train_transformed.loc[:,~X_train_transformed.columns.duplicated()]\n",
        "  X_test_transformed = X_test_transformed.loc[:,~X_test_transformed.columns.duplicated()]\n",
        "  return X_train_transformed,X_test_transformed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S5sh0bN41BAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ag_regress_model_load(label,model,frac,detn):\n",
        "  from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import r2_score, root_mean_squared_error\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import shap\n",
        "  MODEL_PATH = \"/content/drive/My Drive/[PBA] Code/model/\"\n",
        "  model_path = f\"{MODEL_PATH}/{label}{model}/\"\n",
        "  predictor = TabularPredictor.load(model_path,require_py_version_match=False,require_version_match=False)\n",
        "\n",
        "  ag_features = predictor.features()\n",
        "  print('ag features:', ag_features)\n",
        "  print(detn.shape)\n",
        "  print('wk1_cg_weight_weekly' in detn.columns)\n",
        "\n",
        "  X = detn[ag_features]\n",
        "  y = detn[label]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
        "\n",
        "  train_data = TabularDataset(X_train[ag_features].join(y_train))\n",
        "  test_data = TabularDataset(X_test[ag_features].join(y_test))\n",
        "  y_pred = predictor.predict(test_data.drop(columns=[label]))\n",
        "  print(predictor.evaluate(test_data, silent=True))\n",
        "  rmse = root_mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "  print(f'AutoGluon rmse: {rmse:f}')\n",
        "\n",
        "  class AutogluonWrapper2:\n",
        "    def __init__(self, predictor, feature_names):\n",
        "        self.ag_model = predictor\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def predict(self, X):\n",
        "        if isinstance(X, pd.Series):\n",
        "            X = X.values.reshape(1,-1)\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.feature_names)\n",
        "        return self.ag_model.predict(X)\n",
        "  ag_wrapper = AutogluonWrapper2(predictor, X_train.columns)\n",
        "\n",
        "\n",
        "  explainer = shap.KernelExplainer(ag_wrapper.predict, shap.sample(X_train, 100))\n",
        "\n",
        "  NSHAP_SAMPLES = 100  # how many samples to use to approximate each Shapely value, larger values will be slower\n",
        "  N_VAL = 30\n",
        "\n",
        "  X_test_sample = X_test[ag_features].sample(frac=frac, random_state=42) # Use random_state for reproducibility\n",
        "  shap_values = explainer.shap_values(X_test_sample, nsamples=NSHAP_SAMPLES)\n",
        "  shap.summary_plot(shap_values, X_test_sample)\n",
        "  return explainer,predictor,ag_features,y_pred,rmse,X_test,y_test"
      ],
      "metadata": {
        "id": "Pfm45igleeM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ag_model_load(label,frac,detn):\n",
        "  return ag_model_load_suffix(label,frac,detn,'')\n"
      ],
      "metadata": {
        "id": "XCtfoQKalunm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ag_model_load_suffix(label,frac,detn,suffix):\n",
        "  from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import confusion_matrix,f1_score\n",
        "  import shap\n",
        "  import os\n",
        "  os.chdir(\"/content/drive/My Drive/[PBA] Code\")\n",
        "\n",
        "  from util import AutogluonWrapper\n",
        "\n",
        "  os.chdir(\"/content\")\n",
        "  MODEL_PATH = \"/content/drive/My Drive/[PBA] Code/model\"\n",
        "\n",
        "  model_path = f\"{MODEL_PATH}/{label}{suffix}/\"\n",
        "  predictor = TabularPredictor.load(model_path,require_py_version_match=False,require_version_match=False)\n",
        "\n",
        "  ag_features = predictor.features()\n",
        "  print('ag features:', len(ag_features), ag_features)\n",
        "\n",
        "  X = detn[ag_features]\n",
        "  y = detn[label]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43) # Adjust test_size and random_state as needed\n",
        "\n",
        "  train_data = TabularDataset(X_train[ag_features].join(y_train))\n",
        "  test_data = TabularDataset(X_test[ag_features].join(y_test))\n",
        "  print(predictor.evaluate(test_data, silent=True))\n",
        "  y_pred = predictor.predict(X_test[ag_features])\n",
        "  y_pred_proba = predictor.predict_proba(X_test[ag_features])\n",
        "  y_pred_proba_all = predictor.predict_proba(X[ag_features])\n",
        "  ag_model_f1 = f1_score(y_test,y_pred )\n",
        "  print(f'AutoGluon f1: {ag_model_f1:f}')\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(cm)\n",
        "\n",
        "  target_class = 1  # can be any possible value of the label column\n",
        "  negative_class = 0\n",
        "  baseline = X_train[ag_features][y_train==negative_class].sample(20, random_state=0)\n",
        "  ag_wrapper = AutogluonWrapper(predictor, ag_features, target_class)\n",
        "  explainer = shap.KernelExplainer(ag_wrapper.predict_proba, baseline)\n",
        "\n",
        "  NSHAP_SAMPLES = 200\n",
        "\n",
        "  X_test_sample = X_test[ag_features].sample(frac=frac, random_state=42) # Use random_state for reproducibility\n",
        "  shap_values = explainer.shap_values(X_test_sample, nsamples=NSHAP_SAMPLES)\n",
        "  shap.summary_plot(shap_values, X_test_sample)\n",
        "  return explainer,predictor,ag_features,y_pred,y_pred_proba,ag_model_f1,X_test,y_test,y_pred_proba_all"
      ],
      "metadata": {
        "id": "u5a4MpJ2kd_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_regress_ag_model(idx,explainer,X_test,label,y_pred,y,rmse,PID,anthropometrics=True):\n",
        "  import shap\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "  shap_values_single = explainer.shap_values(X_test.loc[idx]) # Calculate SHAP values\n",
        "  if anthropometrics:\n",
        "    model_text = 'AutoGluon (anthropometrics) model'\n",
        "  else:\n",
        "    model_text = 'AutoGluon (non-anthropometrics) model'\n",
        "\n",
        "  plt.text(0.3, 0.5, f'{model_text}\\n(rmse: {rmse:.4f}) for\\npredicting {label}\\nfor patient {PID}', transform=plt.gca().transAxes)\n",
        "  plt.text(0.3, 0.1, f'predicted value: {y_pred:.4f}\\nactual value: {y}', transform=plt.gca().transAxes)\n",
        "  exp = shap.Explanation(shap_values_single,\n",
        "                       explainer.expected_value,\n",
        "                       data=X_test.loc[idx].values,\n",
        "                       feature_names=X_test.columns)\n",
        "  shap.plots.waterfall(exp)  # Pass the Explanation object to waterfall plot\n"
      ],
      "metadata": {
        "id": "MOtELtRkp94Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_ag_model(idx,explainer,X_test,label,y_pred_proba,y_pred,y,ag_model_f1,PID):\n",
        "  import shap\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "  shap_values_single = explainer.shap_values(X_test.loc[idx]) # Calculate SHAP values\n",
        "  plt.text(0.3, 0.5, f'AutoGluon model (f1: {ag_model_f1:.4f}) for\\nprobability of {label}\\nfor patient {PID}', transform=plt.gca().transAxes)\n",
        "  plt.text(0.3, 0.1, f'predicted probability: {y_pred_proba:.1%}\\npredicted class: {y_pred}\\nactual class: {y}', transform=plt.gca().transAxes)\n",
        "  exp = shap.Explanation(shap_values_single,\n",
        "                       explainer.expected_value,\n",
        "                       data=X_test.loc[idx].values,\n",
        "                       feature_names=X_test.columns)\n",
        "  shap.plots.waterfall(exp)  # Pass the Explanation object to waterfall plot\n"
      ],
      "metadata": {
        "id": "8TX1werTxULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_test_gbm(X_test_transformed,y_test,gbm,label,detn):\n",
        "  #y_pred_proba = predictor.predict_proba(X_test)\n",
        "  import pandas as pd\n",
        "  # Join X_test and y_test\n",
        "  df = X_test_transformed.join(y_test)\n",
        "\n",
        "  y_pred_proba = gbm.predict_proba(X_test_transformed)\n",
        "  y_pred = gbm.predict(X_test_transformed)\n",
        "  # Rename the y_pred Series to 'predicted'\n",
        "  y_pred = pd.Series(y_pred, name=f'predicted_{label}')\n",
        "\n",
        "  # Join X_test and y_test\n",
        "  df = X_test_transformed.join(y_test)\n",
        "\n",
        "  # prompt: join df to y_pred renaming column to predicted\n",
        "  # Reset index of both DataFrames before concatenation\n",
        "  df = df.reset_index(drop=False)\n",
        "  y_pred = y_pred.reset_index(drop=True)\n",
        "\n",
        "  # Concatenate the DataFrames\n",
        "  df = pd.concat([df, y_pred], axis=1)\n",
        "\n",
        "  # prompt: get the second column in y_pred_proba\n",
        "  df = df.join(pd.Series(y_pred_proba[:, 1], name=f'probability_{label}'))\n",
        "\n",
        "  df = df.set_index('index')\n",
        "\n",
        "  test_labelled_gbm = df.join(detn[['pid']])\n",
        "  return test_labelled_gbm"
      ],
      "metadata": {
        "id": "b9XxCeZ9XRRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_corr_jitter(detn, variable, label, x_jitter):\n",
        "  import seaborn as sns\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  null_ct = detn[variable].isnull().sum()\n",
        "  # Calculate correlation\n",
        "  corr = detn[[variable, label]].corr()\n",
        "  if detn[label].nunique() == 2:\n",
        "    sns.regplot(x=variable, y=label, data=detn, logistic=True, y_jitter=0.1,x_jitter=x_jitter ,line_kws={\"color\": \"red\"}, scatter_kws={'s': 2})\n",
        "  else:\n",
        "    sns.regplot(x=variable, y=label, data=detn, logistic=False, y_jitter=0.0,x_jitter=0.0 ,line_kws={\"color\": \"red\"}, scatter_kws={'s': 2})\n",
        "\n",
        "  plt.text(0.02, 0.3, f\"mean {variable} is {detn[variable].mean():.2f}\", transform=plt.gca().transAxes)\n",
        "  plt.text(0.02, 0.7, f\"Correlation of {variable} to {label}:\\n{corr[variable][label]:.2f}\", transform=plt.gca().transAxes)\n",
        "  if detn[variable].nunique() < 5:\n",
        "     plt.text(0.1, 0.5, f'mean {label} is:\\n{detn[[variable,label]].groupby(variable).mean().round(3)}', transform=plt.gca().transAxes)\n",
        "\n",
        "  plt.text(0.02, 0.2, f\"Null ct: {variable} {null_ct}\", transform=plt.gca().transAxes)\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "FEmalhlXOqlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}