# -*- coding: utf-8 -*-
"""infer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zv7p1ycuoeHZ3EG_rUtUMEq0GwhZXF0i

# summary

visualize model importance features and specific patients' model predictions and why.  Plot anthropometrics by visit and plot cat2/cat1 events on the timeline.

# setup

run

1.   etl
2.   etl_derioration

to get the dataframes that this notebook depends on
"""

# TODO change to digital ocean
MODEL_PATH = "/content/drive/My Drive/[PBA] Code/model"

!pip install import_ipynb --quiet

#!pip install shap --quiet
# survival analysis
!pip install lifelines --quiet

# Commented out IPython magic to ensure Python compatibility.
!git clone -b brian-etl-code https://github.com/The-Taimaka-Project/health-predictions.git
#!git clone https://github.com/The-Taimaka-Project/health-predictions.git

# Change directory to the repository
# %cd health-predictions/packages/inference/run
# %pwd
#TODO move util here

# %cd /content

import pandas as pd

# Set global output format to Pandas
import datetime
import numpy as np
import pickle
import os
import re
import json

#import shap
import import_ipynb
from warnings import simplefilter,filterwarnings

from lifelines import WeibullAFTFitter



from google.colab import drive
drive.mount('/content/drive')



from util import reduce_dimensionality




# prompt: read google shared drive file


dir = "/content/drive/My Drive/[PBA] Data/analysis/"

# use auto ML (autogluon) to predict the 5 deterioration events
!pip install autogluon --quiet

from autogluon.features.generators import AutoMLPipelineFeatureGenerator
from autogluon.tabular import TabularDataset, TabularPredictor

import logging
# Create a logger# Create a logger
logger = logging.getLogger('my_logger')
logger.setLevel(logging.DEBUG) # Set the minimum logging level

# Create a handler to output logs to the console
console_handler = logging.StreamHandler()

file_handler = logging.FileHandler('my_log.log')
file_handler.setLevel(logging.INFO) # Set the logging level for the handler


# Create a formatter to specify the log message format
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(lineno)d - %(funcName)s - %(message)s')

# Add the formatter to the handler

file_handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(file_handler)

# TODO replace with read data from csv and datatypes dict from json or Postgres SQL 
with open(dir + 'admit_weekly.pkl', 'rb') as f:
  admit_weekly = pickle.load(f)
with open(dir + 'admit_current.pkl', 'rb') as f:
  admit_current = pickle.load(f)

simplefilter(action="ignore", category=pd.errors.PerformanceWarning)
simplefilter(action="ignore", category=FutureWarning)

pid_probabilities = admit_current[['pid','status','site_current']].copy()
logger.debug(pid_probabilities.shape)

def read_detn(label):
  # Load the deterioration time series, admit plus up to 3 visits before the event
  detn = pd.read_csv(dir + f"/{label}.csv")
  with open(dir + f"/{label}.json", 'r') as f:
    detn_dtypes = json.load(f)
  # prompt: apply detn_types to detn

  # Iterate over the dictionary and apply the data types to detn2
  for col, dtype in detn_dtypes.items():
    if col in detn.columns:
        try:
            if dtype == 'category':
                detn[col] = detn[col].astype('object') # Read as object, convert to category later if needed
            else:
                detn[col] = detn[col].astype(dtype)
        except Exception as e:
            logger.error(f"Could not convert column {col} to {dtype}: {e}")
  return detn

"""# new onset medical complication

New onset medical complication - 'cat1' complication (see vars in raw ODK data with cat1_ prefix)
"""

# prompt: use pickle to read deterioration dataframe
label = 'new_onset_medical_complication'

#detn = read_detn(label)

with open(dir + f'{label}.pkl', 'rb') as f:
  detn = pickle.load(f)
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')


detn = detn[detn['weekly_last_admit'].notnull()].copy()

detn.drop(columns='c_admission_other',inplace=True) # causes duplicate column if get_dummies invoked
detn.drop(columns='phone_owner_other',inplace=True) # messes up get_dummies
detn.drop(columns='calc_dayssincevita',inplace=True) # incorrectly calculated for non-active, messes up detn_wk1_only models

y_cat1_cols = [col for col in detn.columns if col.startswith('y_cat1')]
detn.drop(columns=y_cat1_cols,inplace=True) # only looking at label new_onset_medical_complication, not the actual categories

logger.debug(detn.shape)

#detn['glbsite'] = detn['glbsite'].str.lower()
#detn['wk1_autosite'] = detn['wk1_autosite'].str.lower()
detn['wk1_calc_los'] = round(detn['wk1_calc_los'],0)
detn['weekly_last_muac'] = round(detn['weekly_last_muac'],1)

detn['wk1_calc_los'].fillna(0,inplace=True)
LOS_CUTOFF = 12
MUAC_CUTOFF = 12.5
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()},{detn.shape}')


detn = detn[(detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF) ].copy()

detn = reduce_dimensionality(detn,['muac_diff_ratio','muac'],'muac_diff_ratio_z')
detn = reduce_dimensionality(detn,['household_adults','household_slept','living_children'],'household_adults_slept_living_children_z')

logger.debug(f'{detn[label].sum()},{detn[label].mean()},{detn.shape}')

"""## predictive model importances

### AutoGluon

#### stratified models by length of service
"""

def run_ag_model(label,detn,suffix):
  from autogluon.tabular import TabularDataset, TabularPredictor

  model_path = f"{MODEL_PATH}/{label}{suffix}/"
  predictor = TabularPredictor.load(model_path,require_py_version_match=False,require_version_match=False)

  ag_features = predictor.features()
  logger.debug(f'ag features:, {len(ag_features)}, {ag_features}')


  y_pred_proba_all = predictor.predict_proba(detn[ag_features])

  return y_pred_proba_all

detn_admit_only,_,_,_ = split_detn_new_onset_medical_complication(detn,label)

#detn_admit_only = make_dummy_columns(detn_admit_only)
pid_not_in_admit = detn[~detn['pid'].isin(detn_admit_only['pid'])]['pid']
detn_filtered = detn[detn['pid'].isin(pid_not_in_admit)].copy()

y_pred_proba_all1 = run_ag_model(label,detn_admit_only,'1')
y_pred_proba_all2 = run_ag_model(label,detn_filtered,'not1')

#y_pred_proba_all_stratified = pd.concat([y_pred_proba_all1,y_pred_proba_all2,y_pred_proba_all3,y_pred_proba_all4],axis=0)
y_pred_proba_all_stratified = pd.concat([y_pred_proba_all1,y_pred_proba_all2],axis=0)
y_pred_proba_all_stratified_series = y_pred_proba_all_stratified[1].rename(f'probability_{label}_stratified')
pid_probabilities = pid_probabilities.join(y_pred_proba_all_stratified_series)



# survival
logger.debug(f'{detn[label].sum()},{detn.shape}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn[label].sum()},{detn.shape}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# prompt: filter detn for label_date not null or wk1_calcdate_weekly not null

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn[label].sum()},{detn.shape}')

# prompt: create column in detn called final_date which is status_date or wk1_calcdate_weekly whichever is greater
# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(detn[label_date].notnull().sum())

# prompt: for rows in detn where label==1 set duration_days to label_date - calcdate

# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

# prompt: clip detn['duration_days'] to 0.1

detn['duration_days'] = detn['duration_days'].clip(lower=0.1)


# AFT survival regression
selected_columns = ['muac_diff','wk1_muac','muac_diff_ratio_rate']

X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regr_cols = ['wk1_muac','muac','muac_diff','muac_diff_ratio_rate',label,'duration_days','pid','status']
regression_dataset = detn[regr_cols].copy()
regression_dataset['wk1_muac'].fillna(regression_dataset['muac'],inplace=True)
regression_dataset.drop(columns=['muac'],inplace=True)
regression_dataset.dropna(inplace=True)
#regression_dataset['wk1_muac'].fillna(0,inplace=True)


regression_dataset['duration_days'] = regression_dataset['duration_days'].clip(lower=.01)
aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True, formula=' wk1_muac + muac_diff + muac_diff_ratio_rate')

logger.debug(aft.print_summary())


# censored (i.e., patients w/o complication yet) survival prediction

regr_cols = ['wk1_muac','muac','muac_diff','muac_diff_ratio_rate',label,'duration_days','pid','status']
censored_subjects = detn[regr_cols].copy()
censored_subjects['wk1_muac'].fillna(censored_subjects['muac'],inplace=True)
censored_subjects.drop(columns=['muac'],inplace=True)

regression_dataset[regression_dataset['wk1_muac'].notnull()][['muac_diff','muac_diff_ratio_rate']].isnull().sum()

censored_subjects['muac_diff_ratio_rate'].fillna(0,inplace=True)

censored_subjects.dropna(inplace=True)

censored_subjects = censored_subjects.loc[detn[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]


y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','wk1_muac','muac_diff','muac_diff_ratio_rate',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')

# poor weight gain

label = 'detn_weight_loss_ever'

#with open(dir + f'{label}.pkl', 'rb') as f:
#  detn = pickle.load(f)
detn = read_detn(label)

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 12
MUAC_CUTOFF = 12.7

detn = detn[((detn['wk1_b_discharged']==0) & (detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF)) ].copy()


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn = reduce_dimensionality(detn,['household_adults','household_slept','living_children'],'household_adults_slept_living_children_z')
detn = reduce_dimensionality(detn,['resp_rate', 'temperature'],'resp_rate_temperature')

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

# infer
y_pred_proba_all = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)


# survival
logger.debug(f'{detn[label].sum()},{detn.shape}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn[label].sum()},{detn.shape}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn[label].sum()},{detn.shape}')

# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(detn[label_date].notnull().sum())


# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

"""### AFT survival regression"""

selected_columns = ['weight_trend','pid']


X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regression_dataset = regression_dataset.dropna()

aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True,formula='weight_trend')
logger.debug(aft.print_summary())

"""### censored (i.e., patients w/o weight loss yet) survival prediction"""

censored_subjects=data_x_numeric.join(data_y)

censored_subjects['weight_trend'].fillna(0,inplace=True)

censored_subjects = censored_subjects.loc[censored_subjects[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]

y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','weight_trend',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)

# muac loss 2 weeks consecutive

label = 'muac_loss_2_weeks_consecutive'

#with open(dir + f'{label}.pkl', 'rb') as f:
#  detn = pickle.load(f)

detn = read_detn(label)


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 12
MUAC_CUTOFF = 12.7

# prompt: detn where wk1_muac between 11.5 and 12.6 and wk1_calc_los between 10 and 12

detn = detn[((detn['wk1_b_discharged']==0) & (detn['weekly_last_muac']< MUAC_CUTOFF) & (detn['wk1_calc_los']< LOS_CUTOFF)) ].copy()


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn = reduce_dimensionality(detn,['household_adults','household_slept','living_children'],'household_adults_slept_living_children_z')



detn = reduce_dimensionality(detn,['weekly_avg_muac','weekly_last_wfh',],'muac_wfh_z')

detn = reduce_dimensionality(detn,['wk1_muac_diff_rate','muac_diff_ratio_rate','muac_diff_ratio'],'muac_diff_rate_ratio_z')


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

# run inference
y_pred_proba_all = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)

# survival
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn.shape,detn[label].sum()}')

label_date = f'{label}_date'

detn.loc[detn[label] == 0, label_date] = None

# drop those w/o a visit, hence no lifetime to be plotted)
detn = detn[(detn[label_date].notnull()) | (detn['wk1_calcdate_weekly'].notnull())].copy()
logger.debug(f'{detn.shape,detn[label].sum()}')

# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

logger.debug(f'{detn.shape,detn[label].sum()}')

# prompt: for rows in detn where label==1 set duration_days to label_date - calcdate

# Calculate duration_days only for rows where label == 1
detn.loc[detn[label] == 1, 'duration_days'] = (detn.loc[detn[label] == 1, label_date] - detn.loc[detn[label] == 1, 'calcdate']).dt.days

detn['duration_days'] = detn['duration_days'].clip(lower=0.1)

selected_columns = ['wfh_trend','pid']


X = detn[selected_columns].copy()

data_y = detn[[label,'duration_days']].copy()
data_x_numeric = X.copy()
data_y = detn[[label,'duration_days']].copy()

regression_dataset=data_x_numeric.join(data_y)

regression_dataset = regression_dataset.dropna()

aft = WeibullAFTFitter()
aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True,formula='wfh_trend')
logger.debug(aft.print_summary())


censored_subjects=data_x_numeric.join(data_y)

censored_subjects['wfh_trend'].fillna(0,inplace=True)

censored_subjects = censored_subjects.loc[censored_subjects[label]==0].copy()

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)

censored_subjects_last_obs = censored_subjects[duration_days_col]

y_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_median.rename(f'median_days_to_{label}',inplace=True)

df = pd.merge(censored_subjects[['pid','wfh_trend',duration_days_col]],y_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)

label = 'nonresponse'

detn = read_detn(label)


logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

# prompt: create column in detn called final_date which is status_date or wk1_calcdate_weekly whichever is greater
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn[label].sum()},{detn.shape}')


# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

# prompt: only keep detn rows where duration between label==1 min(duration_days) and max(duration_days)
min_duration = detn.groupby(label)['duration_days'].min()[1]
max_duration = detn.groupby(label)['duration_days'].max()[1]

detn_filtered = detn[
    (detn['duration_days'] >= min_duration) &
    (detn['duration_days'] <= max_duration)
]

detn = detn_filtered.copy()
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

detn = reduce_dimensionality(detn,['household_adults','household_slept','living_children'],'household_adults_slept_living_children_z')

detn = reduce_dimensionality(detn,['weekly_avg_muac','weekly_last_wfh',],'muac_wfh')

detn['muac_diff_ratio'] = -detn['muac_diff_ratio']
detn['muac_diff_ratio_rate'] = -detn['muac_diff_ratio_rate']


detn = reduce_dimensionality(detn,['muac_diff_ratio','muac','muac_diff_ratio_rate'],'muac_diff_ratio_rate_z')
detn = reduce_dimensionality(detn,['duration_days','wk1_calc_los'],'duration_z')
#detn.drop(columns=['duration_days','wk1_calc_los'],inplace=True)
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

y_pred_proba_all = run_ag_model(label,detn,'')

pred_proba_all_series = y_pred_proba_all[1].rename(f'probability_{label}')
pid_probabilities = pid_probabilities.join(pred_proba_all_series)
logger.debug(pid_probabilities.shape)

label = 'status_dead'

detn = read_detn(label)

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

# TODO move this to etl_deterioration
detn = pd.merge(detn, admit_current[['pid','status','status_date']], on='pid', how='inner')
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
# Create 'final_date' column, choosing the later date between 'status_date' and 'wk1_calcdate_weekly'
detn['final_date'] = detn[['status_date', 'wk1_calcdate_weekly']].max(axis=1)

# Fill NaN (active patients w/o a visit) values in 'final_date' with today's date
detn['final_date'] = detn['final_date'].fillna(datetime.date.today())
detn['final_date'] = pd.to_datetime(detn['final_date'])
detn['duration_days'] = detn['final_date'] - detn['calcdate']
detn['duration_days'] = detn['duration_days'].dt.days

label_date = f'{label}_date'
# for the majority that are missing label_date
detn['duration_days'] = detn['calcdate'].max() - detn['calcdate']

# overwrite only for those with non-null label_date
detn.loc[detn[label_date].notnull(), 'duration_days'] = detn.loc[detn[label_date].notnull(), label_date] - detn.loc[detn[label_date].notnull(), 'calcdate']

detn['duration_days'] = detn['duration_days'].dt.days

detn['wk1_calc_los'].fillna(0,inplace=True)

LOS_CUTOFF = 11
MUAC_CUTOFF = 12.1
NULL_MUAC_LOS_CUTOFF = 4
DURATION_DAYS_CUTOFF = 101

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

detn = detn[(((detn['weekly_last_muac'].isnull()) & (detn['wk1_calc_los'] < NULL_MUAC_LOS_CUTOFF)) & (detn['duration_days'] < DURATION_DAYS_CUTOFF) | ((detn['weekly_last_muac'] < MUAC_CUTOFF) & (detn['wk1_calc_los'] < LOS_CUTOFF)))]

logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')

detn = reduce_dimensionality(detn,['household_adults','household_slept','living_children'],'household_adults_slept_living_children_z')

detn = reduce_dimensionality(detn,['resp_rate', 'temperature'],'resp_rate_temperature_z')
detn = reduce_dimensionality(detn,['weekly_avg_muac','weekly_last_wfh',],'muac_wfh')

detn['muac_diff_ratio'] = -detn['muac_diff_ratio']

detn = reduce_dimensionality(detn,['muac_diff_ratio','muac'],'muac_diff_ratio_z')
logger.debug(f'{detn.shape,detn[label].sum()},{detn[label].mean()}')
detn = reduce_dimensionality(detn,['wfa_trend','hfa_trend'],'wfa_hfa_trend_z')
detn = reduce_dimensionality(detn,['cat1_complications_weekly','admit_cat1_complications'],'cat1_complications_z')
detn = reduce_dimensionality(detn,['wk1_rainy_season_weekly','lean_season_admit'],'season_z')
detn = reduce_dimensionality(detn,['wfh_rsquared','wfh_trend'],'wfh_trend_z')

# prompt: clip lower boundary of detn['duration_days'] to 0

detn['duration_days'] = detn['duration_days'].clip(lower=0)

detn_admit_only,_,_,_ = split_detn_new_onset_medical_complication(detn,label)
logger.debug(f'{detn_admit_only.shape,detn_admit_only[label].sum()},{detn_admit_only[label].mean()}')

# Find 'pid' values in detn that are NOT in detn_admit_only
pid_not_in_admit = detn[~detn['pid'].isin(detn_admit_only['pid'])]['pid']

# Get rows from detn where 'pid' is in pid_not_in_admit
detn_filtered = detn[detn['pid'].isin(pid_not_in_admit)].copy()
detn_filtered.replace([np.inf, -np.inf], np.nan, inplace=True)


y_pred_proba_all1 = run_ag_model(label,detn_admit_only,'1')
y_pred_proba_all2 = run_ag_model(label,detn_filtered,'not1')

y_pred_proba_all_stratified = pd.concat([y_pred_proba_all1,y_pred_proba_all2],axis=0)
y_pred_proba_all_stratified_series = y_pred_proba_all_stratified[1].rename(f'probability_{label}_stratified')
pid_probabilities = pid_probabilities.join(y_pred_proba_all_stratified_series)
logger.debug(pid_probabilities.shape)

# clip for better visualization and so AFT models will work, guaranteeing all durations >0
#max_duration = detn[detn[label]==1]['duration_days'].max() +1


detn['duration_days'] = detn['duration_days'].clip(lower=.01)


regr_cols = ['wk1_muac','muac',label,'duration_days','pid']
regression_dataset = detn[regr_cols].copy()
regression_dataset['wk1_muac'].fillna(regression_dataset['muac'],inplace=True)
regression_dataset.drop(columns=['muac'],inplace=True)

regression_dataset.dropna(inplace=True)


#regression_dataset['duration_days'] = regression_dataset['duration_days'].clip(lower=.01)
aft = WeibullAFTFitter()

aft.fit(regression_dataset, duration_col='duration_days', event_col=f'{label}', ancillary=True, formula=' wk1_muac')

logger.debug(aft.print_summary())


regr_cols = ['wk1_muac','muac',label,'duration_days','pid']
censored_subjects = detn[regr_cols].copy()
censored_subjects['wk1_muac'].fillna(censored_subjects['muac'],inplace=True)
censored_subjects.drop(columns=['muac'],inplace=True)


censored_subjects.dropna(inplace=True)

censored_subjects = censored_subjects.loc[detn[label]==0].copy()

censored_subjects_last_obs = censored_subjects['duration_days']
censored_subjects_last_obs = censored_subjects_last_obs.clip(lower=0.01)

y_pred_median = aft.predict_median(censored_subjects,conditional_after=censored_subjects_last_obs)
y_pred_median.rename('median_predicted_days_to_death',inplace=True)

duration_days_col = f'duration_days_{label}'

censored_subjects.rename(columns={'duration_days':duration_days_col},inplace=True)


df = pd.merge(censored_subjects[['pid',duration_days_col]],y_pred_median,left_index=True,right_index=True)

pid_probabilities =pd.merge(pid_probabilities,df,on='pid',how='left')
logger.debug(pid_probabilities.shape)

# TODO write pid_probabilities to Postgres table
#pid_probabilities.to_excel('pid_probabilities8.xlsx',index=None)
