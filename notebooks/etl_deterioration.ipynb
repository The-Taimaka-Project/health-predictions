{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2HISbIusYHQ"
      },
      "source": [
        "# Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CyFVx57AhLE"
      },
      "source": [
        "deterioration definition and analysis\n",
        "\n",
        "Deterioration includes any of the following, alone or in combination:\n",
        "* Death\n",
        "* New onset medical complication - 'cat1' complication (see vars in raw ODK data with cat1_ prefix)\n",
        "** cat1 complications include lack of appetite (anorexia)\n",
        "** Note: cat1 complications prompt ITP referral in current ODK setup, though sometimes the caregiver or family members refuse referral\n",
        "* Poor weight gain\n",
        "** Weight at week 3 is lower than weight at admission\n",
        "** Weight loss for 3 consecutive weeks (not related to loss of oedema)\n",
        "** Static weight or weight loss for 4 consecutive weeks\n",
        "** Poor weight gain (lt 5 g/kg/day) for 4 consecutive weeks\n",
        "* Failure to lose oedema\n",
        "** New appearance of oedema (onset of oedema when previously absent)\n",
        "** Worsening/increase of oedema (grade 1-->2, 2-->3)\n",
        "** Oedema not disappearing/reducing at 3rd week after initial appearance (static grade of oedema)\n",
        "* Poor MUAC gain\n",
        "** Static MUAC or MUAC loss for 2 consecutive weeks\n",
        "* Discharge as not responding to treatment (status == ‘nonresponse’)\n",
        "\n",
        "Note that deterioration in WHZ/WLZ (increased wasting), WAZ (increased underweight), HAZ (increased stunting) are NOT included -- my guess is that this is because they are difficult for workers to calculate in a typical OTP setup??? But it would be reasonably easy to include in this analysis if we set a threshold.\n",
        "\n",
        "In general, complications requiring hospitalization (cat1) are pretty well defined in our program + data. However, poor weight gain, failure to lose oedema, and poor MUAC gain are less well defined (i.e., definitions aren't already built into the health worker ODK).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v78QoCF2BY8g"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyNN1piGxxrh",
        "outputId": "0fa477eb-6f8e-4d49-b4d6-b108eecccd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'health-predictions'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 206 (delta 39), reused 17 (delta 13), pack-reused 136 (from 1)\u001b[K\n",
            "Receiving objects: 100% (206/206), 724.96 KiB | 2.26 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "/content/health-predictions/packages/inference/run\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone -b brian-etl-code https://github.com/The-Taimaka-Project/health-predictions.git\n",
        "#!git clone https://github.com/The-Taimaka-Project/health-predictions.git\n",
        "\n",
        "# Change directory to the repository\n",
        "%cd health-predictions/taimaka_health_predictions/inference\n",
        "%pwd\n",
        "from util import regress, convert_bool_to_int, infer_phq_score\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNVaF5mFlIj4"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "# Create a logger# Create a logger\n",
        "logger = logging.getLogger('my_logger')\n",
        "logger.setLevel(logging.DEBUG) # Set the minimum logging level\n",
        "\n",
        "# Create a handler to output logs to the console\n",
        "console_handler = logging.StreamHandler()\n",
        "\n",
        "file_handler = logging.FileHandler('my_log.log')\n",
        "file_handler.setLevel(logging.INFO) # Set the logging level for the handler\n",
        "\n",
        "\n",
        "# Create a formatter to specify the log message format\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(lineno)d - %(funcName)s - %(message)s')\n",
        "\n",
        "# Add the formatter to the handler\n",
        "\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the logger\n",
        "logger.addHandler(file_handler)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkJoF7IUAjEf",
        "outputId": "ed0f64dd-a886-4cba-b430-f39352d086e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import statsmodels.formula.api as smf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from warnings import simplefilter\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "# prompt: read google shared drive file\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "dir = \"/content/drive/My Drive/[PBA] Data/\"\n",
        "\n",
        "os.chdir(\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0T2gK2qsPlK"
      },
      "outputs": [],
      "source": [
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "simplefilter(action=\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3VfYoq3XhS5"
      },
      "outputs": [],
      "source": [
        "# Load the pickle file\n",
        "with open(dir + \"analysis/admit_weekly.pkl\", \"rb\") as f:\n",
        "    admit_weekly = pickle.load(f)\n",
        "with open(dir + \"analysis/admit_processed_raw.pkl\", \"rb\") as f:\n",
        "    admit_raw = pickle.load(f)\n",
        "with open(dir + \"analysis/admit_current_mh.pkl\", \"rb\") as f:\n",
        "    admit_current_mh = pickle.load(f)\n",
        "\n",
        "with open(dir + \"analysis/admit_current.pkl\", \"rb\") as f:\n",
        "    admit_current = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LmzLuaF1ebi"
      },
      "outputs": [],
      "source": [
        "#admit_weekly = admit_weekly.astype(admit_weekly_dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27TKvKPwbLz0"
      },
      "outputs": [],
      "source": [
        "numeric_cols = admit_weekly.select_dtypes(include=np.number).columns\n",
        "numeric_cat1_cols = [col for col in admit_weekly.columns if col.startswith(\"cat1_\")]\n",
        "numeric_cat2_cols = [col for col in admit_weekly.columns if col.startswith(\"cat2_\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aan52xT1kG0j"
      },
      "source": [
        "# Discharge as not responding to treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RELbm5RAk-Ou"
      },
      "source": [
        "status == 'nonresponse'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cfiePwSkOkT"
      },
      "outputs": [],
      "source": [
        "# prompt: find rows with current_status == 'nonresponse'\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "# Find rows where 'current_status' is 'nonresponse'\n",
        "nonresponse_rows = admit_weekly[admit_weekly[\"status\"] == \"nonresponse\"]\n",
        "\n",
        "admit_weekly[\"nonresponse\"] = admit_weekly[\"status\"] == \"nonresponse\"\n",
        "\n",
        "# Display or further process the nonresponse rows\n",
        "# print(nonresponse_rows)\n",
        "\n",
        "# Get the unique PIDs of patients with 'current_status' as 'nonresponse'\n",
        "pids_nonresponse = nonresponse_rows[\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAAM2j8tlSca"
      },
      "source": [
        "# Death"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYkR10u0lmPz",
        "outputId": "5c8eebc7-3d39-4bed-9c46-b96b88ec1e51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:253\n"
          ]
        }
      ],
      "source": [
        "dead_rows = admit_weekly[admit_weekly[\"status_dead\"] == True]\n",
        "# Get the unique PIDs of patients with 'current_status' as 'dead'\n",
        "pids_dead = dead_rows[\"pid\"].unique()\n",
        "logger.debug(len(pids_dead))\n",
        "# admit_weekly.loc[admit_weekly['status_dead'] == True , 'status_dead_date'] = admit_weekly.loc[admit_weekly['status_dead'] == True, 'status_date']\n",
        "\n",
        "# admit_weekly['status_dead_date'].notnull().sum()\n",
        "\n",
        "# first_date_series = get_first_detn_date(admit_weekly,'status_dead',date_col='status_date')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0siz-Erljab"
      },
      "source": [
        "# only data with visits filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QV3VrnX-k0p",
        "outputId": "bdf42c62-f872-460d-d818-9712a65499d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:10322, (66693, 1519)\n",
            "DEBUG:my_logger:Unique PIDs in admit_weekly_no_weekly: 575, Shape of admit_weekly_no_weekly: (575, 1519)\n",
            "DEBUG:my_logger:Difference in unique PIDs between admit_weekly and admit_weekly_no_weekly: 9747\n"
          ]
        }
      ],
      "source": [
        "# prompt: get admit_weekly where calcdate_weekly is null\n",
        "\n",
        "# Assuming admit_weekly is already loaded as in the provided code\n",
        "logger.debug(f\"{admit_weekly['pid'].nunique()}, {admit_weekly.shape}\")\n",
        "# Filter for rows where 'calcdate_weekly' is null\n",
        "admit_weekly_no_weekly = admit_weekly[admit_weekly[\"calcdate_weekly\"].isnull()].copy()\n",
        "logger.debug(f\"Unique PIDs in admit_weekly_no_weekly: {admit_weekly_no_weekly['pid'].nunique()}, Shape of admit_weekly_no_weekly: {admit_weekly_no_weekly.shape}\")\n",
        "logger.debug(f\"Difference in unique PIDs between admit_weekly and admit_weekly_no_weekly: {admit_weekly['pid'].nunique() - admit_weekly_no_weekly['pid'].nunique()}\")\n",
        "pids_with_visits = list(\n",
        "    set(admit_weekly[\"pid\"].unique()) - set(admit_weekly_no_weekly[\"pid\"].unique())\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znBrZO6aus-S"
      },
      "outputs": [],
      "source": [
        "# prompt: drop admit_weekly rows where calcdate_weekly is null as we're only interested in visit time sequences\n",
        "admit_weekly_all = admit_weekly.copy()  # save all as death and nonresponse happen at admission, too\n",
        "\n",
        "# Drop rows where 'calcdate_weekly' is null\n",
        "admit_weekly.dropna(subset=[\"calcdate_weekly\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKJqODuCtJeq"
      },
      "outputs": [],
      "source": [
        "# prompt: get max sequence_num by pid\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded and processed as in the provided code.\n",
        "\n",
        "# Group by 'pid' and get the maximum 'sequence_num' for each 'pid'\n",
        "max_sequence_num_by_pid = admit_weekly.groupby(\"pid\")[\"sequence_num\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxeyNnHE9q7q"
      },
      "outputs": [],
      "source": [
        "# prompt: left join admit_weekly to max_sequence_num_by_pid on pid\n",
        "\n",
        "# Merge the DataFrames\n",
        "admit_weekly = pd.merge(\n",
        "    admit_weekly,\n",
        "    max_sequence_num_by_pid.rename(\"max_sequence_num\"),\n",
        "    left_on=\"pid\",\n",
        "    right_index=True,\n",
        "    how=\"left\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlbEpi8ipWtK"
      },
      "source": [
        "# Poor Weight Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FazHCvKC8RqD"
      },
      "outputs": [],
      "source": [
        "# prompt: get admit_weekly where sequence_num ==3\n",
        "\n",
        "\n",
        "# Filter for sequence_num == 3\n",
        "admit_weekly_seq3 = admit_weekly[admit_weekly[\"sequence_num\"] == 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zi9FdAZ8YQh"
      },
      "source": [
        "* Weight at week 3 is lower than weight at admission\n",
        "* Weight loss for 3 consecutive weeks (not related to loss of oedema)\n",
        "* Static weight or weight loss for 4 consecutive weeks\n",
        "* Poor weight gain (<5 g/kg/day) for 4 consecutive weeks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPf6izqsJcSN"
      },
      "outputs": [],
      "source": [
        "# get prior weight, lag_1\n",
        "admit_weekly[f\"weight_weekly_lag_1\"] = admit_weekly.groupby(\"pid\")[\"weight_weekly\"].shift(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQputM7KG8Lm"
      },
      "outputs": [],
      "source": [
        "# prompt: Static weight or weight loss for 4 consecutive weeks\n",
        "\n",
        "\n",
        "# Static weight or weight loss for 4 consecutive weeks\n",
        "def static_or_weight_loss_4_weeks(df):\n",
        "    # Create a boolean Series indicating whether the weight is static or decreased compared to 4 weeks prior\n",
        "    df[\"static_weight_loss_1w\"] = df[\"weight_weekly\"] <= df[\"weight_weekly_lag_1\"]\n",
        "\n",
        "    # Group by 'pid' and check for 4 consecutive True values in 'static_or_loss_4w'\n",
        "    # Using rolling window to check consecutive values\n",
        "    static_or_loss_4w_consecutive = (\n",
        "        df.groupby(\"pid\")[\"static_weight_loss_1w\"]\n",
        "        .rolling(window=4, min_periods=4)\n",
        "        .apply(lambda x: all(x), raw=True)\n",
        "    )\n",
        "\n",
        "    # Instead of direct assignment, use reset_index to align the index:\n",
        "    df[\"static_or_weight_loss_4_weeks\"] = static_or_loss_4w_consecutive.reset_index(\n",
        "        level=0, drop=True\n",
        "    ).fillna(False)\n",
        "    pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "    # prompt: convert 0 to False and 1 to True in admit_weekly['static_or_loss_4w_consecutive']\n",
        "    df[\"static_or_weight_loss_4_weeks\"] = df[\"static_or_weight_loss_4_weeks\"].replace(\n",
        "        {0: False, 1: True}\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "admit_weekly = static_or_weight_loss_4_weeks(admit_weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GuWIOMTodRr"
      },
      "outputs": [],
      "source": [
        "# prompt: Weight at week 3 is lower than weight at admission\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "# Filter for sequence_num == 3\n",
        "admit_weekly_seq3 = admit_weekly[admit_weekly[\"sequence_num\"] == 3]\n",
        "\n",
        "# Compare weight at week 3 to weight at admission\n",
        "admit_weekly_seq3[\"weight_at_week3_lower_than_admission\"] = (\n",
        "    admit_weekly_seq3[\"weight_weekly\"] < admit_weekly_seq3[\"weight_admit_current\"]\n",
        ")\n",
        "\n",
        "\n",
        "# prompt: join admit_weekly_seq3 to admit_weekly on ['pid', 'calcdate_weekly']\n",
        "# propagates future into the past however, so this is a data leak\n",
        "admit_weekly = pd.merge(\n",
        "    admit_weekly,\n",
        "    admit_weekly_seq3[[\"pid\", \"calcdate_weekly\", \"weight_at_week3_lower_than_admission\"]],\n",
        "    on=[\"pid\", \"calcdate_weekly\"],\n",
        "    how=\"left\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJwHU1muUQFQ"
      },
      "outputs": [],
      "source": [
        "# prompt: Poor weight gain (<5 g/kg/day) for 4 consecutive weeks\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "\n",
        "def poor_weight_gain_4_weeks(df):\n",
        "    # Calculate weight gain per day\n",
        "    # df['weight_gain_per_day'] = df.groupby('pid')['weight_weekly'].diff() / 7  # Assuming weekly measurements\n",
        "    df[\"weight_gain_per_day\"] = (\n",
        "        df[\"weight_diff_weekly\"] * 1000 / df[\"weight_weekly\"] / df[\"calcdate_diff_weekly\"]\n",
        "    )\n",
        "\n",
        "    # Check for poor weight gain (<5 g/kg/day) for 4 consecutive weeks\n",
        "    df[\"poor_weight_gain\"] = df[\"weight_gain_per_day\"] < 5  # Adjust 5 based on your requirement\n",
        "\n",
        "    # Group by 'pid' and check for 4 consecutive True values in 'poor_weight_gain'\n",
        "    poor_weight_gain_4w_consecutive = (\n",
        "        df.groupby(\"pid\")[\"poor_weight_gain\"]\n",
        "        .rolling(window=4, min_periods=4)\n",
        "        .apply(lambda x: all(x), raw=True)\n",
        "    )\n",
        "\n",
        "    # Assign the result back to the DataFrame, handling potential index mismatches\n",
        "    df[\"poor_weight_gain_4_weeks\"] = poor_weight_gain_4w_consecutive.reset_index(\n",
        "        level=0, drop=True\n",
        "    ).fillna(False)\n",
        "    df[\"poor_weight_gain_4_weeks\"] = df[\"poor_weight_gain_4_weeks\"].replace({0: False, 1: True})\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "admit_weekly = poor_weight_gain_4_weeks(admit_weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAPe8Elpbo4S"
      },
      "outputs": [],
      "source": [
        "# prompt: Weight loss for 3 consecutive weeks (not related to loss of oedema)\n",
        "\n",
        "\n",
        "# Weight loss for 3 consecutive weeks (not related to loss of oedema)\n",
        "def weight_loss_3_consecutive_weeks(df):\n",
        "    # Check for weight loss in three consecutive weeks\n",
        "\n",
        "    # Create a boolean Series indicating whether the weight decreased compared to prior\n",
        "    df[\"strict_weight_loss_1w\"] = df[\"weight_weekly\"] < df[\"weight_weekly_lag_1\"]\n",
        "\n",
        "    weight_loss_3_weeks = (\n",
        "        df.groupby(\"pid\")[\"strict_weight_loss_1w\"]\n",
        "        .rolling(window=3, min_periods=3)\n",
        "        .apply(lambda x: all(x), raw=True)\n",
        "    )\n",
        "\n",
        "    # Instead of direct assignment, use reset_index to align the index:\n",
        "    df[\"weight_loss_3_weeks\"] = weight_loss_3_weeks.reset_index(level=0, drop=True).fillna(False)\n",
        "    pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "    # prompt: convert 0 to False and 1 to True in admit_weekly['weight_loss_3_weeks']\n",
        "    df[\"weight_loss_3_weeks\"] = df[\"weight_loss_3_weeks\"].replace({0: False, 1: True})\n",
        "    # not related to loss of oedema\n",
        "    df[\"weight_loss_3_weeks\"] = (df[\"weight_loss_3_weeks\"]) & (df[\"cat2_oedema_weekly\"] == False)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "admit_weekly = weight_loss_3_consecutive_weeks(admit_weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm1yIt2VtgNr"
      },
      "outputs": [],
      "source": [
        "# prompt: get the row with max calcdate_weekly for a pid and then select those where the max row admit_weekly['static_or_weight_loss_4_weeks'] == True\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "# Group by 'pid' and get the row with the maximum 'calcdate_weekly' for each 'pid'\n",
        "max_calcdate_rows = admit_weekly.loc[admit_weekly.groupby(\"pid\")[\"calcdate_weekly\"].idxmax()]\n",
        "\n",
        "# Filter the rows where 'static_or_weight_loss_4_weeks' is True in the max 'calcdate_weekly' rows\n",
        "pids_static_or_weight_loss_4_weeks_latest = max_calcdate_rows[\n",
        "    max_calcdate_rows[\"static_or_weight_loss_4_weeks\"] == True\n",
        "][\"pid\"].unique()\n",
        "pids_poor_weight_gain_4_weeks_latest = max_calcdate_rows[\n",
        "    max_calcdate_rows[\"poor_weight_gain_4_weeks\"] == True\n",
        "][\"pid\"].unique()\n",
        "pids_weight_loss_3_weeks_latest = max_calcdate_rows[\n",
        "    max_calcdate_rows[\"weight_loss_3_weeks\"] == True\n",
        "][\"pid\"].unique()\n",
        "\n",
        "pids_static_or_weight_loss_4_weeks = admit_weekly[\n",
        "    admit_weekly[\"static_or_weight_loss_4_weeks\"] == True\n",
        "][\"pid\"].unique()\n",
        "pids_poor_weight_gain_4_weeks = admit_weekly[admit_weekly[\"poor_weight_gain_4_weeks\"] == True][\n",
        "    \"pid\"\n",
        "].unique()\n",
        "pids_weight_loss_3_weeks = admit_weekly[admit_weekly[\"weight_loss_3_weeks\"] == True][\"pid\"].unique()\n",
        "pids_weight_at_week3_lower_than_admission = admit_weekly[\n",
        "    admit_weekly[\"weight_at_week3_lower_than_admission\"] == True\n",
        "][\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5X1nJnNHCkM"
      },
      "outputs": [],
      "source": [
        "# prompt: admit_weekly[weight_loss_ever] = (static_or_weight_loss_4_weeks | poor_weight_gain_4_weeks | weight_loss_3_weeks| weight_at_week3_lower_than_admission)\n",
        "\n",
        "admit_weekly[\"detn_weight_loss_ever\"] = (\n",
        "    admit_weekly[\"static_or_weight_loss_4_weeks\"]\n",
        "    | admit_weekly[\"poor_weight_gain_4_weeks\"]\n",
        "    | admit_weekly[\"weight_loss_3_weeks\"]\n",
        "    | admit_weekly[\"weight_at_week3_lower_than_admission\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG61CXraHxvc"
      },
      "outputs": [],
      "source": [
        "# prompt: admit_weekly[detn_weight_loss_latest] = (static_or_weight_loss_4_weeks | poor_weight_gain_4_weeks | weight_loss_3_weeks| weight_at_week3_lower_than_admission) & sequence_num == max_sequence_num\n",
        "\n",
        "admit_weekly[\"detn_weight_loss_latest\"] = (\n",
        "    admit_weekly[\"static_or_weight_loss_4_weeks\"]\n",
        "    | admit_weekly[\"poor_weight_gain_4_weeks\"]\n",
        "    | admit_weekly[\"weight_loss_3_weeks\"]\n",
        "    | admit_weekly[\"weight_at_week3_lower_than_admission\"]\n",
        ") & (admit_weekly[\"sequence_num\"] == admit_weekly[\"max_sequence_num\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlejrr7NILl8"
      },
      "outputs": [],
      "source": [
        "# prompt: pids_weight_loss_latest = admit_weekly[detn_weight_loss_latest] == True\n",
        "\n",
        "pids_weight_loss_latest = admit_weekly[admit_weekly[\"detn_weight_loss_latest\"] == True][\n",
        "    \"pid\"\n",
        "].unique()\n",
        "\n",
        "# prompt: pids_weight_loss_latest =  set of pids_static_or_weight_loss_4_weeks_latest, pids_poor_weight_gain_4_weeks_latest, pids_weight_loss_3_weeks_latest\n",
        "# TODO why are 8 less when doing this way?\n",
        "# pids_weight_loss_latest = list(set(list(pids_static_or_weight_loss_4_weeks_latest) + list(pids_poor_weight_gain_4_weeks_latest) + list(pids_weight_loss_3_weeks_latest)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvmz-RvbI56e"
      },
      "outputs": [],
      "source": [
        "pids_weight_loss_ever = admit_weekly[admit_weekly[\"detn_weight_loss_ever\"] == True][\"pid\"].unique()\n",
        "# pids_weight_loss_ever = list(set(list(pids_static_or_weight_loss_4_weeks) + list(pids_poor_weight_gain_4_weeks) + list(pids_weight_loss_3_weeks) + list(pids_weight_at_week3_lower_than_admission)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3UGbMdRDC2s"
      },
      "source": [
        "# cat1 complications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsOEL-uTSPWG"
      },
      "source": [
        "## count cat1, cat2 occurrences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emzwo8sYRopG"
      },
      "outputs": [],
      "source": [
        "# get the column names\n",
        "# Filter columns that contain 'cat1' and end with '_weekly'\n",
        "cat1_weekly_cols = [\n",
        "    col for col in admit_weekly.columns if \"cat1\" in col and col.endswith(\"_weekly\")\n",
        "]\n",
        "cat2_weekly_cols = [\n",
        "    col for col in admit_weekly.columns if \"cat2\" in col and col.endswith(\"_weekly\")\n",
        "]\n",
        "\n",
        "cat1_weekly_cols = admit_weekly[cat1_weekly_cols].select_dtypes(include=[\"bool\"]).columns\n",
        "cat2_weekly_cols = admit_weekly[cat2_weekly_cols].select_dtypes(include=[\"bool\"]).columns\n",
        "\n",
        "# Filter admit_raw columns that contain 'cat1' and can be summed\n",
        "cat1_cols = (\n",
        "    admit_raw[[col for col in admit_raw.columns if \"cat1\" in col]]\n",
        "    .select_dtypes(include=[\"bool\"])\n",
        "    .columns\n",
        ")\n",
        "cat2_cols = (\n",
        "    admit_raw[[col for col in admit_raw.columns if \"cat2\" in col]]\n",
        "    .select_dtypes(include=[\"bool\"])\n",
        "    .columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_I1OFpRJsTM"
      },
      "outputs": [],
      "source": [
        "# prompt: get columns that contain cat1 and end in _weekly from admit_weekly\n",
        "# prompt: sum cat1_cols by pid\n",
        "\n",
        "# Group by 'pid' and sum the 'cat1' columns\n",
        "cat1_sum_by_pid = admit_raw.groupby(\"pid\")[cat1_cols].sum()\n",
        "# Calculate the sum of each row in cat1_sum_by_pid\n",
        "cat1_sum_by_pid = cat1_sum_by_pid.sum(axis=1)\n",
        "cat1_sum_by_pid.name = \"admit_cat1_complications\"\n",
        "# Group by 'pid' and sum the 'cat2' columns\n",
        "cat2_sum_by_pid = admit_raw.groupby(\"pid\")[cat2_cols].sum()\n",
        "# Calculate the sum of each row in cat2_sum_by_pid\n",
        "cat2_sum_by_pid = cat2_sum_by_pid.sum(axis=1)\n",
        "cat2_sum_by_pid.name = \"admit_cat2_complications\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZQzhKwxS7gM"
      },
      "outputs": [],
      "source": [
        "def count_cat1_cat2(admit_weekly, cat1_weekly_cols, cat2_weekly_cols):\n",
        "    # Group by 'pid' and sum the 'cat1' columns\n",
        "    cat1_sum_by_pid_weekly = admit_weekly.groupby(\"pid\")[cat1_weekly_cols].sum()\n",
        "    # Calculate the sum of each row in cat1_sum_by_pid\n",
        "    cat1_sum_by_pid_weekly = cat1_sum_by_pid_weekly.sum(axis=1)\n",
        "    # Group by 'pid' and sum the 'cat2' columns\n",
        "    cat2_sum_by_pid_weekly = admit_weekly.groupby(\"pid\")[cat2_weekly_cols].sum()\n",
        "    # Calculate the sum of each row in cat2_sum_by_pid\n",
        "    cat2_sum_by_pid_weekly = cat2_sum_by_pid_weekly.sum(axis=1)\n",
        "    cat1_sum_by_pid_weekly.name = \"cat1_complications_weekly\"\n",
        "    cat2_sum_by_pid_weekly.name = \"cat2_complications_weekly\"\n",
        "    return cat1_sum_by_pid_weekly, cat2_sum_by_pid_weekly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPsa-RniLj2M"
      },
      "source": [
        "New onset medical complication - 'cat1' complication (see vars in raw ODK data with cat1_ prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED-q-UJYMYI6"
      },
      "outputs": [],
      "source": [
        "# prompt: append _weekly to weekly_raw cat1 columns\n",
        "\n",
        "\n",
        "cat1_weekly = [\n",
        "    col + \"_weekly\"\n",
        "    for col in [\n",
        "        \"cat1_fever\",\n",
        "        \"cat1_hypothermia\",\n",
        "        \"cat1_measles\",\n",
        "        \"cat1_breath\",\n",
        "        \"cat1_vomiting\",\n",
        "        \"cat1_bloodstool\",\n",
        "        \"cat1_dehyd\",\n",
        "        \"cat1_fissures\",\n",
        "        \"cat1_orash\",\n",
        "        \"cat1_ears\",\n",
        "        \"cat1_noeat\",\n",
        "        \"cat1_notests\",\n",
        "        \"cat1_anemia\",\n",
        "        \"cat1_overall\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5yyAk4ENgjk"
      },
      "outputs": [],
      "source": [
        "# prompt: lag each column in cat1_weekly\n",
        "\n",
        "# Assuming 'admit_weekly' DataFrame and 'cat1_weekly' list are already defined as in the previous code.\n",
        "\n",
        "for col in cat1_weekly:\n",
        "    admit_weekly[f\"{col}_lag_1\"] = admit_weekly.groupby(\"pid\")[col].shift(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCnaQLPuyujZ"
      },
      "outputs": [],
      "source": [
        "# prompt: create a map of selected_columns to cat1_weekly\n",
        "# Create a dictionary to map selected_columns to their corresponding weekly columns\n",
        "col_map = {\n",
        "    \"cat1_fever_admit_current\": \"cat1_fever_weekly\",\n",
        "    \"cat1_hypothermia_admit_current\": \"cat1_hypothermia_weekly\",\n",
        "    \"cat1_measles_admit_current\": \"cat1_measles_weekly\",\n",
        "    \"cat1_breath_admit_current\": \"cat1_breath_weekly\",\n",
        "    \"cat1_vomiting_admit_current\": \"cat1_vomiting_weekly\",\n",
        "    \"cat1_bloodstool_admit_current\": \"cat1_bloodstool_weekly\",\n",
        "    \"cat1_dehyd_admit_current\": \"cat1_dehyd_weekly\",\n",
        "    \"cat1_fissures_admit_current\": \"cat1_fissures_weekly\",\n",
        "    \"cat1_orash_admit_current\": \"cat1_orash_weekly\",\n",
        "    \"cat1_ears_admit_current\": \"cat1_ears_weekly\",\n",
        "    \"cat1_noeat_admit_current\": \"cat1_noeat_weekly\",\n",
        "    \"cat1_notests_admit_current\": \"cat1_notests_weekly\",\n",
        "    \"cat1_anemia_admit_current\": \"cat1_anemia_weekly\",\n",
        "    \"cat1_overall_admit_current\": \"cat1_overall_weekly\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peiA37TAzn1k"
      },
      "outputs": [],
      "source": [
        "filtered_admit_weekly = admit_weekly[admit_weekly[\"sequence_num\"] == 1]\n",
        "\n",
        "for key, value in col_map.items():\n",
        "    # Find rows where the lag of the current cat1 column is False and differs from the current value\n",
        "    filtered_admit_weekly[f\"{key}_diff_from_first_visit_and_admit_is_false\"] = (\n",
        "        filtered_admit_weekly[key] != filtered_admit_weekly[f\"{value}\"]\n",
        "    ) & (filtered_admit_weekly[f\"{key}\"] == False)\n",
        "\n",
        "rows_meeting_first_criteria = filtered_admit_weekly[\n",
        "    filtered_admit_weekly[\n",
        "        [f\"{col}_diff_from_first_visit_and_admit_is_false\" for col in col_map.keys()]\n",
        "    ].any(axis=1)\n",
        "].copy()\n",
        "\n",
        "rows_meeting_first_criteria_pids = rows_meeting_first_criteria[\"pid\"].unique()\n",
        "\n",
        "rows_meeting_first_criteria[\"new_onset_medical_complication\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWHjaIg1OaP_"
      },
      "outputs": [],
      "source": [
        "# prompt: find rows where lag of cat1_weekly is false and differs from current value\n",
        "\n",
        "# Assuming admit_weekly DataFrame and cat1_weekly list are already defined.\n",
        "\n",
        "for col in cat1_weekly:\n",
        "    # Find rows where the lag of the current cat1 column is False and differs from the current value\n",
        "    admit_weekly[f\"{col}_diff_from_lag_and_lag_is_false\"] = (\n",
        "        admit_weekly[col] != admit_weekly[f\"{col}_lag_1\"]\n",
        "    ) & (admit_weekly[f\"{col}_lag_1\"] == False)\n",
        "\n",
        "# Example: Display rows where any of the cat1 columns meet the criteria\n",
        "rows_meeting_criteria = admit_weekly[\n",
        "    admit_weekly[[f\"{col}_diff_from_lag_and_lag_is_false\" for col in cat1_weekly]].any(axis=1)\n",
        "]\n",
        "\n",
        "rows_meeting_criteria = rows_meeting_criteria[\n",
        "    ~rows_meeting_criteria[\"pid\"].isin(rows_meeting_first_criteria_pids)\n",
        "].copy()\n",
        "\n",
        "rows_meeting_criteria[\"new_onset_medical_complication\"] = True\n",
        "\n",
        "# prompt: concatenate rows_meeting_criteria and rows_meeting_first_criteria\n",
        "\n",
        "# Concatenate the two DataFrames\n",
        "concatenated_rows = pd.concat([rows_meeting_criteria, rows_meeting_first_criteria])\n",
        "\n",
        "admit_weekly = pd.merge(\n",
        "    admit_weekly,\n",
        "    concatenated_rows[[\"pid\", \"calcdate_weekly\", \"new_onset_medical_complication\"]],\n",
        "    on=[\"pid\", \"calcdate_weekly\"],\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "\n",
        "pids_with_new_onset_medical_complication = concatenated_rows[\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARDOVFvp-fj_"
      },
      "outputs": [],
      "source": [
        "# prompt: set admit_weekly['new_onset_medical_complication_latest']  = (new_onset_medical_complication ==True & sequence_num == max_sequence_num)\n",
        "\n",
        "# Assuming admit_weekly DataFrame, max_sequence_num column, and new_onset_medical_complication column are already defined.\n",
        "\n",
        "admit_weekly[\"new_onset_medical_complication_latest\"] = (\n",
        "    admit_weekly[\"new_onset_medical_complication\"] == True\n",
        ") & (admit_weekly[\"sequence_num\"] == admit_weekly[\"max_sequence_num\"])\n",
        "\n",
        "pids_with_new_onset_medical_complication_latest = admit_weekly[\n",
        "    admit_weekly[\"new_onset_medical_complication_latest\"] == True\n",
        "][\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uje5eMZhMXKR"
      },
      "outputs": [],
      "source": [
        "# Find column names containing 'diff_from_lag_and_lag_is_false'\n",
        "columns_with_diff_from_lag = [\n",
        "    col for col in admit_weekly.columns if \"diff_from_lag_and_lag_is_false\" in col\n",
        "]\n",
        "\n",
        "\n",
        "# prompt: for col in admit_weekly columns_with_diff_from_lag remove _weekly_diff_from_lag_and_lag_is_false and prepend y_\n",
        "\n",
        "for col in columns_with_diff_from_lag:\n",
        "    new_col_name = \"y_\" + col.replace(\"_weekly_diff_from_lag_and_lag_is_false\", \"\")\n",
        "    admit_weekly = admit_weekly.rename(columns={col: new_col_name})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW5eAnkxPWw-"
      },
      "outputs": [],
      "source": [
        "y_cat1 = columns_with_diff_from_lag.copy()\n",
        "y_cat1 = [\"y_\" + col.replace(\"_weekly_diff_from_lag_and_lag_is_false\", \"\") for col in y_cat1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N5nom26NCB0"
      },
      "outputs": [],
      "source": [
        "cols_admit_current_diff_from_first_visit_and_admit_is_false = [\n",
        "    col\n",
        "    for col in filtered_admit_weekly.columns\n",
        "    if \"diff_from_first_visit_and_admit_is_false\" in col\n",
        "]\n",
        "\n",
        "for col in cols_admit_current_diff_from_first_visit_and_admit_is_false:\n",
        "    new_col_name = \"temp_\" + col.replace(\n",
        "        \"_admit_current_diff_from_first_visit_and_admit_is_false\", \"\"\n",
        "    )\n",
        "    filtered_admit_weekly = filtered_admit_weekly.rename(columns={col: new_col_name})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPdkQBeqOEQp"
      },
      "outputs": [],
      "source": [
        "temp_cat1 = cols_admit_current_diff_from_first_visit_and_admit_is_false.copy()\n",
        "temp_cat1 = [\n",
        "    \"temp_\" + col.replace(\"_admit_current_diff_from_first_visit_and_admit_is_false\", \"\")\n",
        "    for col in temp_cat1\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phNpGyzCNu0b"
      },
      "outputs": [],
      "source": [
        "admit_weekly = admit_weekly.join(filtered_admit_weekly[temp_cat1], how=\"left\")\n",
        "for col in temp_cat1:\n",
        "    admit_weekly.loc[\n",
        "        (admit_weekly[\"sequence_num\"] == 1), col.replace(\"temp_\", \"y_\")\n",
        "    ] = admit_weekly[col]\n",
        "\n",
        "admit_weekly.drop(columns=temp_cat1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-hLGYc9fCFo",
        "outputId": "5a1cbdaf-01db-4f40-c173-7532b4ad5fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat1_fever_admit_current',\n",
              " 'cat1_hypothermia_admit_current',\n",
              " 'cat1_measles_admit_current',\n",
              " 'cat1_resp_admit_current',\n",
              " 'cat1_breath_admit_current',\n",
              " 'cat1_vomiting_admit_current',\n",
              " 'cat1_bloodstool_admit_current',\n",
              " 'cat1_diarrhea_admit_current',\n",
              " 'cat1_dehyd_admit_current',\n",
              " 'cat1_fissures_admit_current',\n",
              " 'cat1_orash_admit_current',\n",
              " 'cat1_eyes_admit_current',\n",
              " 'cat1_ears_admit_current',\n",
              " 'cat1_noeat_admit_current',\n",
              " 'cat1_notests_admit_current',\n",
              " 'cat1_anemia_admit_current',\n",
              " 'cat1_overall_admit_current',\n",
              " 'cat1_fever_weekly',\n",
              " 'cat1_hypothermia_weekly',\n",
              " 'cat1_measles_weekly',\n",
              " 'cat1_resp_weekly',\n",
              " 'cat1_breath_weekly',\n",
              " 'cat1_vomiting_weekly',\n",
              " 'cat1_bloodstool_weekly',\n",
              " 'cat1_diarrhea_weekly',\n",
              " 'cat1_dehyd_weekly',\n",
              " 'cat1_fissures_weekly',\n",
              " 'cat1_orash_weekly',\n",
              " 'cat1_eyes_weekly',\n",
              " 'cat1_ears_weekly',\n",
              " 'cat1_unresolvedmalaria',\n",
              " 'cat1_noeat_weekly',\n",
              " 'cat1_notests_weekly',\n",
              " 'cat1_anemia_weekly',\n",
              " 'cat1_overall_weekly',\n",
              " 'cat1_fever_weekly_lag_1',\n",
              " 'cat1_hypothermia_weekly_lag_1',\n",
              " 'cat1_measles_weekly_lag_1',\n",
              " 'cat1_breath_weekly_lag_1',\n",
              " 'cat1_vomiting_weekly_lag_1',\n",
              " 'cat1_bloodstool_weekly_lag_1',\n",
              " 'cat1_dehyd_weekly_lag_1',\n",
              " 'cat1_fissures_weekly_lag_1',\n",
              " 'cat1_orash_weekly_lag_1',\n",
              " 'cat1_ears_weekly_lag_1',\n",
              " 'cat1_noeat_weekly_lag_1',\n",
              " 'cat1_notests_weekly_lag_1',\n",
              " 'cat1_anemia_weekly_lag_1',\n",
              " 'cat1_overall_weekly_lag_1',\n",
              " 'y_cat1_fever',\n",
              " 'y_cat1_hypothermia',\n",
              " 'y_cat1_measles',\n",
              " 'y_cat1_breath',\n",
              " 'y_cat1_vomiting',\n",
              " 'y_cat1_bloodstool',\n",
              " 'y_cat1_dehyd',\n",
              " 'y_cat1_fissures',\n",
              " 'y_cat1_orash',\n",
              " 'y_cat1_ears',\n",
              " 'y_cat1_noeat',\n",
              " 'y_cat1_notests',\n",
              " 'y_cat1_anemia',\n",
              " 'y_cat1_overall']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: find cat1 columns in admit_weekly\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "# Filter columns that contain 'cat1' and end with '_weekly'\n",
        "cat1_weekly_cols = [col for col in admit_weekly.columns if \"cat1\" in col]\n",
        "\n",
        "# Print the column names\n",
        "cat1_weekly_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ90GcWoTWPf"
      },
      "source": [
        "# Failure to lose oedema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_pnJl8Tt3j"
      },
      "source": [
        "* New appearance of oedema (onset of oedema when previously absent)\n",
        "* Worsening/increase of oedema (grade 1-->2, 2-->3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlErAq9lW8cD"
      },
      "outputs": [],
      "source": [
        "# prompt: set value of None to False for admit_weekly['cat2_oedema_weekly']\n",
        "\n",
        "\n",
        "admit_weekly[\"oedema_status_weekly\"] = admit_weekly[\"oedema_status_weekly\"].fillna(\"healthy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSXsyN4ZZHmZ"
      },
      "outputs": [],
      "source": [
        "# prompt: lag c_oedema_weekly,cat2_oedema_weekly\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded as in the provided code.\n",
        "\n",
        "# Create lagged columns for 'c_oedema_weekly' and 'cat2_oedema_weekly'\n",
        "admit_weekly[\"c_oedema_weekly_lag_1\"] = admit_weekly.groupby(\"pid\")[\"c_oedema_weekly\"].shift(1)\n",
        "admit_weekly[\"cat2_oedema_weekly_lag_1\"] = admit_weekly.groupby(\"pid\")[\"cat2_oedema_weekly\"].shift(\n",
        "    1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-EUi_8gaCs0"
      },
      "outputs": [],
      "source": [
        "# prompt: find rows where lag of cat2_oedema_weekly_lag_1 is false and differs from current value or c_oedema_weekly is greater than lagged value\n",
        "\n",
        "# Find rows where the lag of 'cat2_oedema_weekly_lag_1' is False and differs from the current value\n",
        "# Or where 'c_oedema_weekly' is greater than the lagged value\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded and processed as in the provided code.\n",
        "\n",
        "# Identify rows meeting the specified criteria\n",
        "admit_weekly[\"oedema_criteria_met\"] = (\n",
        "    (admit_weekly[\"cat2_oedema_weekly\"] != admit_weekly[\"cat2_oedema_weekly_lag_1\"])\n",
        "    & (admit_weekly[\"cat2_oedema_weekly_lag_1\"] == False)\n",
        "    | (admit_weekly[\"c_oedema_weekly\"] > admit_weekly[\"c_oedema_weekly_lag_1\"])\n",
        "    | (admit_weekly[\"cat2_oedema_weekly\"] == True)\n",
        "    & (admit_weekly[\"cat2_oedema_admit_current\"] == False)\n",
        ")\n",
        "\n",
        "# Display or further process the rows where the criteria is met\n",
        "oedema_rows = admit_weekly[admit_weekly[\"oedema_criteria_met\"]]\n",
        "pids_oedema_criteria_met = oedema_rows[\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZwWV9kEbbqr"
      },
      "source": [
        "* Oedema not disappearing/reducing at 3rd week after initial appearance (static grade of oedema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQzhSfkZbeyL"
      },
      "outputs": [],
      "source": [
        "admit_weekly[\"oedema_initial_appearance\"] = (\n",
        "    admit_weekly[\"cat2_oedema_weekly\"] != admit_weekly[\"cat2_oedema_weekly_lag_1\"]\n",
        ") & (admit_weekly[\"cat2_oedema_weekly_lag_1\"] == False) | (\n",
        "    admit_weekly[\"cat2_oedema_weekly\"] == True\n",
        ") & (\n",
        "    admit_weekly[\"cat2_oedema_admit_current\"] == False\n",
        ")\n",
        "\n",
        "# Display or further process the rows where the criteria is met\n",
        "oedema_appearance_rows = admit_weekly[admit_weekly[\"oedema_initial_appearance\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VJ67Rd-dUYe"
      },
      "outputs": [],
      "source": [
        "# prompt: get those rows with pid, sequence_num+3 in oedema_appearance_rows\n",
        "\n",
        "# Assuming oedema_appearance_rows DataFrame is already created as in the provided code.\n",
        "\n",
        "# Create a new DataFrame with 'pid' and 'sequence_num+3'\n",
        "new_oedema_appearance_rows = oedema_appearance_rows[\n",
        "    [\"pid\", \"sequence_num\", \"cat2_oedema_weekly\"]\n",
        "].copy()\n",
        "new_oedema_appearance_rows[\"sequence_num\"] = new_oedema_appearance_rows[\"sequence_num\"] + 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJk-3ySed0D4"
      },
      "outputs": [],
      "source": [
        "# prompt: join admit_weekly,new_oedema_appearance_rows on pid,sequence_num\n",
        "\n",
        "# Assuming admit_weekly and new_oedema_appearance_rows DataFrames are already defined.\n",
        "\n",
        "# Perform the merge operation\n",
        "admit_weekly = pd.merge(\n",
        "    admit_weekly,\n",
        "    new_oedema_appearance_rows,\n",
        "    on=[\"pid\", \"sequence_num\"],\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_3rd_week\"),\n",
        ")\n",
        "\n",
        "# Now 'merged_df' contains the joined data.  You can further process or analyze it as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7lG4qdsfPk5"
      },
      "outputs": [],
      "source": [
        "# prompt: find rows where cat2_oedema_weekly is True and cat2_oedema_weekly_3rd_week is True\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded and processed as in the provided code.\n",
        "\n",
        "# Find rows where both 'cat2_oedema_weekly' and 'cat2_oedema_weekly_3rd_week' are True\n",
        "filtered_rows = admit_weekly[\n",
        "    (admit_weekly[\"cat2_oedema_weekly\"] == True)\n",
        "    & (admit_weekly[\"cat2_oedema_weekly_3rd_week\"].isnull())\n",
        "]\n",
        "\n",
        "admit_weekly[\"oedema_not_disappearing\"] = (admit_weekly[\"cat2_oedema_weekly\"] == True) & (\n",
        "    admit_weekly[\"cat2_oedema_weekly_3rd_week\"].isnull()\n",
        ")\n",
        "\n",
        "\n",
        "# Display or further process the filtered rows\n",
        "pids_oedema_not_disappearing = filtered_rows[\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVdkEykLAuo_"
      },
      "outputs": [],
      "source": [
        "# prompt: admit_weekly['failure_to_lose_oedema_latest'] = (oedema_not_disappearing | oedema_criteria_met) & sequence_num == max_sequence_num\n",
        "\n",
        "# Assuming admit_weekly, oedema_not_disappearing, oedema_criteria_met, and max_sequence_num are defined.\n",
        "\n",
        "admit_weekly[\"failure_to_lose_oedema_latest\"] = (\n",
        "    (admit_weekly[\"oedema_not_disappearing\"] == True)\n",
        "    | (admit_weekly[\"oedema_criteria_met\"] == True)\n",
        ") & (admit_weekly[\"sequence_num\"] == admit_weekly[\"max_sequence_num\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF2-W2A99cwY"
      },
      "outputs": [],
      "source": [
        "pids_failure_to_lose_oedema_latest = admit_weekly[\n",
        "    admit_weekly[\"failure_to_lose_oedema_latest\"] == True\n",
        "][\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvANxxi1hQyi"
      },
      "outputs": [],
      "source": [
        "pids_failure_to_lose_oedema = list(\n",
        "    set(list(pids_oedema_not_disappearing) + list(pids_oedema_criteria_met))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9f4TVXhoYE"
      },
      "source": [
        "# Poor MUAC gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywgGmDxWhwd1"
      },
      "source": [
        "Static MUAC or MUAC loss for 2 consecutive weeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nl70TTAiRST"
      },
      "outputs": [],
      "source": [
        "# prompt: find 'muac_weekly' <= prior 'muac_weekly' for 2 consecutive rows\n",
        "\n",
        "# Assuming admit_weekly DataFrame is already loaded and processed as in the provided code.\n",
        "\n",
        "# Create a lagged column for 'muac_weekly'\n",
        "admit_weekly[\"muac_weekly_lag_1\"] = admit_weekly.groupby(\"pid\")[\"muac_weekly\"].shift(1)\n",
        "\n",
        "# Check for static or MUAC loss for 2 consecutive weeks\n",
        "admit_weekly[\"muac_loss_2_weeks\"] = admit_weekly[\"muac_weekly\"] <= admit_weekly[\"muac_weekly_lag_1\"]\n",
        "\n",
        "# Group by 'pid' and check for 2 consecutive True values in 'muac_loss_2_weeks'\n",
        "muac_loss_2_weeks_consecutive = (\n",
        "    admit_weekly.groupby(\"pid\")[\"muac_loss_2_weeks\"]\n",
        "    .rolling(window=2, min_periods=2)\n",
        "    .apply(lambda x: all(x), raw=True)\n",
        ")\n",
        "\n",
        "# Assign the result back to the DataFrame, handling potential index mismatches\n",
        "admit_weekly[\"muac_loss_2_weeks_consecutive\"] = muac_loss_2_weeks_consecutive.reset_index(\n",
        "    level=0, drop=True\n",
        ").fillna(False)\n",
        "# Now 'admit_weekly' contains a new column 'muac_loss_2_weeks_consecutive' indicating whether MUAC has been static or decreased for two consecutive weeks for each patient.\n",
        "\n",
        "# Convert to boolean\n",
        "admit_weekly[\"muac_loss_2_weeks_consecutive\"] = admit_weekly[\n",
        "    \"muac_loss_2_weeks_consecutive\"\n",
        "].astype(bool)\n",
        "\n",
        "# get the pids\n",
        "pids_muac_loss = admit_weekly[admit_weekly[\"muac_loss_2_weeks_consecutive\"] == True][\"pid\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB8p8flBDqWN"
      },
      "outputs": [],
      "source": [
        "# prompt: admit_weekly[muac_loss_2_weeks_consecutive_latest] = muac_loss_2_weeks_consecutive & sequence_num = max_sequence_num\n",
        "\n",
        "# Assuming admit_weekly and relevant columns are already defined as in the provided code.\n",
        "\n",
        "# Create 'muac_loss_2_weeks_consecutive_latest' based on the condition\n",
        "admit_weekly[\"muac_loss_2_weeks_consecutive_latest\"] = (\n",
        "    admit_weekly[\"muac_loss_2_weeks_consecutive\"] == True\n",
        ") & (admit_weekly[\"sequence_num\"] == admit_weekly[\"max_sequence_num\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju-gdGwS7Qc8"
      },
      "outputs": [],
      "source": [
        "# get the pids\n",
        "pids_muac_loss_latest = admit_weekly[admit_weekly[\"muac_loss_2_weeks_consecutive_latest\"] == True][\n",
        "    \"pid\"\n",
        "].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG1KqIJOhZb_"
      },
      "outputs": [],
      "source": [
        "#current = pd.read_csv(\n",
        "#    \"/content/drive/My Drive/[PBA] Full datasets/\" + \"FULL_pba_current_processed_2024-11-15.csv\"\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnvdXtwJnBKD",
        "outputId": "fe5ed837-f6f1-4b45-a837-bfbe664d22b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:9747\n"
          ]
        }
      ],
      "source": [
        "logger.debug(admit_weekly[\"pid\"].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WaoCyTfmbRE"
      },
      "source": [
        "# Consolidate all deterioration types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE7062egnKae"
      },
      "source": [
        "Of the 4928 patients in training data that have 1 or more visit, 1524 (30.8%) have one or more deterioration types:\n",
        "\n",
        "1. poor weight gain (ever), 1021\n",
        "2. new onset medical complications, 516\n",
        "2. failur to lose oedema, 29\n",
        "3. poor MUAC gain, 469\n",
        "4. nonresponse to treatment, 224\n",
        "5. dead, 42\n",
        "\n",
        "total is 2283 due to patients being in multiple deterioration categories\n",
        "\n",
        "These deteriorated at some point in their history.\n",
        "\n",
        "701 are currently deteriorated, 14.2%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nuIn7F3mj1y",
        "outputId": "935091d7-4763-4cf8-d1da-4d34ab0688f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:3527\n"
          ]
        }
      ],
      "source": [
        "pids_deterioration = list(\n",
        "    set(\n",
        "        list(pids_weight_loss_ever)\n",
        "        + list(pids_with_new_onset_medical_complication)\n",
        "        + list(pids_failure_to_lose_oedema)\n",
        "        + list(pids_muac_loss)\n",
        "        + list(pids_nonresponse)\n",
        "        + list(pids_dead)\n",
        "    )\n",
        ")\n",
        "logger.debug(len(pids_deterioration))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6Kap6Y7ml8"
      },
      "source": [
        "these had deterioration at the latest weekly visit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXhgIySJ7gYz",
        "outputId": "b6a4cbaa-2013-4bf0-9810-c64b879ad001"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:1840\n"
          ]
        }
      ],
      "source": [
        "pids_deterioration_latest = list(\n",
        "    set(\n",
        "        list(pids_weight_loss_latest)\n",
        "        + list(pids_with_new_onset_medical_complication_latest)\n",
        "        + list(pids_failure_to_lose_oedema_latest)\n",
        "        + list(pids_muac_loss_latest)\n",
        "        + list(pids_nonresponse)\n",
        "        + list(pids_dead)\n",
        "    )\n",
        ")\n",
        "\n",
        "logger.debug(len(pids_deterioration_latest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVHfU0jYu-J9"
      },
      "outputs": [],
      "source": [
        "# prompt: list columns in admit_weekly starting with loc 980\n",
        "\n",
        "#first_added_col = admit_weekly.columns.get_loc(\"max_sequence_num\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVyY53O29bI"
      },
      "source": [
        "# convert boolean to 1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pe6QaZl0L3E"
      },
      "outputs": [],
      "source": [
        "# Find boolean columns\n",
        "#boolean_columns = admit_weekly.select_dtypes(include=[\"bool\"]).columns\n",
        "#print(\"Boolean columns:\")\n",
        "#boolean_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Ks8PCI0ZUE",
        "outputId": "4864b79c-87f0-43c3-8c6d-0512224d498d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:Single value columns:\n",
            "DEBUG:my_logger:['b_referred_emergency_admit_current', 'state', 'imci_emergency_otp_admit_current', 'b_fl_nasi_admit_current', 'site_type_weekly']\n"
          ]
        }
      ],
      "source": [
        "# prompt: find columns that are single value and nonnull, then drop them\n",
        "\n",
        "single_value_cols = [\n",
        "    col\n",
        "    for col in admit_weekly.columns\n",
        "    if admit_weekly[col].nunique() == 1 and admit_weekly[col].notna().all()\n",
        "]\n",
        "logger.debug(\"Single value columns:\")\n",
        "logger.debug(single_value_cols)\n",
        "admit_weekly.drop(columns=single_value_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxZn0WkoXoIq",
        "outputId": "d3ff8e1f-d622-4f96-81b3-2fe7cdfed63f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:Single value columns:\n",
            "DEBUG:my_logger:[]\n"
          ]
        }
      ],
      "source": [
        "single_value_cols = [\n",
        "    col\n",
        "    for col in admit_weekly_all.columns\n",
        "    if admit_weekly_all[col].nunique() == 1 and admit_weekly_all[col].notna().all()\n",
        "]\n",
        "logger.debug(\"Single value columns:\")\n",
        "logger.debug(single_value_cols)\n",
        "admit_weekly_all.drop(columns=single_value_cols, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEiJK3Ko1HFX"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "\n",
        "\n",
        "def convert_to_bool(df):\n",
        "    # Identify columns that are True/False and convert them to boolean\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_bool_dtype(df[col]):\n",
        "            continue\n",
        "        elif all(x in [True, False, 1, 0] for x in df[col].unique()):\n",
        "            df[col] = df[col].astype(bool)\n",
        "        elif all(x in [True, False, 1, 0, None] for x in df[col].unique()):\n",
        "            df[col] = df[col].replace({None: False}).astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljr0e5cf1Roc"
      },
      "outputs": [],
      "source": [
        "# Identify columns with unique values [True, nan, False] and print null count\n",
        "def find_3val_bool(df):\n",
        "    for col in df.columns:\n",
        "        if len(df[col].unique()) == 3:\n",
        "            unique_vals = df[col].unique()\n",
        "            if all(val in [True, False] or pd.isna(val) for val in unique_vals):\n",
        "                null_ct = df[col].isnull().sum()\n",
        "                size = df[col].size\n",
        "                sum = df[col].sum()\n",
        "                if null_ct > 0:\n",
        "                    logger.debug(\n",
        "                        f\"Found 3-val bool column '{col}' with null count: {null_ct} {null_ct/size*100:.1f}% sum:{sum}\"\n",
        "                    )\n",
        "                else:\n",
        "                    logger.debug(\n",
        "                        f\"Found 3-val bool column '{col}' with null count: {df[col].isnull().sum()} sum:{sum}\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eazL7KtK170g"
      },
      "outputs": [],
      "source": [
        "# prompt: convert detn columns with unique values [True nan False] to boolean\n",
        "\n",
        "\n",
        "# Identify columns with unique values [True, nan, False] and convert them to boolean\n",
        "def convert_3val_bool(df, threshold):\n",
        "    for col in df.columns:\n",
        "        if \"lag\" in col.lower():\n",
        "            continue\n",
        "        if len(df[col].unique()) == 3:\n",
        "            unique_vals = df[col].unique()\n",
        "            if all(val in [True, False] or pd.isna(val) for val in unique_vals):\n",
        "                null_ct = df[col].isnull().sum()\n",
        "                if null_ct < threshold:\n",
        "                    # print(f\"Converting 3-val bool column '{col}' with null count: {null_ct}\")\n",
        "                    df[col] = df[col].fillna(False).astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cc8BjHM1YNt",
        "outputId": "71a19184-113f-4ede-9d83-875791b7b5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:Found 3-val bool column 'weight_at_week3_lower_than_admission' with null count: 56854 86.0% sum:486\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_fever_weekly_lag_1' with null count: 9747 14.7% sum:229\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_hypothermia_weekly_lag_1' with null count: 9747 14.7% sum:11\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_measles_weekly_lag_1' with null count: 9747 14.7% sum:8\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_breath_weekly_lag_1' with null count: 9747 14.7% sum:11\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_vomiting_weekly_lag_1' with null count: 9747 14.7% sum:345\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_bloodstool_weekly_lag_1' with null count: 9747 14.7% sum:166\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_dehyd_weekly_lag_1' with null count: 9747 14.7% sum:329\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_fissures_weekly_lag_1' with null count: 9747 14.7% sum:24\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_orash_weekly_lag_1' with null count: 9747 14.7% sum:32\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_ears_weekly_lag_1' with null count: 9747 14.7% sum:7\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_noeat_weekly_lag_1' with null count: 9747 14.7% sum:267\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_notests_weekly_lag_1' with null count: 9747 14.7% sum:955\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_anemia_weekly_lag_1' with null count: 9747 14.7% sum:16\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_overall_weekly_lag_1' with null count: 9747 14.7% sum:1235\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat2_oedema_weekly_lag_1' with null count: 9747 14.7% sum:35\n"
          ]
        }
      ],
      "source": [
        "find_3val_bool(admit_weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmyMvTWk10Mg"
      },
      "outputs": [],
      "source": [
        "convert_3val_bool(admit_weekly, len(admit_weekly))\n",
        "convert_3val_bool(admit_weekly_all, len(admit_weekly_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peq2a1pD2BT2",
        "outputId": "fc713f4b-b24e-4abd-e1c6-5814f8a639d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:Found 3-val bool column 'cat1_fever_weekly_lag_1' with null count: 9747 14.7% sum:229\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_hypothermia_weekly_lag_1' with null count: 9747 14.7% sum:11\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_measles_weekly_lag_1' with null count: 9747 14.7% sum:8\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_breath_weekly_lag_1' with null count: 9747 14.7% sum:11\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_vomiting_weekly_lag_1' with null count: 9747 14.7% sum:345\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_bloodstool_weekly_lag_1' with null count: 9747 14.7% sum:166\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_dehyd_weekly_lag_1' with null count: 9747 14.7% sum:329\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_fissures_weekly_lag_1' with null count: 9747 14.7% sum:24\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_orash_weekly_lag_1' with null count: 9747 14.7% sum:32\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_ears_weekly_lag_1' with null count: 9747 14.7% sum:7\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_noeat_weekly_lag_1' with null count: 9747 14.7% sum:267\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_notests_weekly_lag_1' with null count: 9747 14.7% sum:955\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_anemia_weekly_lag_1' with null count: 9747 14.7% sum:16\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat1_overall_weekly_lag_1' with null count: 9747 14.7% sum:1235\n",
            "DEBUG:my_logger:Found 3-val bool column 'cat2_oedema_weekly_lag_1' with null count: 9747 14.7% sum:35\n"
          ]
        }
      ],
      "source": [
        "find_3val_bool(admit_weekly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T77X-TZy03li",
        "outputId": "cd3cff02-f395-4f84-ff5a-e5862eab9fdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:Boolean columns:\n",
            "DEBUG:my_logger:['b_presented_emergency', 'b_prevenr', 'b_knowsbday', 'b_heightcheck', 'b_reachcheck', 'ts_assessed_needitp', 'b_hastwin', 'b_twinalive', 'b_twinattended', 'ref_g6u4kg', 'ref_tsref', 'ref_u6sam', 'ref_oedg3', 'ref_oedsam', 'b_needsitp', 'b_wasreferred_admit_current', 'gave_al_act_admit_current', 'b_cgishoh', 'b_isvaxed', 'ses_b_foodsecurity', 'b_movenextvisit_admit_current', 'b_motheralive', 'b_fatheralive', 'b_hadbirthvax_admit', 'b_had6wvax_admit', 'b_rota1diff_admit', 'b_ipv1diff_admit', 'b_had10wvax_admit', 'b_rota2diff_admit', 'b_had14wvax_admit', 'b_rota3diff_admit', 'b_ipv2diff_admit', 'dayssincevita', 'b_has_phone_number_admit', 'b_wast_admit', 'b_muac_waz_admit', 'b_muac_wfh_admit', 'lean_season_admit', 'rainy_season_admit', 'b_outreach_admit_current', 'imci_emergency_itp', 'b_havepid', 'b_hasbc', 'pull_prev_study_recamox', 'b_correct_prevstatus', 'age_height_check', 'age_reach_check', 'b_stand_check_admit_current', 'b_phys_req_itp_admit_current', 'b_twin', 'b_twinhere_admit_current', 'b_twinenr', 'eref_g6u4kg_admit_current', 'eref_tsref', 'eref_u6sam_admit_current', 'eref_oedg3_admit_current', 'eref_oedsam_admit_current', 'eref_overall_admit_current', 'ref_overall_admit_current', 'eff_eref_overall_admit_current', 'eff_ref_overall_admit_current', 'q_override_eref_admit_current', 'override_eref_admit_current', 'b_transfer', 'b_correctadmittype', 'b_curr_bfeed_admit_current', 'b_cg_preg', 'qmother_alive', 'qfather_alive', 'mother_alive', 'father_alive', 'b_phonehave', 'final_havephone', 'phone_owner_samehh_admit_current', 'phone_owner_samecomm', 'b_hascmslip', 'b_fu_hoh', 'b_migrated', 'b_receivedsmc', 'b_bednet', 'foodsecurity_admit_current', 'cat1_fever_admit_current', 'cat1_hypothermia_admit_current', 'cat2_fever_admit_current', 'cat2_hypothermia_admit_current', 'b_cough_admit_current', 'b_coryza_admit_current', 'b_conjunctivitis_admit_current', 'cat1_measles_admit_current', 'cat2_cough_admit_current', 'b_rash_admit_current', 'b_maculopapular_admit_current', 'cat2_rash_admit_current', 'resp1_elev_admit_current', 'resp2_elev_admit_current', 'cat1_resp_admit_current', 'b_chest_indraw_admit_current', 'chest_ind_final_admit_current', 'cat1_breath_admit_current', 'vomit_always_admit_current', 'cat1_vomiting_admit_current', 'cat2_vomiting_admit_current', 'b_bloodstool_admit_current', 'cat1_bloodstool_admit_current', 'cat1_diarrhea_admit_current', 'cat2_diarrhea_admit_current', 'b_profusediar_admit_current', 'b_lethargy_admit_current', 'b_nourine_admit_current', 'precalc_sev_dehyd_admit_current', 'b_darkurine_admit_current', 'b_reducurine_admit_current', 'b_excesscry_admit_current', 'b_unabledrink_admit_current', 'b_eagerdrink_admit_current', 'calc_sev_dehyd_admit_current', 'calc_mod_dehyd_admit_current', 'cat1_dehyd_admit_current', 'cat2_dehyd_admit_current', 'b_white_palms_admit_current', 'b_white_conjunc_admit_current', 'b_visanemia_admit_current', 'b_fissures_admit_current', 'cat1_fissures_admit_current', 'b_orash_admit_current', 'b_able_eat_admit_current', 'cat1_orash_admit_current', 'cat2_orash_admit_current', 'b_bitot', 'b_eyedisch_admit_current', 'cat1_eyes_admit_current', 'cat2_eyes_admit_current', 'b_earpain_admit_current', 'b_eardisch_admit_current', 'b_earswelling_admit_current', 'b_swellingtender_admit_current', 'cat1_ears_admit_current', 'cat2_ears_admit_current', 'b_eating_admit_current', 'cat1_noeat_admit_current', 'cat1_notests_admit_current', 'rdt_result_admit_current', 'q_override_lref', 'override_lref', 'eff_lref_overall', 'aptest_result_admit_current', 'cat1_anemia_admit_current', 'cat2_anemia_admit_current', 'cat1_overall_admit_current', 'cat2_malaria_admit_current', 'cat2_oedema_admit_current', 'cat2_overall_admit_current', 'b_otherdisch_admit_current', 'b_ppu_admit_current', 'foi_result_admit_current', 'send_forhiv', 'send_forhiv_optional', 'b_accepthts', 'cough_bkup', 'final_cough', 'prolong_cough', 'prolong_fever', 'send_fortb', 'b_vaccinated', 'b_hadbirthvax_raw', 'b_had6wvax_raw', 'b_rota1diff_raw', 'b_ipv1diff_raw', 'b_had10wvax_raw', 'b_rota2diff_raw', 'b_had14wvax_raw', 'b_rota3diff_raw', 'b_ipv2diff_raw', 'v_miss0', 'v_miss6', 'v_miss10', 'v_miss14', 'v_missrem', 'v_missoverall', 'b_vitalast4', 'final_vitalast4', 'send_forvax', 'b_acceptri', 'b_needcmforimmun', 'nextweek_vaxcard', 'rdt_result_erefonly_admit_current', 'ttype_override_admit_current', 'give_al_act_admit_current', 'give_amox_admit_current', 'give_lorat_admit_current', 'give_multivite_admit_current', 'give_nystatin_admit_current', 'give_ors_admit_current', 'give_pcm_admit_current', 'give_tetra_admit_current', 'give_zincsulf_admit_current', 'give_zincox_admit_current', 'give_otomed_admit_current', 'give_counselchronicears_admit_current', 'give_anydrugs_admit_current', 'init_remdrugs_admit_current', 'conf_initremover_admit_current', 'tgive_al_act_admit_current', 'tgive_amox_admit_current', 'tgive_lorat_admit_current', 'tgive_multivite_admit_current', 'tgive_nystatin_admit_current', 'tgive_ors_admit_current', 'tgive_pcm_admit_current', 'tgive_tetra_admit_current', 'tgive_zincsulf_admit_current', 'tgive_zincox_admit_current', 'tgive_otomed_admit_current', 'tgive_anydrugs_admit_current', 'two_adddrugs_admit_current', 'fgive_al_act_admit_current', 'fgive_amox_admit_current', 'fgive_lorat_admit_current', 'fgive_multivite_admit_current', 'fgive_nystatin_admit_current', 'fgive_ors_admit_current', 'fgive_pcm_admit_current', 'fgive_tetra_admit_current', 'fgive_zincsulf_admit_current', 'fgive_zincox_admit_current', 'fgive_otomed_admit_current', 'fgive_anydrugs_admit_current', 'changedose_admit_current', 'canmovevisit_admit_current', 'wantmovevisit_admit_current', 'final_movevisit_admit_current', 'b_doublerat_admit_current', 'give_rt_vita', 'give_routamox', 'b_isinedc', 'rdt_overall_admit_current', 'attachments_present_admit_current', 'attachments_expected_admit_current', 'edits_admit_current', 'bfeed_age_admit_current', 'b_wast_raw', 'b_muac_waz_raw', 'b_muac_wfh_raw', 'bfeed_exc', 'bfeed_exc_food', 'bfeed_intro_food', 'lean_season_raw', 'rainy_season_raw', 'emergency_admission', 'dischqualanthro', 'b_has_phone_number_current', 'lean_season_admit_current', 'rainy_season_admit_current', 'status_dead', 'itp1_mgt_checked_folder', 'itp2_mgt_checked_folder', 'itp3_mgt_checked_folder', 'gave_al_act_weekly', 'gave_aa_act', 'md_attachmentspres', 'md_attachmentsexp', 'b_phoneconsent', 'b_wasreferred_weekly', 'b_mamneedsitp', 'b_excluded', 'b_referred_emergency_weekly', 'b_discharged', 'b_dischargednr', 'b_movenextvisit_weekly', 'b_hadcat2complication', 'gave_amox', 'gave_lorat', 'gave_multivite', 'gave_nystatin', 'gave_ors', 'gave_pcm', 'gave_tetra', 'gave_zincsulf', 'gave_zincox', 'gave_routintealbend', 'gave_otomed', 'gave_anydrugs', 'b_added_phone_number', 'b_wast', 'b_muac_waz', 'b_muac_wfh', 'lean_season_weekly', 'rainy_season_weekly', 'b_pidscannable', 'pull_dischqualanthro', 'pull_doneses', 'pull_edc_consent', 'b_outreach_weekly', 'imci_emergency_otp_weekly', 'b_possexclucrit', 'b_contprogram_possexclucrit', 'b_correct_status', 'b_crvisitnum', 'b_updtphone', 'phone_owner_samehh_weekly', 'foodsecurity_weekly', 'b_curr_bfeed_weekly', 'b_stand_check_weekly', 'b_phys_req_itp_weekly', 'b_twinhere_weekly', 'eref_g6u4kg_weekly', 'eref_u6sam_weekly', 'eref_oedg3_weekly', 'eref_oedsam_weekly', 'eref_overall_weekly', 'ref_overall_weekly', 'eff_eref_overall_weekly', 'eff_ref_overall_weekly', 'mam_needsitp', 'q_override_eref_weekly', 'override_eref_weekly', 'cat1_fever_weekly', 'cat1_hypothermia_weekly', 'cat2_fever_weekly', 'cat2_hypothermia_weekly', 'b_cough_weekly', 'b_coryza_weekly', 'b_conjunctivitis_weekly', 'cat1_measles_weekly', 'cat2_cough_weekly', 'b_rash_weekly', 'b_maculopapular_weekly', 'cat2_rash_weekly', 'resp1_elev_weekly', 'resp2_elev_weekly', 'cat1_resp_weekly', 'b_chest_indraw_weekly', 'b_fl_nasi_weekly', 'chest_ind_final_weekly', 'cat1_breath_weekly', 'vomit_always_weekly', 'cat1_vomiting_weekly', 'cat2_vomiting_weekly', 'b_bloodstool_weekly', 'cat1_bloodstool_weekly', 'cat1_diarrhea_weekly', 'cat2_diarrhea_weekly', 'b_profusediar_weekly', 'b_lethargy_weekly', 'b_nourine_weekly', 'precalc_sev_dehyd_weekly', 'b_darkurine_weekly', 'b_reducurine_weekly', 'b_excesscry_weekly', 'b_unabledrink_weekly', 'b_eagerdrink_weekly', 'calc_sev_dehyd_weekly', 'calc_mod_dehyd_weekly', 'cat1_dehyd_weekly', 'cat2_dehyd_weekly', 'b_white_palms_weekly', 'b_white_conjunc_weekly', 'b_visanemia_weekly', 'b_fissures_weekly', 'cat1_fissures_weekly', 'b_orash_weekly', 'b_able_eat_weekly', 'cat1_orash_weekly', 'cat2_orash_weekly', 'b_cornulc', 'b_eyedisch_weekly', 'cat1_eyes_weekly', 'cat2_eyes_weekly', 'b_earpain_weekly', 'b_eardisch_weekly', 'b_earswelling_weekly', 'b_swellingtender_weekly', 'cat1_ears_weekly', 'cat2_ears_weekly', 'b_cgrep_fever', 'b_cgrep_chills', 'b_classicml_sympt', 'b_otherdisch_weekly', 'b_ppu_weekly', 'foi_result_weekly', 'give_rdt', 'rdt_result_weekly', 'cat1_unresolvedmalaria', 'b_eating_weekly', 'cat1_noeat_weekly', 'cat1_notests_weekly', 'aptest_result_weekly', 'cat1_anemia_weekly', 'cat2_anemia_weekly', 'cat1_overall_weekly', 'cat2_malaria_weekly', 'cat2_oedema_weekly', 'cat2_overall_weekly', 'ttype_override_weekly', 'fgive_routamox', 'give_al_act_weekly', 'give_aa_act', 'give_amox_weekly', 'give_lorat_weekly', 'give_multivite_weekly', 'give_nystatin_weekly', 'give_ors_weekly', 'give_pcm_weekly', 'give_tetra_weekly', 'give_zincsulf_weekly', 'give_zincox_weekly', 'give_otomed_weekly', 'give_counselchronicears_weekly', 'give_routalbend', 'give_anydrugs_weekly', 'init_remdrugs_weekly', 'conf_initremover_weekly', 'tgive_al_act_weekly', 'tgive_aa_act', 'tgive_amox_weekly', 'tgive_lorat_weekly', 'tgive_multivite_weekly', 'tgive_nystatin_weekly', 'tgive_ors_weekly', 'tgive_pcm_weekly', 'tgive_tetra_weekly', 'tgive_zincsulf_weekly', 'tgive_zincox_weekly', 'tgive_otomed_weekly', 'tgive_routalbend', 'tgive_anydrugs_weekly', 'two_adddrugs_weekly', 'conf_twoaddover', 'fgive_al_act_weekly', 'fgive_aa_act', 'fgive_amox_weekly', 'fgive_lorat_weekly', 'fgive_multivite_weekly', 'fgive_nystatin_weekly', 'fgive_ors_weekly', 'fgive_pcm_weekly', 'fgive_tetra_weekly', 'fgive_zincsulf_weekly', 'fgive_zincox_weekly', 'fgive_routalbend', 'fgive_otomed_weekly', 'fgive_anydrugs_weekly', 'changedose_weekly', 'calc_anthro_eligibledischarge_normal', 'calc_anthro_eligibledischarge_enhanced', 'calc_anthro_eligibledischarge', 'calc_eligibledischarge', 'discharge_override', 'final_discharge', 'elig_1moreweek', 'calc_eligiblenr', 'dischargenr_override', 'final_dischargenr', 'canmovevisit_weekly', 'wantmovevisit_weekly', 'confirmmovevisit', 'final_movevisit_weekly', 'b_doublerat_weekly', 'rdt_result_erefonly_weekly', 'rdt_overall_weekly', 'attachments_present_weekly', 'attachments_expected_weekly', 'bfeed_age_weekly', 'nonresponse', 'static_weight_loss_1w', 'static_or_weight_loss_4_weeks', 'weight_at_week3_lower_than_admission', 'poor_weight_gain', 'poor_weight_gain_4_weeks', 'strict_weight_loss_1w', 'weight_loss_3_weeks', 'detn_weight_loss_ever', 'detn_weight_loss_latest', 'y_cat1_fever', 'y_cat1_hypothermia', 'y_cat1_measles', 'y_cat1_breath', 'y_cat1_vomiting', 'y_cat1_bloodstool', 'y_cat1_dehyd', 'y_cat1_fissures', 'y_cat1_orash', 'y_cat1_ears', 'y_cat1_noeat', 'y_cat1_notests', 'y_cat1_anemia', 'y_cat1_overall', 'new_onset_medical_complication_latest', 'oedema_criteria_met', 'oedema_initial_appearance', 'oedema_not_disappearing', 'failure_to_lose_oedema_latest', 'muac_loss_2_weeks', 'muac_loss_2_weeks_consecutive', 'muac_loss_2_weeks_consecutive_latest']\n"
          ]
        }
      ],
      "source": [
        "convert_to_bool(admit_weekly)\n",
        "convert_to_bool(admit_weekly_all)\n",
        "# prompt: get boolean columns in det\n",
        "\n",
        "# Assuming 'detn' DataFrame is already loaded as in the provided code.\n",
        "\n",
        "boolean_columns = admit_weekly.select_dtypes(include=[\"bool\"]).columns\n",
        "logger.debug(\"Boolean columns:\")\n",
        "logger.debug(boolean_columns.tolist())\n",
        "\n",
        "# Convert boolean columns to numeric\n",
        "for col in boolean_columns:\n",
        "    admit_weekly[col] = admit_weekly[col].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmU52n3_3Oow"
      },
      "source": [
        "# prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySRBCoOS3Uaz",
        "outputId": "d728cb5d-2e15-4c2f-9464-1b1d6d691d88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:detn_weight_loss_ever             4302\n",
            "new_onset_medical_complication    1284\n",
            "muac_loss_2_weeks_consecutive     1986\n",
            "oedema_not_disappearing             40\n",
            "nonresponse                       7129\n",
            "status_dead                        395\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "detn_cols = [\n",
        "    \"detn_weight_loss_ever\",\n",
        "    \"new_onset_medical_complication\",\n",
        "    \"muac_loss_2_weeks_consecutive\",\n",
        "    \"oedema_not_disappearing\",\n",
        "    \"nonresponse\",\n",
        "    \"status_dead\",\n",
        "]\n",
        "\n",
        "detn_weight_loss_cols = [\n",
        "    \"static_or_weight_loss_4_weeks\",\n",
        "    \"poor_weight_gain_4_weeks\",\n",
        "    \"weight_loss_3_weeks\",\n",
        "    \"weight_at_week3_lower_than_admission\",\n",
        "]\n",
        "\n",
        "logger.debug(admit_weekly[detn_cols].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYVhMg-fjTcq"
      },
      "outputs": [],
      "source": [
        "# add last weekly row to admit_row\n",
        "start_col = admit_weekly.columns.get_loc(\"calcdate_weekly\")\n",
        "end_col = admit_weekly.columns.get_loc(\"sequence_num\")\n",
        "\n",
        "weekly_columns = admit_weekly.columns[start_col : end_col + 1]\n",
        "\n",
        "# Add 'pid' to weekly_columns\n",
        "weekly_columns = weekly_columns.tolist()  # Convert Index to list for mutability\n",
        "\n",
        "weekly_columns.insert(0, \"weight\")\n",
        "weekly_columns.insert(0, \"muac\")\n",
        "weekly_columns.remove(\"weight_weekly\")\n",
        "weekly_columns.remove(\"muac_weekly\")\n",
        "\n",
        "weekly_columns.insert(0, \"wfh\")\n",
        "weekly_columns.insert(0, \"hfa\")\n",
        "weekly_columns.remove(\"wfh_weekly\")\n",
        "weekly_columns.remove(\"hfa_weekly\")\n",
        "weekly_columns.insert(0, \"wfa\")\n",
        "weekly_columns.remove(\"wfa_weekly\")\n",
        "\n",
        "\n",
        "if \"pid\" not in weekly_columns:\n",
        "    weekly_columns.insert(0, \"pid\")  # add pid to the beginning of the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaqfmOEP3sS-"
      },
      "outputs": [],
      "source": [
        "def trend(detn_prior, admit_weekly, admit, detn_col):\n",
        "    # concatenate admit to admit_weekly['pid','calcdate_weekly','weight_weekly']\n",
        "    # Concatenate admit to admit_weekly\n",
        "\n",
        "    anthros = pd.concat(\n",
        "        [\n",
        "            detn_prior[[\"pid\", \"calcdate_weekly\", \"weight\", \"muac\", \"hl\", \"wfh\", \"hfa\", \"wfa\"]],\n",
        "            admit,\n",
        "        ],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "    # prompt: sort anthros by pid, calcdate_weekly\n",
        "\n",
        "    # Sort the 'anthros' DataFrame by 'pid' and then 'calcdate_weekly' so admittance row is first for each pid\n",
        "    anthros = anthros.sort_values(by=[\"pid\", \"calcdate_weekly\"])\n",
        "    # prompt: group anthros by pid, diff calcdate_weekly cumulative days from the first row in that group\n",
        "\n",
        "    # Group by 'pid' and calculate the cumulative difference in days from the first 'calcdate_weekly'\n",
        "    anthros[\"calcdate_weekly\"] = pd.to_datetime(anthros[\"calcdate_weekly\"])\n",
        "    anthros[\"days_since_first\"] = anthros.groupby(\"pid\")[\"calcdate_weekly\"].diff().dt.days\n",
        "    # cumulative days is the regressor column\n",
        "    anthros[\"cumulative_days\"] = anthros.groupby(\"pid\")[\"days_since_first\"].cumsum().fillna(0)\n",
        "    anthros.drop(columns=[\"days_since_first\"], inplace=True)\n",
        "\n",
        "    # prompt: for each pid in admit call weight_regress and add the first return value as 'weight_trend\" and second as weight-rsquared columns in admit\n",
        "    trend_df = pd.DataFrame(columns=[\"pid\"])\n",
        "    positive_pids = admit_weekly.loc[admit_weekly[detn_col] == True, \"pid\"].unique()\n",
        "    # prompt: for each anthro_col in\n",
        "    # prompt: for each anthro_col in 'weight_weekly','muac_weekly','hl_weekly','wfhz_weekly', 'hfaz_weekly', 'wfaz_weekly':\n",
        "    for anthro_col in [\"wfh\", \"hfa\", \"wfa\", \"weight\", \"muac\", \"hl\"]:\n",
        "        logger.debug(anthro_col)\n",
        "        # prompt: for each pid in admit call regress and add the first return value as f'{anthro_col}_trend'\" and second as f'{anthro_col}_rsquared columns in admit\n",
        "\n",
        "        # Apply the function to each unique 'pid' and create new columns\n",
        "        results = []\n",
        "        # only recalculate the trends for the partial weeklies for the pids with the deterioriation\n",
        "        for pid in tqdm(positive_pids):\n",
        "            trend, r_squared = regress(anthros, pid, anthro_col)\n",
        "            results.append(\n",
        "                {\"pid\": pid, f\"{anthro_col}_trend\": trend, f\"{anthro_col}_rsquared\": r_squared}\n",
        "            )\n",
        "\n",
        "        # Convert the list of dictionaries to a DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Merge the results back into the 'admit' DataFrame\n",
        "        trend_df = pd.merge(trend_df, results_df, on=\"pid\", how=\"right\")\n",
        "        logger.debug(trend_df.shape)\n",
        "\n",
        "    # np.-inf breaks downstream models\n",
        "    rsquared_columns = [col for col in trend_df.columns if col.endswith(\"_rsquared\")]\n",
        "    trend_df[rsquared_columns] = trend_df[rsquared_columns].replace(-np.inf, 0)\n",
        "    # just re-use the full weekly for the negative pids, to save time\n",
        "    # Filter admit_weekly for rows where pid is NOT in positive_pids and sequence_num is 1\n",
        "    filtered_admit_weekly = admit_weekly[\n",
        "        ~admit_weekly[\"pid\"].isin(positive_pids) & (admit_weekly[\"sequence_num\"] == 1)\n",
        "    ].copy()\n",
        "    filtered_admit_weekly.rename(\n",
        "        columns={\"weight_weekly\": \"weight\", \"muac_weekly\": \"muac\"}, inplace=True\n",
        "    )\n",
        "    filtered_admit_weekly.rename(\n",
        "        columns={\"wfa_weekly\": \"wfa\", \"wfh_weekly\": \"wfh\", \"hfa_weekly\": \"hfa\"}, inplace=True\n",
        "    )\n",
        "    trend_df = pd.concat(\n",
        "        [\n",
        "            filtered_admit_weekly[\n",
        "                [\n",
        "                    \"pid\",\n",
        "                    \"wfh_trend\",\n",
        "                    \"wfh_rsquared\",\n",
        "                    \"hfa_trend\",\n",
        "                    \"hfa_rsquared\",\n",
        "                    \"wfa_trend\",\n",
        "                    \"wfa_rsquared\",\n",
        "                    \"weight_trend\",\n",
        "                    \"weight_rsquared\",\n",
        "                    \"muac_trend\",\n",
        "                    \"muac_rsquared\",\n",
        "                    \"hl_trend\",\n",
        "                    \"hl_rsquared\",\n",
        "                ]\n",
        "            ],\n",
        "            trend_df,\n",
        "        ],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "    # prompt: get row count by pid in admit_weekly and append that column to admit\n",
        "    # Group by 'pid' and count the number of rows for each 'pid'\n",
        "    row_counts_by_pid = detn_prior.groupby(\"pid\")[\"pid\"].count()\n",
        "\n",
        "    # Rename the 'pid' column to 'row_count'\n",
        "    row_counts_by_pid = row_counts_by_pid.rename(\"row_count\")\n",
        "\n",
        "    # Merge the row counts back into the 'admit' DataFrame\n",
        "    trend_df = pd.merge(trend_df, row_counts_by_pid, left_on=\"pid\", right_index=True, how=\"left\")\n",
        "\n",
        "    return trend_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiXnnN1lr0Lm"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_67AlkU30jiK"
      },
      "source": [
        "only export where pid in pids_with_visits\n",
        "as deterioration by definition requires us to look at a change since admission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6EYE0U7q4bV"
      },
      "outputs": [],
      "source": [
        "def convert_recent_weeklies_to_series(detn_prior, num_of_visits=2, weekly_columns=weekly_columns):\n",
        "    # Group by 'pid' and assign rank within each group based on 'sequence_num'\n",
        "    detn_prior[\"reverse_sequence_num\"] = detn_prior.groupby(\"pid\")[\"sequence_num\"].rank(\n",
        "        method=\"dense\", ascending=False\n",
        "    )\n",
        "    latest_visits = detn_prior[\n",
        "        detn_prior[\"reverse_sequence_num\"].isin(np.arange(1, num_of_visits + 1))\n",
        "    ][weekly_columns]\n",
        "    latest_visits.loc[\n",
        "        (\n",
        "            (latest_visits[\"final_numweeksback\"] == 0)\n",
        "            | (\n",
        "                (latest_visits[\"final_numweeksback\"] > 1)\n",
        "                & (latest_visits[\"final_numweeksback\"] < 2)\n",
        "            )\n",
        "        ),\n",
        "        \"final_numweeksback\",\n",
        "    ] = 1\n",
        "    latest_visits[\"final_numweeksback\"] = latest_visits[\"final_numweeksback\"].fillna(1)\n",
        "    # Replace NaN values with 1 as values are only 1 and 2\n",
        "    latest_visits.sort_values(by=[\"pid\", \"sequence_num\"], ascending=[True, False], inplace=True)\n",
        "    # make wk1 the most recent week\n",
        "    visit_series = (\n",
        "        latest_visits.assign(col=latest_visits.groupby(\"pid\").cumcount() + 1)\n",
        "        .set_index([\"pid\", \"col\"])\n",
        "        .unstack(\"col\")\n",
        "        .sort_index(level=(1, 0), axis=1)\n",
        "    )\n",
        "    visit_series.columns = [f\"wk{y}_{x}\" for x, y in visit_series.columns]\n",
        "    # prompt: make visit_series.index a column named 'pid'\n",
        "    visit_series = visit_series.reset_index()\n",
        "    return visit_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7KZT88mbGnl"
      },
      "outputs": [],
      "source": [
        "def remove_active_most_recent_weekly(admit_weekly):\n",
        "    # prompt: get admit_weekly unique pids with status=='active'\n",
        "    recent_pids = admit_weekly[(admit_weekly[\"status\"] == \"active\")][\"pid\"].unique()\n",
        "\n",
        "    # prompt: delete the most recent calcdate_weekly from admit_weekly where pid in recent_pids\n",
        "    # Group by pid and find the maximum calcdate_weekly for each pid in recent_pids\n",
        "    max_calcdate_weekly = (\n",
        "        admit_weekly[admit_weekly[\"pid\"].isin(recent_pids)].groupby(\"pid\")[\"calcdate_weekly\"].max()\n",
        "    )\n",
        "\n",
        "    # Merge the maximum calcdate_weekly back into the original dataframe\n",
        "    admit_weekly = admit_weekly.merge(\n",
        "        max_calcdate_weekly.rename(\"max_calcdate_weekly\"),\n",
        "        left_on=\"pid\",\n",
        "        right_index=True,\n",
        "        how=\"left\",\n",
        "    )\n",
        "\n",
        "    # Filter out rows with calcdate_weekly equal to the maximum for each pid in recent_pids\n",
        "    rows_to_delete = admit_weekly[\n",
        "        (admit_weekly[\"pid\"].isin(recent_pids))\n",
        "        & (admit_weekly[\"calcdate_weekly\"] == admit_weekly[\"max_calcdate_weekly\"])\n",
        "    ]\n",
        "\n",
        "    # Delete the rows\n",
        "    admit_weekly = admit_weekly.drop(rows_to_delete.index)\n",
        "\n",
        "    # Drop the temporary 'max_calcdate_weekly' column\n",
        "    admit_weekly = admit_weekly.drop(\"max_calcdate_weekly\", axis=1)\n",
        "    return admit_weekly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs1kOHuwgkcy"
      },
      "outputs": [],
      "source": [
        "# get the admittance date, weight and muac\n",
        "admit = admit_weekly[\n",
        "    [\n",
        "        \"pid\",\n",
        "        \"calcdate_admit_current\",\n",
        "        \"weight_admit_current\",\n",
        "        \"muac_admit_current\",\n",
        "        \"hl_admit\",\n",
        "        \"wfh_admit_current\",\n",
        "        \"hfa_admit_current\",\n",
        "        \"wfa_admit_current\",\n",
        "    ]\n",
        "].drop_duplicates(subset=[\"pid\"], keep=\"last\")\n",
        "\n",
        "# make the admit columns look like the weekly ones\n",
        "admit.rename(\n",
        "    columns={\n",
        "        \"calcdate_admit_current\": \"calcdate_weekly\",\n",
        "        \"weight_admit_current\": \"weight\",\n",
        "        \"muac_admit_current\": \"muac\",\n",
        "        \"hl_admit\": \"hl\",\n",
        "        \"wfa_admit_current\": \"wfa\",\n",
        "        \"hfa_admit_current\": \"hfa\",\n",
        "        \"wfa_admit_current\": \"wfa\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikuURwXs_H5s"
      },
      "outputs": [],
      "source": [
        "# prompt: keep the most recent num_recent-most calcdate_weekly from admit_weekly groupby('pid') and pid in recent_pids\n",
        "\n",
        "\n",
        "def remove_recent_weeklies(admit_weekly, recent_pids, num_recent=4):\n",
        "    \"\"\"Removes the most recent weekly entries for each pid.\n",
        "\n",
        "    Args:\n",
        "        admit_weekly: DataFrame.\n",
        "        num_recent: The number of recent entries to remove.\n",
        "        recent_pids: List of PIDs for which to remove recent entries.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Modified DataFrame with recent entries removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Group by 'pid' and rank the rows by 'calcdate_weekly' in descending order.\n",
        "    admit_weekly[\"rank\"] = (\n",
        "        admit_weekly[admit_weekly[\"pid\"].isin(recent_pids)]\n",
        "        .groupby(\"pid\")[\"calcdate_weekly\"]\n",
        "        .rank(method=\"dense\", ascending=False)\n",
        "    )\n",
        "\n",
        "    # Identify rows to delete (most recent num_recent entries)\n",
        "    rows_to_delete = admit_weekly[admit_weekly[\"rank\"] <= num_recent]\n",
        "\n",
        "    # Drop the identified rows\n",
        "    admit_weekly = admit_weekly.drop(rows_to_delete.index)\n",
        "    admit_weekly.drop(columns=[\"rank\"], inplace=True)\n",
        "    return admit_weekly\n",
        "\n",
        "\n",
        "# Example usage (assuming recent_pids is defined):\n",
        "# admit_weekly = remove_recent_weeklies(admit_weekly, recent_pids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywUTr7850xTA"
      },
      "outputs": [],
      "source": [
        "def weekly_agg(detn_prior, admit):\n",
        "    anthros = pd.concat(\n",
        "        [\n",
        "            detn_prior[[\"pid\", \"calcdate_weekly\", \"weight\", \"muac\", \"hl\", \"wfh\", \"hfa\", \"wfa\"]],\n",
        "            admit,\n",
        "        ],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "    # prompt: sort anthros by pid, calcdate_weekly\n",
        "\n",
        "    # Sort the 'anthros' DataFrame by 'pid' and then 'calcdate_weekly' so admittance row is first for each pid\n",
        "    anthros = anthros.sort_values(by=[\"pid\", \"calcdate_weekly\"])\n",
        "\n",
        "    weekly_agg = anthros.groupby(\"pid\").agg(\n",
        "        weekly_row_count=(\"pid\", \"count\"),\n",
        "        weekly_first_admit=(\"calcdate_weekly\", \"first\"),\n",
        "        weekly_last_admit=(\"calcdate_weekly\", \"last\"),\n",
        "        weekly_last_muac=(\"muac\", \"last\"),\n",
        "        weekly_first_muac=(\"muac\", \"first\"),\n",
        "        weekly_avg_muac=(\"muac\", \"mean\"),\n",
        "        weekly_first_weight=(\"weight\", \"first\"),\n",
        "        weekly_last_weight=(\"weight\", \"last\"),\n",
        "        weekly_avg_weight=(\"weight\", \"mean\"),\n",
        "        weekly_first_hl=(\"hl\", \"first\"),\n",
        "        weekly_last_hl=(\"hl\", \"last\"),\n",
        "        weekly_min_hl=(\"hl\", \"min\"),\n",
        "        weekly_max_hl=(\"hl\", \"max\"),\n",
        "        weekly_avg_hl=(\"hl\", \"mean\"),\n",
        "        weekly_first_wfh=(\"wfh\", \"first\"),\n",
        "        weekly_last_wfh=(\"wfh\", \"last\"),\n",
        "        weekly_min_wfh=(\"wfh\", \"min\"),\n",
        "        weekly_max_wfh=(\"wfh\", \"max\"),\n",
        "        weekly_avg_wfh=(\"wfh\", \"mean\"),\n",
        "        weekly_first_hfa=(\"hfa\", \"first\"),\n",
        "        weekly_last_hfa=(\"hfa\", \"last\"),\n",
        "        weekly_min_hfa=(\"hfa\", \"min\"),\n",
        "        weekly_max_hfa=(\"hfa\", \"max\"),\n",
        "        weekly_avg_hfa=(\"hfa\", \"mean\"),\n",
        "        weekly_first_wfa=(\"wfa\", \"first\"),\n",
        "        weekly_last_wfa=(\"wfa\", \"last\"),\n",
        "        weekly_min_wfa=(\"wfa\", \"min\"),\n",
        "        weekly_max_wfa=(\"wfa\", \"max\"),\n",
        "        weekly_avg_wfa=(\"wfa\", \"mean\"),\n",
        "    )\n",
        "\n",
        "    weekly_agg[\"muac_diff\"] = weekly_agg[\"weekly_last_muac\"] - weekly_agg[\"weekly_first_muac\"]\n",
        "    weekly_agg[\"weight_diff\"] = weekly_agg[\"weekly_last_weight\"] - weekly_agg[\"weekly_first_weight\"]\n",
        "    weekly_agg[\"calcdate_diff\"] = weekly_agg[\"weekly_last_admit\"] - weekly_agg[\"weekly_first_admit\"]\n",
        "    weekly_agg[\"calcdate_diff\"] = weekly_agg[\"calcdate_diff\"].dt.total_seconds() / (24 * 60 * 60)\n",
        "    weekly_agg[\"hl_diff\"] = weekly_agg[\"weekly_last_hl\"] - weekly_agg[\"weekly_first_hl\"]\n",
        "    weekly_agg[\"wfh_diff\"] = weekly_agg[\"weekly_last_wfh\"] - weekly_agg[\"weekly_first_wfh\"]\n",
        "    weekly_agg[\"hfa_diff\"] = weekly_agg[\"weekly_last_hfa\"] - weekly_agg[\"weekly_first_hfa\"]\n",
        "    weekly_agg[\"wfa_diff\"] = weekly_agg[\"weekly_last_wfa\"] - weekly_agg[\"weekly_first_wfa\"]\n",
        "\n",
        "    weekly_agg[\"weight_diff_ratio\"] = weekly_agg[\"weight_diff\"] / weekly_agg[\"weekly_first_weight\"]\n",
        "    weekly_agg[\"weight_diff_ratio_rate\"] = (\n",
        "        weekly_agg[\"weight_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "    )\n",
        "    weekly_agg[\"muac_diff_ratio\"] = weekly_agg[\"muac_diff\"] / weekly_agg[\"weekly_first_weight\"]\n",
        "    weekly_agg[\"muac_diff_ratio_rate\"] = weekly_agg[\"muac_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "\n",
        "    weekly_agg[\"hl_diff_ratio\"] = weekly_agg[\"hl_diff\"] / weekly_agg[\"weekly_first_hl\"]\n",
        "    weekly_agg[\"hl_diff_ratio_rate\"] = weekly_agg[\"hl_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "    weekly_agg[\"wfh_diff_ratio\"] = weekly_agg[\"wfh_diff\"] / weekly_agg[\"weekly_first_wfh\"]\n",
        "    weekly_agg[\"wfh_diff_ratio_rate\"] = weekly_agg[\"wfh_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "\n",
        "    weekly_agg[\"hfa_diff_ratio\"] = weekly_agg[\"hfa_diff\"] / weekly_agg[\"weekly_first_hfa\"]\n",
        "    weekly_agg[\"hfa_diff_ratio_rate\"] = weekly_agg[\"hfa_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "    weekly_agg[\"wfa_diff_ratio\"] = weekly_agg[\"wfa_diff\"] / weekly_agg[\"weekly_first_wfa\"]\n",
        "    weekly_agg[\"wfa_diff_ratio_rate\"] = weekly_agg[\"wfa_diff_ratio\"] / weekly_agg[\"calcdate_diff\"]\n",
        "\n",
        "    return weekly_agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXNzKnbCDN14"
      },
      "outputs": [],
      "source": [
        "# Load the mental health\n",
        "\n",
        "admit_current_mh = convert_bool_to_int(admit_current_mh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMLgIwTwXAYy"
      },
      "outputs": [],
      "source": [
        "def only_rows_before_detn(detn, detn_col):\n",
        "    # Get unique PIDs where 'detn_col' is True in the detn DataFrame\n",
        "    detn_ever_pids = detn.loc[detn[detn_col] == True, \"pid\"].unique()\n",
        "    # Find PIDs in 'detn' that are NOT in 'detn_ever_pids'\n",
        "    pids_not_in_ever_pids = detn.loc[~detn[\"pid\"].isin(detn_ever_pids), \"pid\"].unique()\n",
        "\n",
        "    # prompt: remove rows with sequence_number >= first detn_ever group by pid\n",
        "\n",
        "    # Group by 'pid' and find the first occurrence of 'detn_ever' == True\n",
        "    if detn_col == \"nonresponse\":\n",
        "        first_detn_ever = (\n",
        "            detn.loc[detn[detn_col] == True].groupby(\"pid\")[\"sequence_num\"].max().reset_index()\n",
        "        )\n",
        "    else:\n",
        "        first_detn_ever = (\n",
        "            detn.loc[detn[detn_col] == True].groupby(\"pid\")[\"sequence_num\"].min().reset_index()\n",
        "        )\n",
        "\n",
        "    # prompt: get admit_weekly[y_cat1] for max sequence_number by pid\n",
        "\n",
        "    # Get admit_weekly[y_cat1] for max sequence_number by pid\n",
        "    y_cat1_copy = y_cat1.copy()\n",
        "    y_cat1_copy.insert(0, \"pid\")\n",
        "\n",
        "    # Rename the 'sequence_number' column to 'first_detn_seq' for clarity\n",
        "    first_detn_ever = first_detn_ever.rename(columns={\"sequence_num\": \"first_detn_seq\"})\n",
        "\n",
        "    # detn.drop(columns=['first_detn_seq'], inplace=True)\n",
        "    # Merge the 'first_detn_seq' back into the original DataFrame\n",
        "    detn = pd.merge(detn, first_detn_ever, on=\"pid\", how=\"left\")\n",
        "\n",
        "    y_detn_cat1 = detn[detn[\"sequence_num\"] == detn[\"first_detn_seq\"]][y_cat1_copy].copy()\n",
        "\n",
        "    # Filter out rows where 'sequence_number' is greater than or equal to 'first_detn_seq'\n",
        "    # max_sequence_rows = detn.loc[detn.groupby('pid')['sequence_num'].idxmax()]\n",
        "\n",
        "    # Filter out rows where 'sequence_number' is greater than or equal to 'first_detn_seq'\n",
        "    # print(detn[detn['pid']=='23-0107'][['pid','sequence_num','first_detn_seq']])\n",
        "    seq_ct = 0\n",
        "    if detn_col == \"nonresponse\":\n",
        "        # for nonresponse, discard the few rows before the event happened to discourage the model from keying on los or duration\n",
        "        seq_ct = 3\n",
        "\n",
        "    detn_prior = detn[\n",
        "        ((detn[\"sequence_num\"] + seq_ct) < detn[\"first_detn_seq\"])\n",
        "        & (detn[\"pid\"].isin(detn_ever_pids))\n",
        "    ].copy()\n",
        "\n",
        "    # print(detn_prior[detn_prior['pid']=='23-0107'][['pid','sequence_num','first_detn_seq']])\n",
        "\n",
        "    # print(detn.loc[detn['detn_ever'] == False].shape)\n",
        "\n",
        "    # detn_prior contains only rows before the first deterioration for each patient plus all non-deteriorated patients with all their rows\n",
        "    # Concatenate detn_prior and detn.loc[detn['detn_ever'] == False]\n",
        "    detn_prior = pd.concat([detn_prior, detn[~detn[\"pid\"].isin(detn_ever_pids)]])\n",
        "    # clean up the working column\n",
        "    detn.drop(columns=[\"first_detn_seq\"], inplace=True)\n",
        "\n",
        "    # Create a pandas Series where the index is detn_ever_pids and the value is True\n",
        "    detn_ever_pids_series = pd.Series(index=detn_ever_pids, data=True)\n",
        "\n",
        "    pids_not_in_ever_pids = pd.Series(index=pids_not_in_ever_pids, data=False)\n",
        "    # Concatenate the two Series\n",
        "    y_detn = pd.concat([detn_ever_pids_series, pids_not_in_ever_pids])\n",
        "    # Rename the Series\n",
        "    y_detn.name = detn_col\n",
        "    # print('a',y_detn_cat1.shape)\n",
        "\n",
        "    y_detn_cat1 = pd.merge(y_detn_cat1, y_detn, how=\"right\", left_on=\"pid\", right_on=y_detn.index)\n",
        "    # print('b',y_detn_cat1.shape,y_detn.shape)\n",
        "    y_detn_cat1.fillna(False, inplace=True)\n",
        "    for col in y_cat1:\n",
        "        y_detn_cat1[col] = y_detn_cat1[col].astype(int)\n",
        "    return detn_prior, y_detn, y_detn_cat1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91IsdFyiBTGe"
      },
      "outputs": [],
      "source": [
        "def prepare_export(detn_col=\"new_onset_medical_complication\"):\n",
        "    # get rows prior to the deterioration\n",
        "    y_detn_cat1 = pd.DataFrame()\n",
        "\n",
        "    recent_pids = admit_weekly[(admit_weekly[\"status\"] == \"active\")][\"pid\"].unique()\n",
        "    if detn_col in [\"new_onset_medical_complication\"]:\n",
        "        # remove the row that may have the deterioration we want to predict, if pid is currently active\n",
        "        detn_prior, y_detn, y_detn_cat1 = only_rows_before_detn(\n",
        "            remove_recent_weeklies(admit_weekly, recent_pids, num_recent=1), detn_col\n",
        "        )\n",
        "    elif detn_col in [\"oedema_not_disappearing\"]:\n",
        "        # remove the row that may have the deterioration we want to predict, if pid is currently active\n",
        "        detn_prior, y_detn, _ = only_rows_before_detn(\n",
        "            remove_recent_weeklies(admit_weekly, recent_pids, num_recent=1), detn_col\n",
        "        )\n",
        "    elif detn_col == \"muac_loss_2_weeks_consecutive\":\n",
        "        # remove the 2 rows that may have the deterioration we want to predict, if pid is currently active\n",
        "        detn_prior, y_detn, _ = only_rows_before_detn(\n",
        "            remove_recent_weeklies(admit_weekly, recent_pids, num_recent=2), detn_col\n",
        "        )\n",
        "    elif detn_col == \"detn_weight_loss_ever\":\n",
        "        # remove the 4 rows that may have the deterioration we want to predict, if pid is currently active\n",
        "        detn_prior, y_detn, _ = only_rows_before_detn(\n",
        "            remove_recent_weeklies(admit_weekly, recent_pids, num_recent=4), detn_col\n",
        "        )\n",
        "    elif detn_col == \"status_dead\":\n",
        "        y_detn = pd.Series(index=admit_weekly_all[\"pid\"].unique(), dtype=bool)\n",
        "        y_detn[:] = 0  # Initialize all values to False\n",
        "        y_detn[pids_dead] = 1\n",
        "        y_detn.rename(detn_col, inplace=True)\n",
        "        # remove no rows as death status is set from current status\n",
        "        detn_prior = admit_weekly_all.copy()\n",
        "    elif detn_col == \"nonresponse\":\n",
        "        # y_detn = pd.Series(index=admit_weekly_all['pid'].unique(), dtype=bool)\n",
        "        # y_detn[:] = 0  # Initialize all values to False\n",
        "        # y_detn[pids_nonresponse] = 1\n",
        "        # y_detn.rename(detn_col,inplace=True)\n",
        "        # remove no rows as nonresponse is set from current status\n",
        "        # detn_prior = admit_weekly_all.copy()\n",
        "        detn_prior, y_detn, _ = only_rows_before_detn(\n",
        "            remove_recent_weeklies(admit_weekly, recent_pids, num_recent=4), detn_col\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        detn_prior, y_detn, _ = only_rows_before_detn(admit_weekly, detn_col)\n",
        "\n",
        "    # get weekly aggregate stats\n",
        "    detn_prior.rename(columns={\"weight_weekly\": \"weight\", \"muac_weekly\": \"muac\"}, inplace=True)\n",
        "    detn_prior.rename(\n",
        "        columns={\"wfa_weekly\": \"wfa\", \"wfh_weekly\": \"wfh\", \"hfa_weekly\": \"hfa\"}, inplace=True\n",
        "    )\n",
        "    detn_prior.sort_values(by=[\"pid\", \"calcdate_weekly\"], inplace=True)\n",
        "    weekly_agg_stats = weekly_agg(detn_prior, admit)\n",
        "    # print(weekly_agg_stats[weekly_agg_stats['pid']== '24-3335'])\n",
        "    detn_prior.rename(columns={\"weight_weekly\": \"weight\", \"muac_weekly\": \"muac\"}, inplace=True)\n",
        "    detn_prior.rename(\n",
        "        columns={\"wfa_weekly\": \"wfa\", \"wfh_weekly\": \"wfh\", \"hfa_weekly\": \"hfa\"}, inplace=True\n",
        "    )\n",
        "    # get trend for those rows\n",
        "    trend_stats = trend(detn_prior, admit_weekly, admit, detn_col)\n",
        "    visit_series = convert_recent_weeklies_to_series(\n",
        "        detn_prior, num_of_visits=3, weekly_columns=weekly_columns\n",
        "    )\n",
        "    export = pd.merge(\n",
        "        admit_raw, visit_series, on=\"pid\", how=\"left\"\n",
        "    )  # no overlap so suffix isn't used\n",
        "    # add weekly stats columns to admit_raw\n",
        "    export = pd.merge(export, weekly_agg_stats, on=\"pid\", how=\"left\")\n",
        "    # add trends to admit_raw\n",
        "    export = pd.merge(export, trend_stats, on=\"pid\", how=\"left\")\n",
        "\n",
        "    # Merge with admit_raw cat2_sum_by_pid based on the 'pid' column\n",
        "    export = pd.merge(export, cat1_sum_by_pid, on=\"pid\", how=\"left\")\n",
        "    export = pd.merge(export, cat2_sum_by_pid, on=\"pid\", how=\"left\")\n",
        "\n",
        "    # get weekly cat1, cat2 counts up to deterioration\n",
        "    numeric_cols = detn_prior.select_dtypes(include=[\"number\", \"bool\"]).columns\n",
        "    numeric_cat1_cols = [col for col in numeric_cols if col.startswith(\"cat1_\")]\n",
        "    numeric_cat2_cols = [col for col in numeric_cols if col.startswith(\"cat2_\")]\n",
        "\n",
        "    cat1_sum_by_pid_weekly, cat2_sum_by_pid_weekly = count_cat1_cat2(\n",
        "        detn_prior, numeric_cat1_cols, numeric_cat2_cols\n",
        "    )\n",
        "    # cat1_sum_by_pid_weekly, cat2_sum_by_pid_weekly = count_cat1_cat2(detn_prior, cat1_weekly_cols, cat2_weekly_cols)\n",
        "\n",
        "    export = pd.merge(export, cat1_sum_by_pid_weekly, on=\"pid\", how=\"left\")\n",
        "    export = pd.merge(export, cat2_sum_by_pid_weekly, on=\"pid\", how=\"left\")\n",
        "\n",
        "    # prompt: filter export where pid in pids_with_visits\n",
        "    # as deterioration by definition requires us to look at a change since admission\n",
        "    # export = export[export['pid'].isin(pids_with_visits)]\n",
        "\n",
        "    # prompt: find columns that are single value and nonnull, then drop them\n",
        "\n",
        "    single_value_cols = [\n",
        "        col for col in export.columns if export[col].nunique() == 1 and export[col].notna().all()\n",
        "    ]\n",
        "\n",
        "    export.drop(columns=single_value_cols, inplace=True)\n",
        "    convert_3val_bool(export, len(export))\n",
        "    convert_to_bool(export)\n",
        "    boolean_columns = export.select_dtypes(include=[\"bool\"]).columns\n",
        "    # Convert boolean columns to numeric\n",
        "    for col in boolean_columns:\n",
        "        export[col] = export[col].astype(int)\n",
        "    export = infer_phq_score(admit_current_mh, admit_current, export)\n",
        "\n",
        "    return export, y_detn, y_detn_cat1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMvkkEtZeHUw"
      },
      "outputs": [],
      "source": [
        "# prompt: select number, int and boolean columns from admit_weekly\n",
        "\n",
        "# Assuming 'admit_weekly' DataFrame is already defined and loaded.\n",
        "# Example usage:\n",
        "\n",
        "# Select number, integer and boolean columns\n",
        "#numeric_cols = admit_weekly.select_dtypes(include=[\"number\", \"bool\"]).columns\n",
        "#selected_columns = admit_weekly[numeric_cols]\n",
        "\n",
        "# print(selected_columns.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1UH3ebRCs5M"
      },
      "outputs": [],
      "source": [
        "# prompt: find columns in admit_weekly that are numeric and start with 'cat1_'\n",
        "\n",
        "# Assuming 'admit_weekly' is your DataFrame.\n",
        "#numeric_cat1_cols = admit_weekly.select_dtypes(include=np.number).columns\n",
        "#result = [col for col in numeric_cat1_cols if col.startswith(\"cat1_\")]\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErR5xjYR8LMC"
      },
      "outputs": [],
      "source": [
        "deterioration_types = [\n",
        "    \"detn_weight_loss_ever\",\n",
        "    \"new_onset_medical_complication\",\n",
        "    \"muac_loss_2_weeks_consecutive\",\n",
        "    \"oedema_not_disappearing\",\n",
        "    \"nonresponse\",\n",
        "    \"status_dead\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_eMXoVJcXMP"
      },
      "outputs": [],
      "source": [
        "def get_first_detn_date(admit_weekly, variable, date_col=\"calcdate_weekly\"):\n",
        "    # Group by 'pid' and filter for 'new_onset_medical_complication' == True\n",
        "    filtered_df = admit_weekly[admit_weekly[variable] == True].groupby(\"pid\")\n",
        "    # 'status_date' for nonresponse variable\n",
        "    # Get the minimum 'calcdate_weekly' for each group\n",
        "    min_calcdate = filtered_df[date_col].min()\n",
        "\n",
        "    min_calcdate.rename(f\"{variable}_date\", inplace=True)\n",
        "    min_calcdate = min_calcdate.reset_index()\n",
        "\n",
        "    return min_calcdate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cb2XpJ3qvBuo",
        "outputId": "34611763-d406-4828-e459-0f9ad9f08e98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:detn_weight_loss_ever\n",
            "DEBUG:my_logger:wfh\n",
            "100%|██████████| 2152/2152 [00:17<00:00, 123.28it/s]\n",
            "DEBUG:my_logger:(2152, 3)\n",
            "DEBUG:my_logger:hfa\n",
            "100%|██████████| 2152/2152 [00:17<00:00, 120.60it/s]\n",
            "DEBUG:my_logger:(2152, 5)\n",
            "DEBUG:my_logger:wfa\n",
            "100%|██████████| 2152/2152 [00:16<00:00, 130.14it/s]\n",
            "DEBUG:my_logger:(2152, 7)\n",
            "DEBUG:my_logger:weight\n",
            "100%|██████████| 2152/2152 [00:17<00:00, 124.13it/s]\n",
            "DEBUG:my_logger:(2152, 9)\n",
            "DEBUG:my_logger:muac\n",
            "100%|██████████| 2152/2152 [00:16<00:00, 129.13it/s]\n",
            "DEBUG:my_logger:(2152, 11)\n",
            "DEBUG:my_logger:hl\n",
            "100%|██████████| 2152/2152 [00:17<00:00, 123.56it/s]\n",
            "DEBUG:my_logger:(2152, 13)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:new_onset_medical_complication\n",
            "DEBUG:my_logger:wfh\n",
            "100%|██████████| 1201/1201 [00:08<00:00, 134.01it/s]\n",
            "DEBUG:my_logger:(1201, 3)\n",
            "DEBUG:my_logger:hfa\n",
            "100%|██████████| 1201/1201 [00:08<00:00, 133.61it/s]\n",
            "DEBUG:my_logger:(1201, 5)\n",
            "DEBUG:my_logger:wfa\n",
            "100%|██████████| 1201/1201 [00:08<00:00, 134.75it/s]\n",
            "DEBUG:my_logger:(1201, 7)\n",
            "DEBUG:my_logger:weight\n",
            "100%|██████████| 1201/1201 [00:08<00:00, 141.07it/s]\n",
            "DEBUG:my_logger:(1201, 9)\n",
            "DEBUG:my_logger:muac\n",
            "100%|██████████| 1201/1201 [00:08<00:00, 134.70it/s]\n",
            "DEBUG:my_logger:(1201, 11)\n",
            "DEBUG:my_logger:hl\n",
            "100%|██████████| 1201/1201 [00:09<00:00, 133.31it/s]\n",
            "DEBUG:my_logger:(1201, 13)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:my_logger:muac_loss_2_weeks_consecutive\n",
            "DEBUG:my_logger:wfh\n",
            " 82%|████████▏ | 1003/1229 [00:09<00:02, 108.79it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-37781f624c38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeterioration_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_detn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_detn_cat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetn_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# get date of when deterioration first occurred and set it (for hazard analysis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-2fbd8a70a1cd>\u001b[0m in \u001b[0;36mprepare_export\u001b[0;34m(detn_col)\u001b[0m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# get trend for those rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrend_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetn_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madmit_weekly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetn_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     visit_series = convert_recent_weeklies_to_series(\n\u001b[1;32m     62\u001b[0m         \u001b[0mdetn_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_visits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekly_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweekly_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-d1f57300bb80>\u001b[0m in \u001b[0;36mtrend\u001b[0;34m(detn_prior, admit_weekly, admit, detn_col)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# only recalculate the trends for the partial weeklies for the pids with the deterioriation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_pids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_squared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manthros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manthro_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             results.append(\n\u001b[1;32m     40\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"pid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{anthro_col}_trend\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{anthro_col}_rsquared\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr_squared\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/health-predictions/packages/inference/run/util.py\u001b[0m in \u001b[0;36mregress\u001b[0;34m(df, pid, variable)\u001b[0m\n\u001b[1;32m    528\u001b[0m   \"\"\"\n\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Filter for the specific pid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0manthros_pid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pid\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manthros_pid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for col in deterioration_types:\n",
        "    logger.debug(col)\n",
        "    export, y_detn, y_detn_cat1 = prepare_export(detn_col=col)\n",
        "\n",
        "    # get date of when deterioration first occurred and set it (for hazard analysis)\n",
        "    if col in [\"nonresponse\", \"status_dead\"]:\n",
        "        first_detn_date = get_first_detn_date(admit_weekly_all, col, \"status_date\")\n",
        "    else:\n",
        "        first_detn_date = get_first_detn_date(admit_weekly, col, \"calcdate_weekly\")\n",
        "\n",
        "    # first_detn_date= get_first_detn_date(admit_weekly,col,date_col)\n",
        "    export = pd.merge(export, first_detn_date, on=\"pid\", how=\"left\")\n",
        "    # prompt: add series y_detn_ever as column to admit_raw\n",
        "    # do inner join which discards patients w/no visit as their deterioration status needs to be decided still,\n",
        "    # TODO probably could include death cases\n",
        "    # current = pd.read_csv(dir+\"train_pba_current_processed_2024-11-02.csv\")\n",
        "\n",
        "    # just include all pids\n",
        "    detn_ever_pids = admit_weekly.loc[admit_weekly[col] == True, \"pid\"].unique()\n",
        "    detn_ever_pids_series = pd.Series(index=detn_ever_pids, data=True)\n",
        "    pids_not_in_ever_pids = admit_weekly.loc[\n",
        "        ~admit_weekly[\"pid\"].isin(detn_ever_pids), \"pid\"\n",
        "    ].unique()\n",
        "    pids_not_in_ever_pids_series = pd.Series(index=pids_not_in_ever_pids, data=False)\n",
        "    # Concatenate the two Series\n",
        "    y_detn_all = pd.concat([pids_not_in_ever_pids_series, detn_ever_pids_series])\n",
        "    # Rename the Series\n",
        "    y_detn_all.name = col\n",
        "    print(y_detn_all.sum())\n",
        "    if col == \"new_onset_medical_complication\":\n",
        "        export = export.merge(y_detn_cat1, on=\"pid\", how=\"left\")\n",
        "    elif col in [\"nonresponse\", \"status_dead\"]:\n",
        "        y_detn.name = col\n",
        "        export = export.merge(y_detn, left_on=\"pid\", right_index=True, how=\"left\")\n",
        "    else:\n",
        "        export = export.merge(y_detn_all, left_on=\"pid\", right_index=True, how=\"left\")\n",
        "    export = export.replace(-np.inf, 0)\n",
        "    export[col].fillna(False, inplace=True)\n",
        "    export[col] = export[col].astype(int)\n",
        "    export[\"row_count\"].fillna(0, inplace=True)\n",
        "    export[\"weekly_row_count\"].fillna(0, inplace=True)\n",
        "    logger.debutg(f'{export.shape}, {export[\"pid\"].nunique()}, {export[col].sum()}')\n",
        "    with open(dir + f\"analysis/{col}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(export, f)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z2HISbIusYHQ"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
